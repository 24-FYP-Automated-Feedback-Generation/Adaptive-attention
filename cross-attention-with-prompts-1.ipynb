{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10048507,"sourceType":"datasetVersion","datasetId":6190931},{"sourceId":10242967,"sourceType":"datasetVersion","datasetId":6334477},{"sourceId":10316683,"sourceType":"datasetVersion","datasetId":6386966}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/cross-attentive-adaptaion-1-42e20c40-7d10-4162-a18f-309d0acc6c84.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20250102/auto/storage/goog4_request&X-Goog-Date=20250102T105331Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=92b18c91ac0391ce0f352019d957ea52f7551e139baab2f4c2d081e7b00cb3f45fdac366f022f08f3e38baf8177fd795fc8c3fbd8cad5cf56a5768e1f52ecd6947542b4909e5a9704e2c78165792da412f67a2b5534420e56e9210a50dc822df79a0f3bb4d052e68727023669ed37f2efb85edb1d0d5fdf1d1e463a7004504deba440307442799c3c2b8c08fe8201d4f5c87f259697732dcad9ac39bece48fa8c1893336ef7d31b901d26c6edfc0d56ea8bdc41c37a1801047f9dba70ca1e3df6573910e4c014ed26b38930e479a44a1e54ca15f5838a9a6213b67a6a320eb0377e57092dc5909688f060cd8995b0bfa7bcccc11ea2ad546230a88177d6e0307","timestamp":1736008492951}],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"43da2f47170345cdac7f72b7aa84662d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_89cfdd8672f548ca8d8722b5c39c240e","IPY_MODEL_a4ecd892759c4a759d069e293e15b779","IPY_MODEL_9aa1e481e9ad45678462f48428086ace"],"layout":"IPY_MODEL_f3027b6fbe7048378dd09030b5bd55d5"}},"89cfdd8672f548ca8d8722b5c39c240e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e0f7c835fd643c98686a30e7a7062fe","placeholder":"​","style":"IPY_MODEL_ad0642d1ee6e4d5381fb0fa34a4cf092","value":"config.json: 100%"}},"a4ecd892759c4a759d069e293e15b779":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d36ea6e57e142c1b40a79c8f4b8f6de","max":1208,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26e3066c9eb940c297966090d0d235d8","value":1208}},"9aa1e481e9ad45678462f48428086ace":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_feee58e7de1340e3860c9a95ff4be60e","placeholder":"​","style":"IPY_MODEL_2976f27258574b5d867dfdcbe16f9769","value":" 1.21k/1.21k [00:00&lt;00:00, 53.0kB/s]"}},"f3027b6fbe7048378dd09030b5bd55d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e0f7c835fd643c98686a30e7a7062fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad0642d1ee6e4d5381fb0fa34a4cf092":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d36ea6e57e142c1b40a79c8f4b8f6de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26e3066c9eb940c297966090d0d235d8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"feee58e7de1340e3860c9a95ff4be60e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2976f27258574b5d867dfdcbe16f9769":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"749891e75dea4320a86201f0d4e19c16":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3958951756e54804a17eae58aa6ba0dc","IPY_MODEL_1255530944b14d6099c53a92b2bb2bff","IPY_MODEL_eeb45b3f646a49f0bdee3d80c96685f5"],"layout":"IPY_MODEL_e2f860e4136747159aa03f5d0a63d368"}},"3958951756e54804a17eae58aa6ba0dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e539e4ce810e416986a5fa7f70494395","placeholder":"​","style":"IPY_MODEL_1d3f7e377c3348948b55485857e699f1","value":"spiece.model: 100%"}},"1255530944b14d6099c53a92b2bb2bff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3add81bdfdc2431cb7e774e4fa914292","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f85aef5dcd64053acd7128663657eb0","value":791656}},"eeb45b3f646a49f0bdee3d80c96685f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9fc7544d1fc4848a6161195104e1254","placeholder":"​","style":"IPY_MODEL_ee7e1d8f427d4b698a0ad3fa19904269","value":" 792k/792k [00:00&lt;00:00, 3.72MB/s]"}},"e2f860e4136747159aa03f5d0a63d368":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e539e4ce810e416986a5fa7f70494395":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d3f7e377c3348948b55485857e699f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3add81bdfdc2431cb7e774e4fa914292":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f85aef5dcd64053acd7128663657eb0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a9fc7544d1fc4848a6161195104e1254":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee7e1d8f427d4b698a0ad3fa19904269":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e04aa00ec6bf4c4cac5f87cf82a27c54":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7ec5c7b696ff4d7098aa0681b6ca625c","IPY_MODEL_9d530a87310d4c47bfa18a06fca19e17","IPY_MODEL_35a0e3310300473b97150e41c0b23339"],"layout":"IPY_MODEL_2d05e85d3811459e92522fe7c44cc95a"}},"7ec5c7b696ff4d7098aa0681b6ca625c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_744b2bc8bdb346c79694d3164fb8ceec","placeholder":"​","style":"IPY_MODEL_d03e94b61e1840e3b7f0ec3c6ae37b73","value":"tokenizer.json: 100%"}},"9d530a87310d4c47bfa18a06fca19e17":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_da66806b1f8f4a8faacf076045688d5f","max":1389353,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f000bf67102b4b0bb9322d272114958b","value":1389353}},"35a0e3310300473b97150e41c0b23339":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1770b5d95b514882bdc265bada08474f","placeholder":"​","style":"IPY_MODEL_6f0779e020d44f9396ca5294b3d3975a","value":" 1.39M/1.39M [00:00&lt;00:00, 6.71MB/s]"}},"2d05e85d3811459e92522fe7c44cc95a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"744b2bc8bdb346c79694d3164fb8ceec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d03e94b61e1840e3b7f0ec3c6ae37b73":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da66806b1f8f4a8faacf076045688d5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f000bf67102b4b0bb9322d272114958b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1770b5d95b514882bdc265bada08474f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f0779e020d44f9396ca5294b3d3975a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b4b9829c79e4df2b7f6159d4a54634a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7574a2d7e4984e01ae8043545e7f3afb","IPY_MODEL_b28caa6bf2c745ec87cf3da06ecf4a7a","IPY_MODEL_cd57ad7ae6aa4ced911bf44e8b5437d0"],"layout":"IPY_MODEL_5c3722f465704e12bd93bb12220b42e3"}},"7574a2d7e4984e01ae8043545e7f3afb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c98216f497743dcb8abb7b69be97942","placeholder":"​","style":"IPY_MODEL_9c817b66deb1488b95479ae5667b71cf","value":"tokenizer_config.json: 100%"}},"b28caa6bf2c745ec87cf3da06ecf4a7a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7e2b480b24244919ec20c23720905a7","max":26,"min":0,"orientation":"horizontal","style":"IPY_MODEL_214c9d061b734b03bd0060ef66fb8c02","value":26}},"cd57ad7ae6aa4ced911bf44e8b5437d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_770dcb4cf22d4035a8b8970aed70412b","placeholder":"​","style":"IPY_MODEL_17209e71fc73454b99b23f617c5643d0","value":" 26.0/26.0 [00:00&lt;00:00, 465B/s]"}},"5c3722f465704e12bd93bb12220b42e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c98216f497743dcb8abb7b69be97942":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c817b66deb1488b95479ae5667b71cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7e2b480b24244919ec20c23720905a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"214c9d061b734b03bd0060ef66fb8c02":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"770dcb4cf22d4035a8b8970aed70412b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17209e71fc73454b99b23f617c5643d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3aa1fd17abe44555ad47e0aef4c0f938":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1bd6bdbc2e204509b9c209417733f4a3","IPY_MODEL_b8ac8ee34b73426190aebf20a0035eae","IPY_MODEL_4cf598feee2b4c71938150e26c9cb341"],"layout":"IPY_MODEL_0ceaa855fc724594bf8c64a8a1926e01"}},"1bd6bdbc2e204509b9c209417733f4a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d80c6058f22a47cba18ed8e9eb1918ae","placeholder":"​","style":"IPY_MODEL_13f8d88212184e9889255c7e6f3f6dfd","value":"vocab.json: 100%"}},"b8ac8ee34b73426190aebf20a0035eae":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_de559a29729142c2ae3a9641c96478c4","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6df5febabe3c4062ab1c055fbea5a62b","value":1042301}},"4cf598feee2b4c71938150e26c9cb341":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfa0c3241c3f4d9088aa3a8ea8b45f70","placeholder":"​","style":"IPY_MODEL_ebcecc565e484bf0933b3910d4566726","value":" 1.04M/1.04M [00:00&lt;00:00, 5.02MB/s]"}},"0ceaa855fc724594bf8c64a8a1926e01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d80c6058f22a47cba18ed8e9eb1918ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13f8d88212184e9889255c7e6f3f6dfd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de559a29729142c2ae3a9641c96478c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6df5febabe3c4062ab1c055fbea5a62b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dfa0c3241c3f4d9088aa3a8ea8b45f70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebcecc565e484bf0933b3910d4566726":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3809873b8cf045c2a00ba0d62261f6af":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3d86694625d04af094c9beef8a589b16","IPY_MODEL_d5f9e809db154bbfaf908195b325a34a","IPY_MODEL_fc953cc050dd4280846cd6e1219019e7"],"layout":"IPY_MODEL_9c27818ce29f4d4c97bc4b55efad78e5"}},"3d86694625d04af094c9beef8a589b16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_617a9be3796a4ec8a0377b82f497635e","placeholder":"​","style":"IPY_MODEL_8309e9644c074f83ab4f4fd9a99582c6","value":"merges.txt: 100%"}},"d5f9e809db154bbfaf908195b325a34a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d533ecc7ab649aaa98a0712db11376c","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_357500baa89547c39282e086ab7a7999","value":456318}},"fc953cc050dd4280846cd6e1219019e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_942420c6462c47ca8a0e09321e60a1b8","placeholder":"​","style":"IPY_MODEL_9f5ed38aa7a94a00b373febd383104ca","value":" 456k/456k [00:00&lt;00:00, 13.6MB/s]"}},"9c27818ce29f4d4c97bc4b55efad78e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"617a9be3796a4ec8a0377b82f497635e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8309e9644c074f83ab4f4fd9a99582c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d533ecc7ab649aaa98a0712db11376c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"357500baa89547c39282e086ab7a7999":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"942420c6462c47ca8a0e09321e60a1b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f5ed38aa7a94a00b373febd383104ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e801240ba7b42378f27c301c0fcac29":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7b0cabdc4da749228879868a6f7e34e3","IPY_MODEL_9fabc1f28c5248d8a9ced22a2f37890c","IPY_MODEL_95879c33ac2c4568a8b1fbe4f4004009"],"layout":"IPY_MODEL_421fb5dee92b40e0b94ac250bdc13adc"}},"7b0cabdc4da749228879868a6f7e34e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbf744dc7b674882b25a67ececf38fe3","placeholder":"​","style":"IPY_MODEL_ff2f0600478f4801a2243cb634722196","value":"tokenizer.json: 100%"}},"9fabc1f28c5248d8a9ced22a2f37890c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0bffe6826a9406dad9fbc75e1dc99e2","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e1c977ccc074a42b090867b860297b9","value":1355256}},"95879c33ac2c4568a8b1fbe4f4004009":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f73dfe68ceb4c2689ad142e0664340a","placeholder":"​","style":"IPY_MODEL_ebeab6709a6e40e58a661585be171203","value":" 1.36M/1.36M [00:00&lt;00:00, 6.66MB/s]"}},"421fb5dee92b40e0b94ac250bdc13adc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbf744dc7b674882b25a67ececf38fe3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff2f0600478f4801a2243cb634722196":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0bffe6826a9406dad9fbc75e1dc99e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e1c977ccc074a42b090867b860297b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3f73dfe68ceb4c2689ad142e0664340a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebeab6709a6e40e58a661585be171203":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44d5dfde2cea49ffb47e9fa82d182502":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_78c2f42c86e644469f1697ace4f1e0d1","IPY_MODEL_bde9128d864947c7918a2192aa8bbdcb","IPY_MODEL_cf1a5f16f250435082e836611374688a"],"layout":"IPY_MODEL_c9185e9b796c4e14be13dc4ebb223e1b"}},"78c2f42c86e644469f1697ace4f1e0d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e24b589fab34b389b4b7b1bab26f624","placeholder":"​","style":"IPY_MODEL_5a6158b3e2934c5d9e2b15c2f6951031","value":"config.json: 100%"}},"bde9128d864947c7918a2192aa8bbdcb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aaa6919585e543969a70349ec8f870da","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9140609d187f48869561947744cda081","value":665}},"cf1a5f16f250435082e836611374688a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bb1f013da664d34a26d042c062d8336","placeholder":"​","style":"IPY_MODEL_ee1033c0ac7245358b4aaeb3ae3e2502","value":" 665/665 [00:00&lt;00:00, 15.5kB/s]"}},"c9185e9b796c4e14be13dc4ebb223e1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e24b589fab34b389b4b7b1bab26f624":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a6158b3e2934c5d9e2b15c2f6951031":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aaa6919585e543969a70349ec8f870da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9140609d187f48869561947744cda081":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7bb1f013da664d34a26d042c062d8336":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee1033c0ac7245358b4aaeb3ae3e2502":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n# THEN FEEL FREE TO DELETE THIS CELL.\n# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# NOTEBOOK.\nimport kagglehub\nuom200407h_modified_dataset_path = kagglehub.dataset_download('uom200407h/modified-dataset')\nuom200644f_metacognitive_dataset_path = kagglehub.dataset_download('uom200644f/metacognitive-dataset')\nbashadithennakoon_metacognitive_feedback_for_algorithm_solving_path = kagglehub.dataset_download('bashadithennakoon/metacognitive-feedback-for-algorithm-solving')\n\nprint('Data source import complete.')\n","metadata":{"id":"WZPr7xkva7Id"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"XOEjCcHQa7Ii","execution":{"iopub.status.busy":"2025-01-09T12:53:25.041937Z","iopub.execute_input":"2025-01-09T12:53:25.042240Z","iopub.status.idle":"2025-01-09T12:53:25.352980Z","shell.execute_reply.started":"2025-01-09T12:53:25.042210Z","shell.execute_reply":"2025-01-09T12:53:25.352216Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/metacognitive-feedback-for-algorithm-solving/final_dataset_with_annotated_metacognitive_feedback_gpt-4o-mini.csv\n/kaggle/input/metacognitive-dataset/metacognitive-dataset.csv\n/kaggle/input/modified-dataset/modified_dataset.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install transformers torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:53:25.354117Z","iopub.execute_input":"2025-01-09T12:53:25.354473Z","iopub.status.idle":"2025-01-09T12:53:29.776264Z","shell.execute_reply.started":"2025-01-09T12:53:25.354442Z","shell.execute_reply":"2025-01-09T12:53:29.775205Z"},"id":"TYCjdbZUa7Ij","executionInfo":{"status":"ok","timestamp":1736007590391,"user_tz":-330,"elapsed":3960,"user":{"displayName":"","userId":""}},"outputId":"1854ce93-b297-4c35-e5fc-59e5bbd37f61","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import AutoModel, AutoTokenizer, GPT2Model,GPT2Tokenizer,GPT2LMHeadModel\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration , T5Tokenizer , T5Model\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:53:29.778001Z","iopub.execute_input":"2025-01-09T12:53:29.778252Z","iopub.status.idle":"2025-01-09T12:53:34.578940Z","shell.execute_reply.started":"2025-01-09T12:53:29.778232Z","shell.execute_reply":"2025-01-09T12:53:34.578272Z"},"id":"IgJFcjXoa7Il"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:53:34.580214Z","iopub.execute_input":"2025-01-09T12:53:34.580734Z","iopub.status.idle":"2025-01-09T12:53:34.635184Z","shell.execute_reply.started":"2025-01-09T12:53:34.580700Z","shell.execute_reply":"2025-01-09T12:53:34.634371Z"},"id":"DpWbDwTja7Im"},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:53:34.636152Z","iopub.execute_input":"2025-01-09T12:53:34.636376Z","iopub.status.idle":"2025-01-09T12:53:34.654202Z","shell.execute_reply.started":"2025-01-09T12:53:34.636348Z","shell.execute_reply":"2025-01-09T12:53:34.653601Z"},"id":"kdvnV1Efa7Im","executionInfo":{"status":"ok","timestamp":1736007616811,"user_tz":-330,"elapsed":5,"user":{"displayName":"","userId":""}},"outputId":"802e1d3c-dc5d-41ac-d16a-f50ffb3dacf8","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"checkpoint = \"t5-base\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:53:34.654918Z","iopub.execute_input":"2025-01-09T12:53:34.655175Z","iopub.status.idle":"2025-01-09T12:53:34.672057Z","shell.execute_reply.started":"2025-01-09T12:53:34.655145Z","shell.execute_reply":"2025-01-09T12:53:34.671336Z"},"id":"OvfQPyhKa7In"},"outputs":[],"execution_count":6},{"cell_type":"code","source":"t5_tokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:53:34.672856Z","iopub.execute_input":"2025-01-09T12:53:34.673088Z","iopub.status.idle":"2025-01-09T12:53:36.449168Z","shell.execute_reply.started":"2025-01-09T12:53:34.673069Z","shell.execute_reply":"2025-01-09T12:53:36.448282Z"},"id":"EPXot-9Sa7In","executionInfo":{"status":"ok","timestamp":1736007620230,"user_tz":-330,"elapsed":3422,"user":{"displayName":"","userId":""}},"outputId":"c202dd01-1b26-4400-cd53-9c6f676a8c2a","colab":{"base_uri":"https://localhost:8080/","height":222,"referenced_widgets":["43da2f47170345cdac7f72b7aa84662d","89cfdd8672f548ca8d8722b5c39c240e","a4ecd892759c4a759d069e293e15b779","9aa1e481e9ad45678462f48428086ace","f3027b6fbe7048378dd09030b5bd55d5","7e0f7c835fd643c98686a30e7a7062fe","ad0642d1ee6e4d5381fb0fa34a4cf092","3d36ea6e57e142c1b40a79c8f4b8f6de","26e3066c9eb940c297966090d0d235d8","feee58e7de1340e3860c9a95ff4be60e","2976f27258574b5d867dfdcbe16f9769","749891e75dea4320a86201f0d4e19c16","3958951756e54804a17eae58aa6ba0dc","1255530944b14d6099c53a92b2bb2bff","eeb45b3f646a49f0bdee3d80c96685f5","e2f860e4136747159aa03f5d0a63d368","e539e4ce810e416986a5fa7f70494395","1d3f7e377c3348948b55485857e699f1","3add81bdfdc2431cb7e774e4fa914292","2f85aef5dcd64053acd7128663657eb0","a9fc7544d1fc4848a6161195104e1254","ee7e1d8f427d4b698a0ad3fa19904269","e04aa00ec6bf4c4cac5f87cf82a27c54","7ec5c7b696ff4d7098aa0681b6ca625c","9d530a87310d4c47bfa18a06fca19e17","35a0e3310300473b97150e41c0b23339","2d05e85d3811459e92522fe7c44cc95a","744b2bc8bdb346c79694d3164fb8ceec","d03e94b61e1840e3b7f0ec3c6ae37b73","da66806b1f8f4a8faacf076045688d5f","f000bf67102b4b0bb9322d272114958b","1770b5d95b514882bdc265bada08474f","6f0779e020d44f9396ca5294b3d3975a"]}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab864f1b87f04bcb9b72bbf39fb890cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feea6b5d648c4f019a863a1dcbac4868"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94ee6319a82f46dc95d0c85d6e81e19b"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"t5_tokenizer.vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:53:36.449920Z","iopub.execute_input":"2025-01-09T12:53:36.450150Z","iopub.status.idle":"2025-01-09T12:53:36.454874Z","shell.execute_reply.started":"2025-01-09T12:53:36.450113Z","shell.execute_reply":"2025-01-09T12:53:36.454148Z"},"id":"qX1l99_3a7Io","executionInfo":{"status":"ok","timestamp":1736007620230,"user_tz":-330,"elapsed":3,"user":{"displayName":"","userId":""}},"outputId":"803d67af-d77e-4ecf-8be7-b3813bdd5551","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"32100"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"#set the max length to model's default present max length\nt5_tokenizer.model_max_length = t5_tokenizer.model_max_length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:53:36.456719Z","iopub.execute_input":"2025-01-09T12:53:36.456940Z","iopub.status.idle":"2025-01-09T12:53:36.471308Z","shell.execute_reply.started":"2025-01-09T12:53:36.456921Z","shell.execute_reply":"2025-01-09T12:53:36.470722Z"},"id":"9XRWaOWWa7Ip"},"outputs":[],"execution_count":9},{"cell_type":"code","source":"gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:53:36.472216Z","iopub.execute_input":"2025-01-09T12:53:36.472505Z","iopub.status.idle":"2025-01-09T12:53:38.784845Z","shell.execute_reply.started":"2025-01-09T12:53:36.472484Z","shell.execute_reply":"2025-01-09T12:53:38.783911Z"},"id":"tLqjpzCxa7Ip","executionInfo":{"status":"ok","timestamp":1736007623206,"user_tz":-330,"elapsed":2269,"user":{"displayName":"","userId":""}},"outputId":"fd776e6d-245d-4ab9-cc78-99e969b0a830","colab":{"base_uri":"https://localhost:8080/","height":177,"referenced_widgets":["5b4b9829c79e4df2b7f6159d4a54634a","7574a2d7e4984e01ae8043545e7f3afb","b28caa6bf2c745ec87cf3da06ecf4a7a","cd57ad7ae6aa4ced911bf44e8b5437d0","5c3722f465704e12bd93bb12220b42e3","9c98216f497743dcb8abb7b69be97942","9c817b66deb1488b95479ae5667b71cf","f7e2b480b24244919ec20c23720905a7","214c9d061b734b03bd0060ef66fb8c02","770dcb4cf22d4035a8b8970aed70412b","17209e71fc73454b99b23f617c5643d0","3aa1fd17abe44555ad47e0aef4c0f938","1bd6bdbc2e204509b9c209417733f4a3","b8ac8ee34b73426190aebf20a0035eae","4cf598feee2b4c71938150e26c9cb341","0ceaa855fc724594bf8c64a8a1926e01","d80c6058f22a47cba18ed8e9eb1918ae","13f8d88212184e9889255c7e6f3f6dfd","de559a29729142c2ae3a9641c96478c4","6df5febabe3c4062ab1c055fbea5a62b","dfa0c3241c3f4d9088aa3a8ea8b45f70","ebcecc565e484bf0933b3910d4566726","3809873b8cf045c2a00ba0d62261f6af","3d86694625d04af094c9beef8a589b16","d5f9e809db154bbfaf908195b325a34a","fc953cc050dd4280846cd6e1219019e7","9c27818ce29f4d4c97bc4b55efad78e5","617a9be3796a4ec8a0377b82f497635e","8309e9644c074f83ab4f4fd9a99582c6","9d533ecc7ab649aaa98a0712db11376c","357500baa89547c39282e086ab7a7999","942420c6462c47ca8a0e09321e60a1b8","9f5ed38aa7a94a00b373febd383104ca","5e801240ba7b42378f27c301c0fcac29","7b0cabdc4da749228879868a6f7e34e3","9fabc1f28c5248d8a9ced22a2f37890c","95879c33ac2c4568a8b1fbe4f4004009","421fb5dee92b40e0b94ac250bdc13adc","bbf744dc7b674882b25a67ececf38fe3","ff2f0600478f4801a2243cb634722196","b0bffe6826a9406dad9fbc75e1dc99e2","3e1c977ccc074a42b090867b860297b9","3f73dfe68ceb4c2689ad142e0664340a","ebeab6709a6e40e58a661585be171203","44d5dfde2cea49ffb47e9fa82d182502","78c2f42c86e644469f1697ace4f1e0d1","bde9128d864947c7918a2192aa8bbdcb","cf1a5f16f250435082e836611374688a","c9185e9b796c4e14be13dc4ebb223e1b","7e24b589fab34b389b4b7b1bab26f624","5a6158b3e2934c5d9e2b15c2f6951031","aaa6919585e543969a70349ec8f870da","9140609d187f48869561947744cda081","7bb1f013da664d34a26d042c062d8336","ee1033c0ac7245358b4aaeb3ae3e2502"]}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e2d7775d7674f05a2a2a07da31b9476"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d77c976e2ae42a1a811a48e40cef374"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f26763bd5e9a4353a3910cbb6042d918"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97ea80b3c6004f0883a31db3d3a37ef1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c42e497a78094ff997c2cc2b54d793a1"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:53:38.785749Z","iopub.execute_input":"2025-01-09T12:53:38.785968Z","iopub.status.idle":"2025-01-09T12:53:38.789236Z","shell.execute_reply.started":"2025-01-09T12:53:38.785947Z","shell.execute_reply":"2025-01-09T12:53:38.788609Z"},"id":"LVVTv7zfa7Iq"},"outputs":[],"execution_count":11},{"cell_type":"code","source":"file_path = \"/kaggle/input/metacognitive-feedback-for-algorithm-solving/final_dataset_with_annotated_metacognitive_feedback_gpt-4o-mini.csv\"\ndf = pd.read_csv(file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:53:38.789854Z","iopub.execute_input":"2025-01-09T12:53:38.790037Z","iopub.status.idle":"2025-01-09T12:53:40.520598Z","shell.execute_reply.started":"2025-01-09T12:53:38.790021Z","shell.execute_reply":"2025-01-09T12:53:40.519677Z"},"id":"XrY_OzWca7Iq","executionInfo":{"status":"error","timestamp":1736007624024,"user_tz":-330,"elapsed":822,"user":{"displayName":"","userId":""}},"outputId":"3eb44ee1-99fa-46b6-becc-3de1a7578cb1","colab":{"base_uri":"https://localhost:8080/","height":316}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:53:40.521394Z","iopub.execute_input":"2025-01-09T12:53:40.521653Z","iopub.status.idle":"2025-01-09T12:53:40.527335Z","shell.execute_reply.started":"2025-01-09T12:53:40.521632Z","shell.execute_reply":"2025-01-09T12:53:40.526580Z"},"id":"nqzlVJm_a7Ir"},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Index(['Question 1', 'Response 1', 'Right answer 1', 'Q01', 'Q02', 'Q03',\n       'Q04', 'Q05', 'Q06', 'Q07', 'Q08', 'Q09', 'Q10', 'Q11', 'Q12', 'Q13',\n       'Q14', 'Q15', 'Q16', 'metacognitive_vector', 'metacognitive_feedback'],\n      dtype='object')"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"df.rename(\n    columns={\n        'Question 1': 'Problem',\n        'Response 1': 'Student_code',\n        'Right answer 1': 'Expected_code'\n    },\n    inplace=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:53:40.528146Z","iopub.execute_input":"2025-01-09T12:53:40.528436Z","iopub.status.idle":"2025-01-09T12:53:40.548777Z","shell.execute_reply.started":"2025-01-09T12:53:40.528407Z","shell.execute_reply":"2025-01-09T12:53:40.547945Z"},"id":"ugJYy8pVa7Ir"},"outputs":[],"execution_count":14},{"cell_type":"code","source":"df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:18.644807Z","iopub.execute_input":"2025-01-09T12:58:18.645114Z","iopub.status.idle":"2025-01-09T12:58:18.664879Z","shell.execute_reply.started":"2025-01-09T12:58:18.645092Z","shell.execute_reply":"2025-01-09T12:58:18.664215Z"},"id":"mcjarOz3a7Is"},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                             Problem  \\\n0  Develop a Python program that takes the name o...   \n1  Develop a Python program that takes the name o...   \n2  Develop a Python program that takes the name o...   \n\n                                        Student_code  \\\n0  file_input = input()      file_open = open(fil...   \n1  file_input = input()      file_open = open(fil...   \n2  file_input = input()      file_open = open(fil...   \n\n                                       Expected_code        Q01  \\\n0  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n1  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n2  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n\n             Q02               Q03        Q04            Q05            Q06  \\\n0  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n1  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n2  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n\n                Q07  ...        Q09        Q10            Q11            Q12  \\\n0  1 : Almost Never  ...  3 : Often  3 : Often  2 : Sometimes  2 : Sometimes   \n1  1 : Almost Never  ...  3 : Often  3 : Often  2 : Sometimes  2 : Sometimes   \n2  1 : Almost Never  ...  3 : Often  3 : Often  2 : Sometimes  2 : Sometimes   \n\n             Q13        Q14        Q15               Q16  \\\n0  2 : Sometimes  3 : Often  3 : Often  1 : Almost Never   \n1  2 : Sometimes  3 : Often  3 : Often  1 : Almost Never   \n2  2 : Sometimes  3 : Often  3 : Often  1 : Almost Never   \n\n                                metacognitive_vector  \\\n0  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n1  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n2  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n\n                              metacognitive_feedback  \n0  Your initial code serves as a starting point, ...  \n1  Your code exhibits a solid attempt at reading ...  \n2  It looks like you're in a good place with some...  \n\n[3 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Problem</th>\n      <th>Student_code</th>\n      <th>Expected_code</th>\n      <th>Q01</th>\n      <th>Q02</th>\n      <th>Q03</th>\n      <th>Q04</th>\n      <th>Q05</th>\n      <th>Q06</th>\n      <th>Q07</th>\n      <th>...</th>\n      <th>Q09</th>\n      <th>Q10</th>\n      <th>Q11</th>\n      <th>Q12</th>\n      <th>Q13</th>\n      <th>Q14</th>\n      <th>Q15</th>\n      <th>Q16</th>\n      <th>metacognitive_vector</th>\n      <th>metacognitive_feedback</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>Your initial code serves as a starting point, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>Your code exhibits a solid attempt at reading ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>It looks like you're in a good place with some...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 21 columns</p>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"df['combined_problem_student'] = df['Problem'] + \" \" + df['Student_code']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:20.671954Z","iopub.execute_input":"2025-01-09T12:58:20.672237Z","iopub.status.idle":"2025-01-09T12:58:20.812344Z","shell.execute_reply.started":"2025-01-09T12:58:20.672216Z","shell.execute_reply":"2025-01-09T12:58:20.811438Z"},"id":"Z7eLV0EQa7Is"},"outputs":[],"execution_count":16},{"cell_type":"code","source":"df['combined_problem_expected'] = df['Problem'] + \" \" + df['Expected_code']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:21.061350Z","iopub.execute_input":"2025-01-09T12:58:21.061659Z","iopub.status.idle":"2025-01-09T12:58:21.135866Z","shell.execute_reply.started":"2025-01-09T12:58:21.061633Z","shell.execute_reply":"2025-01-09T12:58:21.134967Z"},"id":"7GpCPwv8a7Is"},"outputs":[],"execution_count":17},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:21.993064Z","iopub.execute_input":"2025-01-09T12:58:21.993341Z","iopub.status.idle":"2025-01-09T12:58:21.998586Z","shell.execute_reply.started":"2025-01-09T12:58:21.993319Z","shell.execute_reply":"2025-01-09T12:58:21.997720Z"},"id":"-EMODMJCa7It"},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Index(['Problem', 'Student_code', 'Expected_code', 'Q01', 'Q02', 'Q03', 'Q04',\n       'Q05', 'Q06', 'Q07', 'Q08', 'Q09', 'Q10', 'Q11', 'Q12', 'Q13', 'Q14',\n       'Q15', 'Q16', 'metacognitive_vector', 'metacognitive_feedback',\n       'combined_problem_student', 'combined_problem_expected'],\n      dtype='object')"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"df.dropna(subset=['Problem', 'metacognitive_feedback', 'combined_problem_student'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:22.372379Z","iopub.execute_input":"2025-01-09T12:58:22.372658Z","iopub.status.idle":"2025-01-09T12:58:22.393235Z","shell.execute_reply.started":"2025-01-09T12:58:22.372634Z","shell.execute_reply":"2025-01-09T12:58:22.392555Z"},"id":"94NeCGFua7It"},"outputs":[],"execution_count":19},{"cell_type":"code","source":"df.reset_index(drop=True, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:23.362616Z","iopub.execute_input":"2025-01-09T12:58:23.362900Z","iopub.status.idle":"2025-01-09T12:58:23.366563Z","shell.execute_reply.started":"2025-01-09T12:58:23.362878Z","shell.execute_reply":"2025-01-09T12:58:23.365754Z"},"id":"9wKnq90Ua7Iu"},"outputs":[],"execution_count":20},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:23.567793Z","iopub.execute_input":"2025-01-09T12:58:23.568011Z","iopub.status.idle":"2025-01-09T12:58:23.594725Z","shell.execute_reply.started":"2025-01-09T12:58:23.567991Z","shell.execute_reply":"2025-01-09T12:58:23.594067Z"},"id":"ZzXH3PICa7Iu"},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Problem                      0\nStudent_code                 0\nExpected_code                0\nQ01                          0\nQ02                          0\nQ03                          0\nQ04                          0\nQ05                          0\nQ06                          0\nQ07                          0\nQ08                          0\nQ09                          0\nQ10                          0\nQ11                          0\nQ12                          0\nQ13                          0\nQ14                          0\nQ15                          0\nQ16                          0\nmetacognitive_vector         0\nmetacognitive_feedback       0\ncombined_problem_student     0\ncombined_problem_expected    0\ndtype: int64"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"df['metacognitive_feedback'][100]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:25.691506Z","iopub.execute_input":"2025-01-09T12:58:25.691839Z","iopub.status.idle":"2025-01-09T12:58:25.697200Z","shell.execute_reply.started":"2025-01-09T12:58:25.691815Z","shell.execute_reply":"2025-01-09T12:58:25.696349Z"},"id":"WtKweTkea7Iu"},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"\"Your current implementation shows a good effort in structuring your code with functions, but there are some areas where further refinement is necessary to meet the problem requirements. First, consider how you're capturing the relationships between birth years and heights. While you are correctly gathering names, birthdates, and heights into a dictionary, think about how you can aggregate heights by decade instead of year. This will streamline the calculation of average heights. It’s crucial to loop through your input list once to comprehend and process the data before beginning any calculations for averaging—possibly consolidating this logic in your `calculate_average_height` function. Also, ensure that you are converting heights to the correct data type before performing any arithmetic operations. Additionally, take a closer look at how you’re determining the range of decades; right now it seems like you may be focusing on unique years instead of decades. Consider creating a systematic structure that easily categorizes each height into its corresponding decade bucket. Furthermore, as you develop your code, remember to monitor your program's flow and adjust accordingly—this is especially important when it comes to function calls and data structure manipulations. Establishing a plan before implementation can significantly enhance your problem-solving process and avoid errors stemming from assumptions. Overall, maintain momentum and continue refining your approach by checking each component against the problem's requirements, thus ensuring coherence and completeness in your solution.\""},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:26.020340Z","iopub.execute_input":"2025-01-09T12:58:26.020605Z","iopub.status.idle":"2025-01-09T12:58:26.039774Z","shell.execute_reply.started":"2025-01-09T12:58:26.020583Z","shell.execute_reply":"2025-01-09T12:58:26.039055Z"},"id":"t_EBOaYQa7Iv"},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"                                             Problem  \\\n0  Develop a Python program that takes the name o...   \n1  Develop a Python program that takes the name o...   \n2  Develop a Python program that takes the name o...   \n3  Develop a Python program that takes the name o...   \n4  Develop a Python program that takes the name o...   \n\n                                        Student_code  \\\n0  file_input = input()      file_open = open(fil...   \n1  file_input = input()      file_open = open(fil...   \n2  file_input = input()      file_open = open(fil...   \n3  file_input = input()      file_open = open(fil...   \n4  file_input = input()      file_open = open(fil...   \n\n                                       Expected_code        Q01  \\\n0  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n1  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n2  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n3  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n4  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n\n             Q02               Q03        Q04            Q05            Q06  \\\n0  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n1  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n2  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n3  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n4  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n\n                Q07  ...            Q11            Q12            Q13  \\\n0  1 : Almost Never  ...  2 : Sometimes  2 : Sometimes  2 : Sometimes   \n1  1 : Almost Never  ...  2 : Sometimes  2 : Sometimes  2 : Sometimes   \n2  1 : Almost Never  ...  2 : Sometimes  2 : Sometimes  2 : Sometimes   \n3  1 : Almost Never  ...  2 : Sometimes  2 : Sometimes  2 : Sometimes   \n4  1 : Almost Never  ...  2 : Sometimes  2 : Sometimes  2 : Sometimes   \n\n         Q14        Q15               Q16  \\\n0  3 : Often  3 : Often  1 : Almost Never   \n1  3 : Often  3 : Often  1 : Almost Never   \n2  3 : Often  3 : Often  1 : Almost Never   \n3  3 : Often  3 : Often  1 : Almost Never   \n4  3 : Often  3 : Often  1 : Almost Never   \n\n                                metacognitive_vector  \\\n0  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n1  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n2  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n3  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n4  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n\n                              metacognitive_feedback  \\\n0  Your initial code serves as a starting point, ...   \n1  Your code exhibits a solid attempt at reading ...   \n2  It looks like you're in a good place with some...   \n3  Your approach to reading the file and splittin...   \n4  Your initial approach to the problem is a good...   \n\n                            combined_problem_student  \\\n0  Develop a Python program that takes the name o...   \n1  Develop a Python program that takes the name o...   \n2  Develop a Python program that takes the name o...   \n3  Develop a Python program that takes the name o...   \n4  Develop a Python program that takes the name o...   \n\n                           combined_problem_expected  \n0  Develop a Python program that takes the name o...  \n1  Develop a Python program that takes the name o...  \n2  Develop a Python program that takes the name o...  \n3  Develop a Python program that takes the name o...  \n4  Develop a Python program that takes the name o...  \n\n[5 rows x 23 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Problem</th>\n      <th>Student_code</th>\n      <th>Expected_code</th>\n      <th>Q01</th>\n      <th>Q02</th>\n      <th>Q03</th>\n      <th>Q04</th>\n      <th>Q05</th>\n      <th>Q06</th>\n      <th>Q07</th>\n      <th>...</th>\n      <th>Q11</th>\n      <th>Q12</th>\n      <th>Q13</th>\n      <th>Q14</th>\n      <th>Q15</th>\n      <th>Q16</th>\n      <th>metacognitive_vector</th>\n      <th>metacognitive_feedback</th>\n      <th>combined_problem_student</th>\n      <th>combined_problem_expected</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>Your initial code serves as a starting point, ...</td>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>Develop a Python program that takes the name o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>Your code exhibits a solid attempt at reading ...</td>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>Develop a Python program that takes the name o...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>It looks like you're in a good place with some...</td>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>Develop a Python program that takes the name o...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>Your approach to reading the file and splittin...</td>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>Develop a Python program that takes the name o...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>Your initial approach to the problem is a good...</td>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>Develop a Python program that takes the name o...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 23 columns</p>\n</div>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport ast\nclass CustomDataset(Dataset):\n    def __init__(self, dataset, t5_tokenizer,gpt2_tokenizer, max_length=512):\n        self.t5_tokenizer = t5_tokenizer\n        self.gpt2_tokenizer = gpt2_tokenizer\n        self.data = dataset\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        metacognitive_vector = self.data['metacognitive_vector'][idx]\n        problem_student_code = self.data['combined_problem_student'][idx]\n        problem_expected_code = self.data['combined_problem_expected'][idx]\n        student_code = self.data['Student_code'][idx]\n        target = self.data['metacognitive_feedback'][idx]\n\n        metacognitive_vector_float = [\n        float(item.strip()) for item in ast.literal_eval(metacognitive_vector)]\n        metacognition_vector_ids = torch.tensor(metacognitive_vector_float, dtype=torch.float)\n\n        problem_student_code_ids = torch.tensor(\n            self.t5_tokenizer.encode(problem_student_code, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n        problem_expected_code_ids = torch.tensor(\n            self.t5_tokenizer.encode(problem_expected_code, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n\n        student_code_ids = torch.tensor(\n            self.t5_tokenizer.encode(student_code, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n        target_ids = torch.tensor(\n            self.t5_tokenizer.encode(target, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n\n        return metacognition_vector_ids, problem_student_code_ids, problem_expected_code_ids, student_code_ids, target_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:27.052486Z","iopub.execute_input":"2025-01-09T12:58:27.052824Z","iopub.status.idle":"2025-01-09T12:58:27.060665Z","shell.execute_reply.started":"2025-01-09T12:58:27.052797Z","shell.execute_reply":"2025-01-09T12:58:27.059588Z"},"id":"nNEuM_O3a7Iv"},"outputs":[],"execution_count":24},{"cell_type":"code","source":"dataset = CustomDataset(df, t5_tokenizer, gpt2_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:28.261744Z","iopub.execute_input":"2025-01-09T12:58:28.262050Z","iopub.status.idle":"2025-01-09T12:58:28.265600Z","shell.execute_reply.started":"2025-01-09T12:58:28.262021Z","shell.execute_reply":"2025-01-09T12:58:28.264698Z"},"id":"Xjf6vXzJa7Iv"},"outputs":[],"execution_count":25},{"cell_type":"code","source":"len(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:29.472851Z","iopub.execute_input":"2025-01-09T12:58:29.473122Z","iopub.status.idle":"2025-01-09T12:58:29.477777Z","shell.execute_reply.started":"2025-01-09T12:58:29.473101Z","shell.execute_reply":"2025-01-09T12:58:29.477080Z"},"id":"qmSPRdvNa7Iw"},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"16803"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"metacognition_vector_ids, problem_student_code_ids, problem_expected_code_ids, student_code_ids, target_ids = dataset[100]\nprint(f\"Metacognition vector IDs: {metacognition_vector_ids}\")\nprint(f\"Expected feedback IDs: {problem_student_code_ids.shape}\")\nprint(f\"Expected encoded feedback IDs: {problem_expected_code_ids.shape}\")\nprint(f\"Student Answer IDs: {student_code_ids}\")\nprint(f\"Target IDs: {target_ids}\")\nprint(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:29.904364Z","iopub.execute_input":"2025-01-09T12:58:29.904617Z","iopub.status.idle":"2025-01-09T12:58:29.994763Z","shell.execute_reply.started":"2025-01-09T12:58:29.904596Z","shell.execute_reply":"2025-01-09T12:58:29.993974Z"},"id":"n5JGeIpza7Ix","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Metacognition vector IDs: tensor([2., 3., 1., 3., 3., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3., 2.])\nExpected feedback IDs: torch.Size([512])\nExpected encoded feedback IDs: torch.Size([512])\nStudent Answer IDs: tensor([ 3785,   834, 11966,  4350,  2423,    77,  2562,  9960,  1713,   188,\n         2176,    15,     6,  2122, 31911,  2294,  2394,     6,  2938, 27436,\n         1713,   279,    32,   115,     6,  4928, 20223, 13523,  4433,     6,\n        27640,     5,   357,  1713, 18610,   760,   630,     6,  2884, 31497,\n         2294,  3072,     6,  2938, 19419,  1713,   308,     9,  6961,     6,\n         1714,    87,  4305, 13523,  3301,     6,  2606, 16029,  1713,   427,\n          162,     6,  2517,    87,  5176, 13523,  2079,     6,  2517, 12100,\n           20,    89,   608,   834, 11966,   599,    77,  2562,   834, 11966,\n         4350,    61,    10,    28,   539,   599,    77,  2562,   834, 11966,\n         4350,   976,    52,  8512,    38,  1042,    10, 10223,  2423, 11966,\n            5,  5236,  6972,  9960,  3785,   834,  3350,  2423,  6306,   908,\n           21,   331,    16, 10223,    10,  3785,   834,  3350,     5,  3096,\n          989,   599,  6757,     5,     7, 14192,  9960,     5,     7,  5900,\n           17,   599,  1686,  8512,    61,  1205,  3785,   834,  3350,    20,\n           89, 11837,   834, 28951,   834,    88,  2632,   599,  1201,  4347,\n         1201,  7318,    10,  1903,  3785,   834,  3350,  2423,  5236,   834,\n        11966,   599,    77,  2562,   834, 11966,  4350,    61,   331,   834,\n         4370,  2423,     2,   215,   834,  3350,  2423,  6306,   908,    21,\n            3,    23,    16,   620,   599,    40,    35,   599,    77,  2562,\n          834,  3350,    61,    61,    10,   331,   834,  4370,  6306,    77,\n         2562,   834,  3350,  6306,    23,   908,  6306,   536,  4275,     7,\n         5900,    17,   599,   121,    87,  8512,  6306,   357,   908,   908,\n         2423,    77,  2562,   834,  3350,  6306,    23,   908,  6306,   357,\n          908,  2281,   599,  6757,   834,  4370,    61,    20,    89,  3519,\n          834,  1201,   599,  4370,    61,    10,  3519,   834,  1201,  2423,\n          567,   782,    21,   843,    16,     3,  4370,    10,     3,    99,\n         3519,   834,  1201,  2423,  2423,   567,   782,    10,  3519,   834,\n         1201,  2423,  4397,     3,    15,    40,    99,  3519,   834,  1201,\n         3155,  4397,    10,  3519,   834,  1201,  2423,  4397,  1205,  3519,\n          834,  1201,    20,    89,  9858,   834,  1201,   599,  4370,    61,\n           10,  9858,   834,  1201,  2423,   567,   782,    21,   843,    16,\n            3,  4370,    10,     3,    99,  9858,   834,  1201,  2423,  2423,\n          567,   782,    10,  9858,   834,  1201,  2423,  4397,     3,    15,\n           40,    99,  9858,   834,  1201,     1,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0])\nTarget IDs: tensor([  696,   750,  4432,  1267,     3,     9,   207,  1941,    16, 21055,\n         1725,    39,  1081,    28,  3621,     6,    68,   132,    33,   128,\n          844,   213,   856, 15489,   297,    19,  1316,    12,   942,     8,\n          682,  1502,     5,  1485,     6,  1099,   149,    25,    31,    60,\n            3, 18147,     8,  3079,   344,  3879,   203,    11,  3902,     7,\n            5,   818,    25,    33,  6549,  7241,  3056,     6,  3879,  5522,\n            7,     6,    11,  3902,     7,   139,     3,     9, 24297,     6,\n          317,    81,   149,    25,    54, 12955,  3902,     7,    57,  5112,\n         1446,    13,   215,     5,   100,    56, 24104,     8, 18643,    13,\n         1348,  3902,     7,     5,    94,    22,     7,  4462,    12,  6494,\n          190,    39,  3785,   570,   728,    12, 21494,    11,   433,     8,\n          331,   274,  1849,   136, 19868,    21,     3,     9, 23980,   318,\n         2748,     7, 15596, 13250,  1014,    48,  9769,    16,    39,     3,\n            2, 10379, 14270,   834, 28951,   834,    88,  2632,     2,  1681,\n            5,  1203,     6,   766,    24,    25,    33,     3, 21049,  3902,\n            7,    12,     8,  2024,   331,   686,   274,  5505,   136,     3,\n            9, 30922,    51,  7578,  2673,     5,  5433,     6,   240,     3,\n            9,  4645,   320,    44,   149,    25,    22,    60,     3, 11682,\n            8,   620,    13,  4160,   117,   269,   230,    34,  1330,   114,\n           25,   164,    36,     3,  7388,    30,   775,   203,  1446,    13,\n         4160,     5,  9151,  1577,     3,     9, 20036,  1809,    24,  1153,\n         9624, 11498,   776,     7,   284,  3902,   139,   165,     3,  9921,\n         5112, 11325,     5,  7053,     6,    38,    25,  1344,    39,  1081,\n            6,  1423,    12,  3393,    39,   478,    31,     7,  2537,    11,\n         6142, 14031,   318,  8048,    19,   902,   359,   116,    34,   639,\n           12,  1681,  3088,    11,   331,  1809, 18175,     7,     5, 26550,\n           53,     3,     9,   515,   274,  4432,    54,  4019,  3391,    39,\n          682,    18,  6065,    53,   433,    11,  1792,  6854,  6269,    51,\n           53,    45, 20298,     5,  9126,     6,  1961, 15290,    11,   916,\n         6273,    77,    53,    39,  1295,    57,  6450,   284,  3876,   581,\n            8,   682,    31,     7,  1502,     6,  2932,     3,  5833,   576,\n          760,  1433,    11,   743,   655,    16,    39,  1127,     5,     1,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0])\n\n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"gpt2_tokenizer.pad_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:36.707432Z","iopub.execute_input":"2025-01-09T12:58:36.707787Z","iopub.status.idle":"2025-01-09T12:58:36.712359Z","shell.execute_reply.started":"2025-01-09T12:58:36.707759Z","shell.execute_reply":"2025-01-09T12:58:36.711675Z"},"id":"6Wlser8Ba7Iy"},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"'<|endoftext|>'"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"gpt2_pad_token_id = gpt2_tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:37.072204Z","iopub.execute_input":"2025-01-09T12:58:37.072429Z","iopub.status.idle":"2025-01-09T12:58:37.075927Z","shell.execute_reply.started":"2025-01-09T12:58:37.072410Z","shell.execute_reply":"2025-01-09T12:58:37.075018Z"},"id":"cnDxXCGxa7Iy"},"outputs":[],"execution_count":29},{"cell_type":"code","source":"gpt2_pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:38.064936Z","iopub.execute_input":"2025-01-09T12:58:38.065238Z","iopub.status.idle":"2025-01-09T12:58:38.069960Z","shell.execute_reply.started":"2025-01-09T12:58:38.065211Z","shell.execute_reply":"2025-01-09T12:58:38.069314Z"},"id":"5LAI9-sZa7Iy"},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"50256"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"t5_tokenizer.pad_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:38.401838Z","iopub.execute_input":"2025-01-09T12:58:38.402138Z","iopub.status.idle":"2025-01-09T12:58:38.406782Z","shell.execute_reply.started":"2025-01-09T12:58:38.402106Z","shell.execute_reply":"2025-01-09T12:58:38.406139Z"},"id":"hEyynFuNa7Iz"},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"'<pad>'"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"t5_pad_token_id = t5_tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:38.791907Z","iopub.execute_input":"2025-01-09T12:58:38.792114Z","iopub.status.idle":"2025-01-09T12:58:38.795236Z","shell.execute_reply.started":"2025-01-09T12:58:38.792096Z","shell.execute_reply":"2025-01-09T12:58:38.794589Z"},"id":"sGPK7f0ba7Iz"},"outputs":[],"execution_count":32},{"cell_type":"code","source":"t5_pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:39.951444Z","iopub.execute_input":"2025-01-09T12:58:39.951761Z","iopub.status.idle":"2025-01-09T12:58:39.956248Z","shell.execute_reply.started":"2025-01-09T12:58:39.951736Z","shell.execute_reply":"2025-01-09T12:58:39.955602Z"},"id":"nWn7op96a7I0"},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":33},{"cell_type":"markdown","source":"# Context Enocder","metadata":{"id":"HK7ybN62a7I0"}},{"cell_type":"code","source":"class ContextEncoder(nn.Module):\n    def __init__(self, t5_model_name='t5-base', output_dim=768 , max_length = 512):\n        super(ContextEncoder, self).__init__()\n\n        self.max_length = max_length\n\n        self.t5_encoder = T5Model.from_pretrained(t5_model_name).encoder\n        self.t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n        self.fc = nn.Linear(self.t5_encoder.config.d_model, output_dim)\n\n    def forward(self, problem_code_ids, attention_masks=None, prompt=\"\"):\n\n        problem_code = [self.t5_tokenizer.decode(ids, skip_special_tokens=True) for ids in problem_code_ids]\n\n        combined_text = prompt + \" \" + \" \".join(problem_code)\n\n        encoded = self.t5_tokenizer(\n            combined_text,\n            max_length=self.max_length, \n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        ).to(problem_code_ids.device)\n\n        encoder_outputs = self.t5_encoder(\n            input_ids=encoded[\"input_ids\"],\n            attention_mask=encoded[\"attention_mask\"]\n        )\n\n        context_hidden_states = encoder_outputs.last_hidden_state\n\n        context_rep = context_hidden_states.mean(dim=1)\n\n        # decoded_combined = [\n        # self.t5_tokenizer.decode(ids, skip_special_tokens=True)\n        # for ids in encoded_outputs]\n\n        context_rep = self.fc(context_rep)\n        final_rep = context_rep.unsqueeze(1)\n\n        return final_rep","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:41.396423Z","iopub.execute_input":"2025-01-09T12:58:41.396786Z","iopub.status.idle":"2025-01-09T12:58:41.402770Z","shell.execute_reply.started":"2025-01-09T12:58:41.396757Z","shell.execute_reply":"2025-01-09T12:58:41.401942Z"},"id":"epmjfCO_a7I2"},"outputs":[],"execution_count":34},{"cell_type":"code","source":"context_encoder = ContextEncoder().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:42.809881Z","iopub.execute_input":"2025-01-09T12:58:42.810144Z","iopub.status.idle":"2025-01-09T12:58:48.545851Z","shell.execute_reply.started":"2025-01-09T12:58:42.810124Z","shell.execute_reply":"2025-01-09T12:58:48.544908Z"},"id":"Fnjz8vrra7I2"},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f2a843c1642474d98b7589cdfb7ff5f"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"context_encoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:48.547035Z","iopub.execute_input":"2025-01-09T12:58:48.547361Z","iopub.status.idle":"2025-01-09T12:58:48.553206Z","shell.execute_reply.started":"2025-01-09T12:58:48.547330Z","shell.execute_reply":"2025-01-09T12:58:48.552566Z"},"id":"GCZmPBnSa7I2"},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"ContextEncoder(\n  (t5_encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (fc): Linear(in_features=768, out_features=768, bias=True)\n)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"\nclass MetacognitionLayer(nn.Module):\n    def __init__(self, metacognitive_dim=16, output_dim=768):\n        super(MetacognitionLayer, self).__init__()\n        #16 to 768 mapping\n        self.metacognitive_fc = nn.Linear(metacognitive_dim, output_dim)\n        self.final_fc = nn.Linear(output_dim, output_dim)\n\n        self.tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n        self.t5_pad_token_id = self.tokenizer.pad_token_id\n        self.encoder = T5Model.from_pretrained(\"t5-base\").encoder\n\n\n\n        for param in self.encoder.parameters():\n            param.requires_grad = False\n\n    def forward(self, metacognitive_vector):\n\n        metacognitive_components = [\n                \"Read\", \"Identify\", \"Rephrase\", \"Examples\", \"Breakdown\", \"Estimate\", \"Plan\",\n                \"Revise\", \"Verify\", \"AvoidMistakes\", \"MonitorSteps\", \"MonitorProcess\",\n                \"ValidateConstraints\", \"Confirm\", \"CheckRequirements\", \"Reflect\"\n            ]\n\n        prompt_text = (\n            f\"Metacognitive feedback helps students reflect on their algorithm-solving strategies. \"\n            f\"The passed metacognitive vector is a 16-dimensional vector describing the student's metacognition profile, \"\n            f\"where each dimension represents one of 16 metacognitive components, rated as 1 (Almost never), \"\n            f\"2 (Sometimes), or 3 (Often). These components include: {', '.join(metacognitive_components)}.\"\n        )\n\n\n        input_prompt = self.tokenizer(\n        prompt_text,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=512-16\n        )\n        input_prompt = {key: value.to(metacognitive_vector.device) for key, value in input_prompt.items()}\n        \n        outputs = self.encoder(input_ids=input_prompt[\"input_ids\"], attention_mask=input_prompt[\"attention_mask\"])\n        prompt_embedding = outputs.last_hidden_state.mean(dim=1)\n        prompt_embedding = prompt_embedding.to(metacognitive_vector.device)\n\n        metacognitive_rep = self.metacognitive_fc(metacognitive_vector)\n        final_rep = self.final_fc(metacognitive_rep)\n        persona_rep = final_rep.unsqueeze(1)\n       \n        final_persona = persona_rep + prompt_embedding\n\n        return final_persona","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:48.554468Z","iopub.execute_input":"2025-01-09T12:58:48.554698Z","iopub.status.idle":"2025-01-09T12:58:48.571272Z","shell.execute_reply.started":"2025-01-09T12:58:48.554679Z","shell.execute_reply":"2025-01-09T12:58:48.570594Z"},"id":"ZX97CJgLa7I2"},"outputs":[],"execution_count":37},{"cell_type":"code","source":"metacognitive_emb = MetacognitionLayer().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:48.572533Z","iopub.execute_input":"2025-01-09T12:58:48.572791Z","iopub.status.idle":"2025-01-09T12:58:49.819722Z","shell.execute_reply.started":"2025-01-09T12:58:48.572772Z","shell.execute_reply":"2025-01-09T12:58:49.819009Z"},"id":"x5pQ-96_a7JA"},"outputs":[],"execution_count":38},{"cell_type":"code","source":"metacognitive_emb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:49.820448Z","iopub.execute_input":"2025-01-09T12:58:49.820690Z","iopub.status.idle":"2025-01-09T12:58:49.826242Z","shell.execute_reply.started":"2025-01-09T12:58:49.820669Z","shell.execute_reply":"2025-01-09T12:58:49.825590Z"},"id":"BQMfinGKa7JA"},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"MetacognitionLayer(\n  (metacognitive_fc): Linear(in_features=16, out_features=768, bias=True)\n  (final_fc): Linear(in_features=768, out_features=768, bias=True)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n)"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"\nclass PAALayer(nn.Module):\n    def __init__(self, hidden_dimension = 768 , tau=0.8,dropout_rate=0.1):\n        super(PAALayer, self).__init__()\n        self.hidden_dimenstion = hidden_dimension\n        self.tau = tau\n\n\n        self.fc = nn.Linear(2 * hidden_dimension, hidden_dimension)\n        self.sigmoid = nn.Sigmoid()\n        self.fc_out = nn.Linear(hidden_dimension, hidden_dimension)\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n\n    def forward(self, hR , oP, oC):\n\n        Mp_input  = torch.cat([hR,oP], dim=-1)\n        Mp = self.fc(Mp_input)\n        Wp = self.sigmoid(Mp)\n\n        Mpersona = Wp\n        Mcontext = 1 - Wp\n\n        oP_weighted = Mpersona * oP\n        oC_weighted = Mcontext * oC\n\n        HPAA = oP_weighted + oC_weighted\n\n        output = self.fc_out(HPAA)\n\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:49.826866Z","iopub.execute_input":"2025-01-09T12:58:49.827044Z","iopub.status.idle":"2025-01-09T12:58:49.839675Z","shell.execute_reply.started":"2025-01-09T12:58:49.827029Z","shell.execute_reply":"2025-01-09T12:58:49.839051Z"},"id":"2Z_xRbhNa7JA"},"outputs":[],"execution_count":40},{"cell_type":"code","source":"paa = PAALayer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:49.840548Z","iopub.execute_input":"2025-01-09T12:58:49.840815Z","iopub.status.idle":"2025-01-09T12:58:49.870655Z","shell.execute_reply.started":"2025-01-09T12:58:49.840788Z","shell.execute_reply":"2025-01-09T12:58:49.870044Z"},"id":"E6kRw4dea7JB"},"outputs":[],"execution_count":41},{"cell_type":"code","source":"paa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:49.872592Z","iopub.execute_input":"2025-01-09T12:58:49.872787Z","iopub.status.idle":"2025-01-09T12:58:49.877235Z","shell.execute_reply.started":"2025-01-09T12:58:49.872770Z","shell.execute_reply":"2025-01-09T12:58:49.876400Z"},"id":"T3pInXDya7JB","outputId":"66ad94da-6b56-4525-e2b2-459d4aa68c9d"},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"PAALayer(\n  (fc): Linear(in_features=1536, out_features=768, bias=True)\n  (sigmoid): Sigmoid()\n  (fc_out): Linear(in_features=768, out_features=768, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"class CustomTransformerBlock(nn.Module):\n    def __init__(self, hidden_size, tau , dropout_rate=0.1):\n        super(CustomTransformerBlock, self).__init__()\n\n        self.input_self_attention = nn.MultiheadAttention(hidden_size, num_heads=12, batch_first=True)\n        self.context_attn = nn.MultiheadAttention(hidden_size, num_heads=12, batch_first=True)\n        self.persona_attn = nn.MultiheadAttention(hidden_size, num_heads=12, batch_first=True)\n        #self.output_attn = nn.MultiheadAttention(hidden_size, num_head = 12, batch_first = True)\n\n        #self.output_self_attention = nn.MultiheadAttention(hidden_size, num_heads=12 , batch_first = True)\n\n        self.paa_layer = PAALayer(hidden_dimension=hidden_size, tau=tau)\n\n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_size, 2048),\n            nn.ReLU(),\n            nn.Dropout(p=0.1),\n            nn.Linear(2048, hidden_size),\n            nn.LayerNorm(hidden_size)\n        )\n        self.layer_norm2 = nn.LayerNorm(hidden_size)\n\n    def forward(self, student_initial_state, encoded_persona, encoded_context):\n\n        # print(\"hr before self attention\", student_initial_state.shape)\n        # expanded_attention_mask = attention_mask.unsqueeze(1).repeat(1, seq_len, 1)\n\n        hR, _ = self.input_self_attention(student_initial_state, student_initial_state, student_initial_state) #query\n        # print(\"encoded_persona\",encoded_persona.shape)\n        oP, _ = self.persona_attn(hR, encoded_persona, encoded_persona )\n        # print(\"attention persona\",oP.shape)\n        # print(\"hr after self attention\",hR.shape)\n        # print(\"encoded context-1\",encoded_context.shape)\n        encoded_context = encoded_context.repeat(hR.size(0), hR.size(1), 1)\n        # print(\"encoded context\",encoded_context.shape)\n        oC, _ = self.context_attn(hR, encoded_context, encoded_context )\n\n\n        HPAA = self.paa_layer(hR, oP, oC)\n\n        mlp_output = self.mlp(HPAA)\n        output = self.layer_norm2(mlp_output)\n\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:49.878228Z","iopub.execute_input":"2025-01-09T12:58:49.878510Z","iopub.status.idle":"2025-01-09T12:58:49.893195Z","shell.execute_reply.started":"2025-01-09T12:58:49.878483Z","shell.execute_reply":"2025-01-09T12:58:49.892406Z"},"id":"wDhvR6Goa7JB"},"outputs":[],"execution_count":43},{"cell_type":"code","source":"custom_layer = CustomTransformerBlock(768,0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:49.893980Z","iopub.execute_input":"2025-01-09T12:58:49.894182Z","iopub.status.idle":"2025-01-09T12:58:50.006100Z","shell.execute_reply.started":"2025-01-09T12:58:49.894159Z","shell.execute_reply":"2025-01-09T12:58:50.005278Z"},"id":"v_D6juUna7JC"},"outputs":[],"execution_count":44},{"cell_type":"code","source":"custom_layer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:50.007182Z","iopub.execute_input":"2025-01-09T12:58:50.007450Z","iopub.status.idle":"2025-01-09T12:58:50.012125Z","shell.execute_reply.started":"2025-01-09T12:58:50.007429Z","shell.execute_reply":"2025-01-09T12:58:50.011492Z"},"id":"nwq9pZy6a7JC","outputId":"6fbb8e91-749a-4ef8-a24e-c2313f8dea45"},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"CustomTransformerBlock(\n  (input_self_attention): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (context_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (persona_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (paa_layer): PAALayer(\n    (fc): Linear(in_features=1536, out_features=768, bias=True)\n    (sigmoid): Sigmoid()\n    (fc_out): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (mlp): Sequential(\n    (0): Linear(in_features=768, out_features=2048, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.1, inplace=False)\n    (3): Linear(in_features=2048, out_features=768, bias=True)\n    (4): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n)"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"class PAAModel(nn.Module):\n    def __init__(self, hidden_size=768, vocab_size = 32100 ,tau=0.8, max_length=512, num_transformer_blocks=4):\n        super(PAAModel , self).__init__()\n        self.hidden_size = hidden_size\n        self.tau = tau\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n        self.num_transformer_blocks = num_transformer_blocks\n\n        self.t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n\n        self.context_encoder = ContextEncoder()\n\n        self.metacognitive_emb = MetacognitionLayer()\n\n        for param in self.context_encoder.parameters():\n            param.requires_grad = False\n\n        for param in self.metacognitive_emb.parameters():\n            param.requires_grad = False\n\n        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n        self.position_embedding = nn.Embedding(max_length, hidden_size)\n        self.dropout = nn.Dropout(p=0.1)        \n\n        self.self_attention = nn.MultiheadAttention(hidden_size, num_heads=12, batch_first=True)\n        self.transformer_blocks = nn.ModuleList([CustomTransformerBlock(hidden_size, tau) for _ in range(num_transformer_blocks)])\n        self.final_fc = nn.Linear(hidden_size, vocab_size)\n        self.layer_norm = nn.LayerNorm(hidden_size)\n\n    def forward(self,  metacognitive_vector_ids,\n                       problem_student_code_ids ,\n                       problem_expected_code_ids ,\n                       expected_attention_mask,\n                       student_attention_mask):\n\n        metacognitive_vector_emb = self.metacognitive_emb(metacognitive_vector_ids)\n\n\n        expected_code_prompt= \"Here this is the combination of the algorithm problem and the expected correct answer code.\"\n        problem_expected_encoded = self.context_encoder(problem_expected_code_ids,\n                                                       attention_masks = expected_attention_mask,prompt=expected_code_prompt)\n\n\n        student_code_prompt=\"Here this is a combination of the algorithm problem and the student solved answer code.\"\n        prompt_ids_student = torch.tensor(\n                                self.t5_tokenizer.encode(student_code_prompt, max_length=self.max_length, truncation=True, padding=\"max_length\")).to(device)\n        prompt_ids_student = prompt_ids_student.unsqueeze(0).expand(problem_student_code_ids.size(0), -1)\n        # print(\"prompt shape\" ,prompt_ids_student.shape)\n        # print(\"problem student shape\",problem_student_code_ids.shape)\n        combined_student_ids = torch.cat([prompt_ids_student, problem_student_code_ids], dim=1)\n        combined_student_ids = combined_student_ids[:, :self.max_length]\n        # print(\"concat prompt student shape\", combined_student_ids.shape)\n        seq_length = combined_student_ids.size(1)\n        token_embeds = self.token_embedding(combined_student_ids)\n        position_ids = torch.arange(0, 1, device=combined_student_ids.device).unsqueeze(0)\n        position_ids = position_ids % self.max_length\n        # print(\"position ids\",len(position_ids))\n        position_embeds = self.position_embedding(position_ids)\n\n        inputs_embeds = token_embeds + position_embeds\n        inputs_embeds = self.dropout(inputs_embeds)\n        combined_attention_mask = (combined_student_ids != self.t5_tokenizer.pad_token_id).bool().to(device)\n        \n        student_initial_state = inputs_embeds\n        transformer_output = student_initial_state\n        # print(\"transformer output shape :\", transformer_output.shape)\n        for transformer_block in self.transformer_blocks:\n            transformer_output = transformer_block(transformer_output, metacognitive_vector_emb, problem_expected_encoded)\n\n        logits = self.final_fc(transformer_output)\n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:50.648840Z","iopub.execute_input":"2025-01-09T12:58:50.649061Z","iopub.status.idle":"2025-01-09T12:58:50.658055Z","shell.execute_reply.started":"2025-01-09T12:58:50.649042Z","shell.execute_reply":"2025-01-09T12:58:50.657220Z"},"id":"YSUMxJOYa7JC"},"outputs":[],"execution_count":46},{"cell_type":"code","source":"model = PAAModel()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:52.192596Z","iopub.execute_input":"2025-01-09T12:58:52.192879Z","iopub.status.idle":"2025-01-09T12:58:55.641380Z","shell.execute_reply.started":"2025-01-09T12:58:52.192858Z","shell.execute_reply":"2025-01-09T12:58:55.640404Z"},"id":"AROFcsBVa7JD"},"outputs":[],"execution_count":47},{"cell_type":"code","source":"model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:55.642605Z","iopub.execute_input":"2025-01-09T12:58:55.642870Z","iopub.status.idle":"2025-01-09T12:58:56.069225Z","shell.execute_reply.started":"2025-01-09T12:58:55.642848Z","shell.execute_reply":"2025-01-09T12:58:56.068552Z"},"id":"ukrbmnT1a7JD","outputId":"6e88007a-2a1a-4a99-b9e0-a11441cfa0f1"},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"PAAModel(\n  (context_encoder): ContextEncoder(\n    (t5_encoder): T5Stack(\n      (embed_tokens): Embedding(32128, 768)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n                (relative_attention_bias): Embedding(32, 12)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=768, out_features=3072, bias=False)\n                (wo): Linear(in_features=3072, out_features=768, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-11): 11 x T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=768, out_features=3072, bias=False)\n                (wo): Linear(in_features=3072, out_features=768, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (fc): Linear(in_features=768, out_features=768, bias=True)\n  )\n  (metacognitive_emb): MetacognitionLayer(\n    (metacognitive_fc): Linear(in_features=16, out_features=768, bias=True)\n    (final_fc): Linear(in_features=768, out_features=768, bias=True)\n    (encoder): T5Stack(\n      (embed_tokens): Embedding(32128, 768)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n                (relative_attention_bias): Embedding(32, 12)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=768, out_features=3072, bias=False)\n                (wo): Linear(in_features=3072, out_features=768, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-11): 11 x T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=768, out_features=3072, bias=False)\n                (wo): Linear(in_features=3072, out_features=768, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (token_embedding): Embedding(32100, 768)\n  (position_embedding): Embedding(512, 768)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (self_attention): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (transformer_blocks): ModuleList(\n    (0-3): 4 x CustomTransformerBlock(\n      (input_self_attention): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n      )\n      (context_attn): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n      )\n      (persona_attn): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n      )\n      (paa_layer): PAALayer(\n        (fc): Linear(in_features=1536, out_features=768, bias=True)\n        (sigmoid): Sigmoid()\n        (fc_out): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (mlp): Sequential(\n        (0): Linear(in_features=768, out_features=2048, bias=True)\n        (1): ReLU()\n        (2): Dropout(p=0.1, inplace=False)\n        (3): Linear(in_features=2048, out_features=768, bias=True)\n        (4): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (final_fc): Linear(in_features=768, out_features=32100, bias=True)\n  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n)"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:56.070624Z","iopub.execute_input":"2025-01-09T12:58:56.070832Z","iopub.status.idle":"2025-01-09T12:58:56.074040Z","shell.execute_reply.started":"2025-01-09T12:58:56.070813Z","shell.execute_reply":"2025-01-09T12:58:56.073294Z"},"id":"w-YM7dxRa7JD"},"outputs":[],"execution_count":49},{"cell_type":"code","source":"import torch.optim as optim\noptimizer = optim.AdamW(model.parameters(), lr=5e-5)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:56.075038Z","iopub.execute_input":"2025-01-09T12:58:56.075222Z","iopub.status.idle":"2025-01-09T12:58:56.510963Z","shell.execute_reply.started":"2025-01-09T12:58:56.075206Z","shell.execute_reply":"2025-01-09T12:58:56.510265Z"},"id":"sCrMz7qra7JE"},"outputs":[],"execution_count":50},{"cell_type":"code","source":"LOSS = torch.nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:56.511681Z","iopub.execute_input":"2025-01-09T12:58:56.512034Z","iopub.status.idle":"2025-01-09T12:58:56.515661Z","shell.execute_reply.started":"2025-01-09T12:58:56.512012Z","shell.execute_reply":"2025-01-09T12:58:56.514858Z"},"id":"kNNiPR_ja7JE"},"outputs":[],"execution_count":51},{"cell_type":"code","source":"num_epochs = 200","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:56.516442Z","iopub.execute_input":"2025-01-09T12:58:56.516722Z","iopub.status.idle":"2025-01-09T12:58:56.535285Z","shell.execute_reply.started":"2025-01-09T12:58:56.516701Z","shell.execute_reply":"2025-01-09T12:58:56.534488Z"},"id":"gMr9Wd8_a7JE"},"outputs":[],"execution_count":52},{"cell_type":"code","source":"df_train = df[0:5000]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:58:56.535839Z","iopub.execute_input":"2025-01-09T12:58:56.536048Z","iopub.status.idle":"2025-01-09T12:58:56.548976Z","shell.execute_reply.started":"2025-01-09T12:58:56.536029Z","shell.execute_reply":"2025-01-09T12:58:56.548266Z"},"id":"U5UW8SVga7JE"},"outputs":[],"execution_count":53},{"cell_type":"code","source":"len(df_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:59:00.209595Z","iopub.execute_input":"2025-01-09T12:59:00.209904Z","iopub.status.idle":"2025-01-09T12:59:00.215864Z","shell.execute_reply.started":"2025-01-09T12:59:00.209879Z","shell.execute_reply":"2025-01-09T12:59:00.214784Z"},"id":"FADAr7rsa7JF","outputId":"2bc20724-bbfe-4263-c10d-0b4469249df3"},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"5000"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:59:00.624322Z","iopub.execute_input":"2025-01-09T12:59:00.624598Z","iopub.status.idle":"2025-01-09T12:59:03.679915Z","shell.execute_reply.started":"2025-01-09T12:59:00.624576Z","shell.execute_reply":"2025-01-09T12:59:03.679019Z"},"id":"V_g5jqJQa7JF","outputId":"d4a9adb3-2204-412b-ea78-d71c8bfcb8fe"},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"Problem                      0\nStudent_code                 0\nExpected_code                0\nQ01                          0\nQ02                          0\nQ03                          0\nQ04                          0\nQ05                          0\nQ06                          0\nQ07                          0\nQ08                          0\nQ09                          0\nQ10                          0\nQ11                          0\nQ12                          0\nQ13                          0\nQ14                          0\nQ15                          0\nQ16                          0\nmetacognitive_vector         0\nmetacognitive_feedback       0\ncombined_problem_student     0\ncombined_problem_expected    0\ndtype: int64"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ntrain_dataset = CustomDataset(df_train, t5_tokenizer , gpt2_tokenizer)\ntrain_dataloader = DataLoader(train_dataset , batch_size = 4 ,shuffle = True )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:59:03.680952Z","iopub.execute_input":"2025-01-09T12:59:03.681178Z","iopub.status.idle":"2025-01-09T12:59:03.694605Z","shell.execute_reply.started":"2025-01-09T12:59:03.681155Z","shell.execute_reply":"2025-01-09T12:59:03.693814Z"},"id":"GqHiiAc1a7JF"},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# from torch.utils.tensorboard import SummaryWriter\n# writer = SummaryWriter()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:59:03.696067Z","iopub.execute_input":"2025-01-09T12:59:03.696266Z","iopub.status.idle":"2025-01-09T12:59:03.709922Z","shell.execute_reply.started":"2025-01-09T12:59:03.696249Z","shell.execute_reply":"2025-01-09T12:59:03.709139Z"},"id":"pvWMI-1La7JG","outputId":"f1ad7ea8-6c46-426b-f009-98b9cf47574e"},"outputs":[],"execution_count":57},{"cell_type":"code","source":"import os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:59:03.710803Z","iopub.execute_input":"2025-01-09T12:59:03.710995Z","iopub.status.idle":"2025-01-09T12:59:03.726227Z","shell.execute_reply.started":"2025-01-09T12:59:03.710979Z","shell.execute_reply":"2025-01-09T12:59:03.725590Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"checkpoint_dir = \"./checkpoints\"\nos.makedirs(checkpoint_dir, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:59:03.727036Z","iopub.execute_input":"2025-01-09T12:59:03.727303Z","iopub.status.idle":"2025-01-09T12:59:03.740398Z","shell.execute_reply.started":"2025-01-09T12:59:03.727277Z","shell.execute_reply":"2025-01-09T12:59:03.739764Z"},"id":"_G2DGEL_a7JG"},"outputs":[],"execution_count":59},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    print(f\"Training started for epoch {epoch + 1}/{num_epochs}\")\n    model.train()\n    total_loss = 0\n\n    for idx, (metacognition_vector_ids,\n              problem_student_code_ids,\n              problem_expected_code_ids,\n              student_code_ids,\n              target_ids) in enumerate(train_dataloader):\n\n        metacognition_vector_ids = metacognition_vector_ids.to(device)\n        problem_student_code_ids = problem_student_code_ids.to(device)\n        problem_expected_code_ids = problem_expected_code_ids.to(device)\n        student_code_ids = student_code_ids.to(device)\n        target_ids = target_ids.to(device)\n\n        #attention masking\n        student_attention_mask = (problem_student_code_ids != t5_pad_token_id).long().to(device)\n        expected_attention_mask = (problem_expected_code_ids != t5_pad_token_id).long().to(device)\n\n\n        optimizer.zero_grad()\n        logits = model(metacognition_vector_ids,\n                       problem_student_code_ids ,\n                       problem_expected_code_ids ,\n                       expected_attention_mask,\n                       student_attention_mask)\n\n\n        logits = logits.view(-1, logits.size(-1))\n        \n        target_ids = target_ids.view(-1)\n        # print(logits.shape)\n        # print(target_ids.shape)\n\n\n        loss = LOSS(logits, target_ids)\n        total_loss += loss.item()\n\n\n        loss.backward()\n        for name, param in model.named_parameters():\n            if 'context_encoder' in name:\n                assert param.grad is None, f\"Gradients found in frozen encoder {name}\"\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n\n        if idx % 10 == 0:\n            print(f\"Batch {idx + 1}/{len(train_dataloader)} | Loss: {loss.item():.4f}\" , end='\\r')\n\n\n\n\n\n\n    if epoch % 10 ==0 :\n            for name, param in model.named_parameters():\n                if param.requires_grad and param.grad is not None:\n                    print(f\"Layer: {name} | Grad Norm: {param.grad.norm().item()}\")\n                elif param.requires_grad:\n                    print(f\"Layer: {name} | Grad: None\")\n\n    if (epoch + 1) % 20 == 0:\n        checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch + 1}.pth\")\n        torch.save({\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': total_loss / max(len(train_dataloader), 1),\n        }, checkpoint_path)\n        print(f\"Checkpoint saved at {checkpoint_path}\")\n\n\n    avg_loss = total_loss / max(len(train_dataloader), 1)\n    #writer.add_scalar(\"Loss/train\", avg_loss, epoch + 1)\n    print(f\"Epoch [{epoch + 1}/{num_epochs}] completed | Average Loss: {avg_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:59:03.741105Z","iopub.execute_input":"2025-01-09T12:59:03.741332Z","execution_failed":"2025-01-10T00:53:18.560Z"},"id":"qBooPbJNa7JG","outputId":"c278d4f2-d0f6-408d-9237-2ae0a600ab95"},"outputs":[{"name":"stdout","text":"Training started for epoch 1/200\nLayer: token_embedding.weight | Grad Norm: 9.865543104670138e-11\nLayer: position_embedding.weight | Grad Norm: 1.025722493053216e-10\nLayer: self_attention.in_proj_weight | Grad: None\nLayer: self_attention.in_proj_bias | Grad: None\nLayer: self_attention.out_proj.weight | Grad: None\nLayer: self_attention.out_proj.bias | Grad: None\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 5.5115694230778445e-09\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 1.45128242756698e-10\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 6.7105778711606945e-09\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 2.550493705122392e-10\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 3.1521871779460753e-09\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 1.9556527508513e-09\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 4.1693146712873386e-09\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 3.488818345331879e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 4.535905873126467e-08\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 2.0004313761035064e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 5.891632426369142e-08\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 3.471287257639233e-09\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 1.1805434851908103e-08\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 6.236581251428674e-10\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 6.880706848733098e-08\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 1.208358835214085e-08\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 8.934047457387351e-08\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 2.1506174618934892e-08\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 1.8991703143456107e-07\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 5.6879343901528046e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 3.5527238928523275e-09\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 3.724149655326414e-09\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 3.722036012732133e-09\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 3.830186390274548e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 1.5177087675510847e-07\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 5.62025270767208e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 2.5408317583242024e-07\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 9.850062987482033e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 1.1880713657319575e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 7.370930177330592e-08\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 1.5008876630417944e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 1.2758489731368172e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 1.7647163303990965e-06\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 7.856867512145982e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 2.0645670701924246e-06\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 1.2878538768745784e-07\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 5.250438448456407e-07\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 2.3184762554251392e-08\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 2.7476401101012016e-06\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 4.158854096658615e-07\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 3.5891439438273665e-06\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 6.903786697876058e-07\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 8.673107913637068e-06\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 1.8715951455305913e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 2.025517318315906e-07\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 2.1075503298106923e-07\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 2.032865893397684e-07\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 2.11099290936545e-07\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 8.201393029594328e-06\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 3.0091067060311616e-07\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 1.2264041288290173e-05\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 5.275132366477919e-07\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 7.097997695382219e-06\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 4.403678758535534e-06\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 8.961203093349468e-06\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 7.614161404490005e-06\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.00012380689440760761\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 5.536508069781121e-06\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.0001332230312982574\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 8.33147805678891e-06\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 2.787506855383981e-05\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 1.341401002719067e-06\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.00016778981080278754\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 2.542990660003852e-05\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.000233435130212456\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 4.147968502365984e-05\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.0006625392707064748\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.00011663336044875905\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 1.7281332475249656e-05\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 1.9794115360127762e-05\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 1.7385966202709824e-05\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 1.989482007047627e-05\nLayer: transformer_blocks.3.input_self_attention.in_proj_weight | Grad Norm: 0.0007727871998213232\nLayer: transformer_blocks.3.input_self_attention.in_proj_bias | Grad Norm: 2.8219943487783894e-05\nLayer: transformer_blocks.3.input_self_attention.out_proj.weight | Grad Norm: 0.0011100362753495574\nLayer: transformer_blocks.3.input_self_attention.out_proj.bias | Grad Norm: 4.75661399832461e-05\nLayer: transformer_blocks.3.context_attn.in_proj_weight | Grad Norm: 0.0008591694058850408\nLayer: transformer_blocks.3.context_attn.in_proj_bias | Grad Norm: 0.0005330384592525661\nLayer: transformer_blocks.3.context_attn.out_proj.weight | Grad Norm: 0.0009867536136880517\nLayer: transformer_blocks.3.context_attn.out_proj.bias | Grad Norm: 0.0009095584973692894\nLayer: transformer_blocks.3.persona_attn.in_proj_weight | Grad Norm: 0.014926683157682419\nLayer: transformer_blocks.3.persona_attn.in_proj_bias | Grad Norm: 0.0006614239537157118\nLayer: transformer_blocks.3.persona_attn.out_proj.weight | Grad Norm: 0.013116422109305859\nLayer: transformer_blocks.3.persona_attn.out_proj.bias | Grad Norm: 0.0009447289048694074\nLayer: transformer_blocks.3.paa_layer.fc.weight | Grad Norm: 0.0027696811594069004\nLayer: transformer_blocks.3.paa_layer.fc.bias | Grad Norm: 0.00011732316488632932\nLayer: transformer_blocks.3.paa_layer.fc_out.weight | Grad Norm: 0.02133581042289734\nLayer: transformer_blocks.3.paa_layer.fc_out.bias | Grad Norm: 0.0029737125150859356\nLayer: transformer_blocks.3.mlp.0.weight | Grad Norm: 0.041171424090862274\nLayer: transformer_blocks.3.mlp.0.bias | Grad Norm: 0.005688954144716263\nLayer: transformer_blocks.3.mlp.3.weight | Grad Norm: 0.3188859522342682\nLayer: transformer_blocks.3.mlp.3.bias | Grad Norm: 0.03360661491751671\nLayer: transformer_blocks.3.mlp.4.weight | Grad Norm: 0.015564760193228722\nLayer: transformer_blocks.3.mlp.4.bias | Grad Norm: 0.017289336770772934\nLayer: transformer_blocks.3.layer_norm2.weight | Grad Norm: 0.017022844403982162\nLayer: transformer_blocks.3.layer_norm2.bias | Grad Norm: 0.018478168174624443\nLayer: final_fc.weight | Grad Norm: 0.9445981383323669\nLayer: final_fc.bias | Grad Norm: 0.03380501642823219\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [1/200] completed | Average Loss: 4.9282\nTraining started for epoch 2/200\nEpoch [2/200] completed | Average Loss: 4.8231\nTraining started for epoch 3/200\nEpoch [3/200] completed | Average Loss: 4.8150\nTraining started for epoch 4/200\nEpoch [4/200] completed | Average Loss: 4.8047\nTraining started for epoch 5/200\nEpoch [5/200] completed | Average Loss: 4.7881\nTraining started for epoch 6/200\nEpoch [6/200] completed | Average Loss: 4.7644\nTraining started for epoch 7/200\nEpoch [7/200] completed | Average Loss: 4.7419\nTraining started for epoch 8/200\nEpoch [8/200] completed | Average Loss: 4.7264\nTraining started for epoch 9/200\nEpoch [9/200] completed | Average Loss: 4.7166\nTraining started for epoch 10/200\nEpoch [10/200] completed | Average Loss: 4.7074\nTraining started for epoch 11/200\nLayer: token_embedding.weight | Grad Norm: 2.1107685911625484e-12\nLayer: position_embedding.weight | Grad Norm: 2.1985889671338743e-12\nLayer: self_attention.in_proj_weight | Grad: None\nLayer: self_attention.in_proj_bias | Grad: None\nLayer: self_attention.out_proj.weight | Grad: None\nLayer: self_attention.out_proj.bias | Grad: None\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 1.1768068464146353e-10\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 3.1141356854336166e-12\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 1.4270278014816284e-10\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 5.480118962786218e-12\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 5.959670257293936e-11\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 3.476683524405999e-11\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 7.620191283530531e-11\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 6.097466281884678e-11\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 9.251110433261545e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 3.5309456747345536e-11\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 1.2099911073093494e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 6.289468251763353e-11\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 2.588049219376387e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 1.3607346152633149e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 1.3744472227017468e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 2.1376606040846013e-10\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 1.7290564535699104e-09\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 3.7392702822991453e-10\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 3.584739616258048e-09\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 8.894735503695017e-10\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 7.68051872102049e-11\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 7.9476314418514e-11\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 7.665535567413784e-11\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 7.950774760789869e-11\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 3.130142589569118e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 1.1455406762062026e-10\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 5.83098547224381e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 2.0074933382296933e-10\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 1.4035702600168065e-09\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 8.187985844898549e-10\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 1.9325756550614415e-09\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 1.4820774607571252e-09\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 2.1739467115367006e-08\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 7.68502761427925e-10\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 3.679643612031214e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 1.3680503396784616e-09\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 2.0395575006659783e-08\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 4.967374844788708e-10\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 1.1158017798607034e-07\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 4.520535501484346e-09\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 2.7961652904195944e-07\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 7.913249611135598e-09\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 1.9791709746641573e-06\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 2.8507081140105583e-08\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 1.943277396776466e-07\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 1.4915572421614343e-07\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 6.041767619535676e-07\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 2.422258376100217e-07\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 8.021391295187641e-06\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 2.9480096941369993e-07\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 8.281534064735752e-06\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 3.697043098327413e-07\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 6.242000836209627e-06\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 3.6413860016182298e-06\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 4.102064394828631e-06\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 6.543172730744118e-06\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.0006718009244650602\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 2.2096637621871196e-05\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.00023629255883861333\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 1.1296900993329473e-05\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 4.647968671633862e-05\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 8.664061397212208e-07\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.0002834009355865419\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 7.272064976859838e-06\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.00047043702215887606\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 6.850047157058725e-06\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.003075007349252701\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 1.7667834981693886e-05\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.0003752420307137072\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.0003626312536653131\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.0004002296773251146\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.00039171072421595454\nLayer: transformer_blocks.3.input_self_attention.in_proj_weight | Grad Norm: 0.010369077324867249\nLayer: transformer_blocks.3.input_self_attention.in_proj_bias | Grad Norm: 0.00038808415411040187\nLayer: transformer_blocks.3.input_self_attention.out_proj.weight | Grad Norm: 0.005668083671480417\nLayer: transformer_blocks.3.input_self_attention.out_proj.bias | Grad Norm: 0.0004819848691113293\nLayer: transformer_blocks.3.context_attn.in_proj_weight | Grad Norm: 0.010219416581094265\nLayer: transformer_blocks.3.context_attn.in_proj_bias | Grad Norm: 0.005961685441434383\nLayer: transformer_blocks.3.context_attn.out_proj.weight | Grad Norm: 0.007021306548267603\nLayer: transformer_blocks.3.context_attn.out_proj.bias | Grad Norm: 0.00947154313325882\nLayer: transformer_blocks.3.persona_attn.in_proj_weight | Grad Norm: 0.1504673957824707\nLayer: transformer_blocks.3.persona_attn.in_proj_bias | Grad Norm: 0.006258377339690924\nLayer: transformer_blocks.3.persona_attn.out_proj.weight | Grad Norm: 0.05862713232636452\nLayer: transformer_blocks.3.persona_attn.out_proj.bias | Grad Norm: 0.005514474119991064\nLayer: transformer_blocks.3.paa_layer.fc.weight | Grad Norm: 0.031648073345422745\nLayer: transformer_blocks.3.paa_layer.fc.bias | Grad Norm: 0.0011024364503100514\nLayer: transformer_blocks.3.paa_layer.fc_out.weight | Grad Norm: 0.1081383153796196\nLayer: transformer_blocks.3.paa_layer.fc_out.bias | Grad Norm: 0.013047818094491959\nLayer: transformer_blocks.3.mlp.0.weight | Grad Norm: 0.1489797830581665\nLayer: transformer_blocks.3.mlp.0.bias | Grad Norm: 0.019693320617079735\nLayer: transformer_blocks.3.mlp.3.weight | Grad Norm: 0.21054892241954803\nLayer: transformer_blocks.3.mlp.3.bias | Grad Norm: 0.0656939372420311\nLayer: transformer_blocks.3.mlp.4.weight | Grad Norm: 0.009718609973788261\nLayer: transformer_blocks.3.mlp.4.bias | Grad Norm: 0.01180757861584425\nLayer: transformer_blocks.3.layer_norm2.weight | Grad Norm: 0.014142036437988281\nLayer: transformer_blocks.3.layer_norm2.bias | Grad Norm: 0.014798939228057861\nLayer: final_fc.weight | Grad Norm: 0.9418924450874329\nLayer: final_fc.bias | Grad Norm: 0.03887123242020607\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [11/200] completed | Average Loss: 4.7015\nTraining started for epoch 12/200\nEpoch [12/200] completed | Average Loss: 4.6960\nTraining started for epoch 13/200\nEpoch [13/200] completed | Average Loss: 4.6908\nTraining started for epoch 14/200\nEpoch [14/200] completed | Average Loss: 4.6858\nTraining started for epoch 15/200\nEpoch [15/200] completed | Average Loss: 4.6818\nTraining started for epoch 16/200\nEpoch [16/200] completed | Average Loss: 4.6780\nTraining started for epoch 17/200\nEpoch [17/200] completed | Average Loss: 4.6738\nTraining started for epoch 18/200\nEpoch [18/200] completed | Average Loss: 4.6701\nTraining started for epoch 19/200\nEpoch [19/200] completed | Average Loss: 4.6670\nTraining started for epoch 20/200\nCheckpoint saved at ./checkpoints/model_epoch_20.pth\nEpoch [20/200] completed | Average Loss: 4.6643\nTraining started for epoch 21/200\nLayer: token_embedding.weight | Grad Norm: 6.568720719267451e-13\nLayer: position_embedding.weight | Grad Norm: 6.81496742718779e-13\nLayer: self_attention.in_proj_weight | Grad: None\nLayer: self_attention.in_proj_bias | Grad: None\nLayer: self_attention.out_proj.weight | Grad: None\nLayer: self_attention.out_proj.bias | Grad: None\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 3.565069073285798e-11\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 9.48063025654755e-13\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 4.2093262903053486e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 1.6359703305587892e-12\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 2.0035079498215147e-11\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 1.1591914927944202e-11\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 2.5349956508935456e-11\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 2.0290770799680224e-11\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 2.915965802152698e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 1.1500335406200435e-11\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 3.8472836028091706e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 2.0499846611898853e-11\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 7.571678006801363e-11\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 3.96031775082295e-12\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 4.533409481144446e-10\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 7.19997325648869e-11\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 5.909432387873892e-10\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 1.2741648014458207e-10\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 1.3144513255625156e-09\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 3.4745961663418257e-10\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 2.7769469015948012e-11\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 2.9959982411620345e-11\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 2.8025274809162504e-11\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 3.0072080242637966e-11\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 1.1650477249602886e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 4.3322408160850756e-11\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 2.2215620454346663e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 7.786513794849625e-11\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 5.92762228190935e-10\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 3.429609374272502e-10\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 7.679359925738538e-10\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 5.946118597499606e-10\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 8.403917561849994e-09\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 3.310685336987973e-10\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 1.3510076612988087e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 5.466692098998749e-10\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 7.863990347800609e-09\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 1.9493061331754546e-10\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 4.408860831972561e-08\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 1.8480648122931598e-09\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 1.1152154399951542e-07\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 3.2408764560898362e-09\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 8.83174209320714e-07\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 1.3384065411514712e-08\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 1.003653764541923e-07\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 7.833094883835656e-08\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 3.5754754890149343e-07\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 1.1285136736205459e-07\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 5.000887085770955e-06\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 1.8567121173873602e-07\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 3.801921820922871e-06\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 2.4622954697406385e-07\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 7.2834013735700864e-06\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 4.214036835037405e-06\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 4.837758297071559e-06\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 8.376487130590249e-06\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.001366899348795414\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 5.2948962547816336e-05\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.0003019177820533514\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 2.2279191398411058e-05\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 3.1474461138714105e-05\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 6.27567885658209e-07\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.00030452298233285546\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 1.0037048923550174e-05\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.0005489240284077823\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 7.154771537898341e-06\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.0050381203182041645\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 1.8600003386382014e-05\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.0006264555850066245\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.0007367515354417264\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.001470558694563806\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.0008283331990242004\nLayer: transformer_blocks.3.input_self_attention.in_proj_weight | Grad Norm: 0.022654065862298012\nLayer: transformer_blocks.3.input_self_attention.in_proj_bias | Grad Norm: 0.0008604470640420914\nLayer: transformer_blocks.3.input_self_attention.out_proj.weight | Grad Norm: 0.008660214953124523\nLayer: transformer_blocks.3.input_self_attention.out_proj.bias | Grad Norm: 0.0007667256868444383\nLayer: transformer_blocks.3.context_attn.in_proj_weight | Grad Norm: 0.007783487439155579\nLayer: transformer_blocks.3.context_attn.in_proj_bias | Grad Norm: 0.0045033772476017475\nLayer: transformer_blocks.3.context_attn.out_proj.weight | Grad Norm: 0.007735675200819969\nLayer: transformer_blocks.3.context_attn.out_proj.bias | Grad Norm: 0.008742163889110088\nLayer: transformer_blocks.3.persona_attn.in_proj_weight | Grad Norm: 0.1664050817489624\nLayer: transformer_blocks.3.persona_attn.in_proj_bias | Grad Norm: 0.00667724059894681\nLayer: transformer_blocks.3.persona_attn.out_proj.weight | Grad Norm: 0.05563291162252426\nLayer: transformer_blocks.3.persona_attn.out_proj.bias | Grad Norm: 0.004829572979360819\nLayer: transformer_blocks.3.paa_layer.fc.weight | Grad Norm: 0.04478359594941139\nLayer: transformer_blocks.3.paa_layer.fc.bias | Grad Norm: 0.0013647202868014574\nLayer: transformer_blocks.3.paa_layer.fc_out.weight | Grad Norm: 0.13273678719997406\nLayer: transformer_blocks.3.paa_layer.fc_out.bias | Grad Norm: 0.012653748504817486\nLayer: transformer_blocks.3.mlp.0.weight | Grad Norm: 0.19275027513504028\nLayer: transformer_blocks.3.mlp.0.bias | Grad Norm: 0.021103523671627045\nLayer: transformer_blocks.3.mlp.3.weight | Grad Norm: 0.2849399149417877\nLayer: transformer_blocks.3.mlp.3.bias | Grad Norm: 0.07481652498245239\nLayer: transformer_blocks.3.mlp.4.weight | Grad Norm: 0.009562619961798191\nLayer: transformer_blocks.3.mlp.4.bias | Grad Norm: 0.010927865281701088\nLayer: transformer_blocks.3.layer_norm2.weight | Grad Norm: 0.013859465718269348\nLayer: transformer_blocks.3.layer_norm2.bias | Grad Norm: 0.013529938645660877\nLayer: final_fc.weight | Grad Norm: 0.9066880941390991\nLayer: final_fc.bias | Grad Norm: 0.03827010095119476\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [21/200] completed | Average Loss: 4.6614\nTraining started for epoch 22/200\nEpoch [22/200] completed | Average Loss: 4.6583\nTraining started for epoch 23/200\nEpoch [23/200] completed | Average Loss: 4.6558\nTraining started for epoch 24/200\nEpoch [24/200] completed | Average Loss: 4.6539\nTraining started for epoch 25/200\nEpoch [25/200] completed | Average Loss: 4.6511\nTraining started for epoch 26/200\nEpoch [26/200] completed | Average Loss: 4.6494\nTraining started for epoch 27/200\nEpoch [27/200] completed | Average Loss: 4.6473\nTraining started for epoch 28/200\nEpoch [28/200] completed | Average Loss: 4.6455\nTraining started for epoch 29/200\nEpoch [29/200] completed | Average Loss: 4.6431\nTraining started for epoch 30/200\nEpoch [30/200] completed | Average Loss: 4.6418\nTraining started for epoch 31/200\nLayer: token_embedding.weight | Grad Norm: 1.9657057015107227e-13\nLayer: position_embedding.weight | Grad Norm: 2.0473530368877307e-13\nLayer: self_attention.in_proj_weight | Grad: None\nLayer: self_attention.in_proj_bias | Grad: None\nLayer: self_attention.out_proj.weight | Grad: None\nLayer: self_attention.out_proj.bias | Grad: None\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 1.0912895587189553e-11\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 2.918867133671482e-13\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 1.2707819171953183e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 4.995908201022026e-13\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 6.496706384667705e-12\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 3.989013980243428e-12\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 8.322503276814164e-12\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 7.015423832135159e-12\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 8.858181549387112e-11\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 4.093328974386079e-12\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 1.1171684410893334e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 7.182555765428145e-12\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 2.1607295813685923e-11\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 1.2177236215910048e-12\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 1.2991076270285618e-10\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 2.5114377591450854e-11\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 1.6767158494968015e-10\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 4.4397010373620205e-11\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 3.582797947210281e-10\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 1.1650984343969384e-10\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 8.370161334869675e-12\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 8.160779343957536e-12\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 8.400802622987591e-12\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 8.173662267851878e-12\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 3.2398150828782946e-10\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 1.2115180386684798e-11\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 6.049516443340508e-10\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 2.144914454382807e-11\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 1.8459686557115162e-10\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 1.1334351512903851e-10\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 2.4673835197219773e-10\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 2.0417798007876797e-10\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 2.5255431079784785e-09\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 1.1697531832055574e-10\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 3.958910532730897e-09\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 1.9410927032392777e-10\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 1.8958361547305458e-09\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 5.3163713015225156e-11\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 1.2666117044091152e-08\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 6.59807097846965e-10\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 3.235269474544111e-08\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 1.1499919905233469e-09\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 2.502898155398725e-07\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 4.4459618209202745e-09\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 4.0875566043041545e-08\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 2.5576495588097714e-08\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 2.3333022625138256e-07\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 5.4439833974129215e-08\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 2.376914608248626e-06\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 8.880992652393616e-08\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 1.1518038718349999e-06\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 7.178473993008083e-08\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 1.0470946563145844e-06\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 6.429219752135396e-07\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 1.4947115687391488e-06\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 1.7712358157950803e-06\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.0006083940388634801\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 2.8050391847500578e-05\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.0001308497303398326\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 7.358270977420034e-06\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 7.791318239469547e-06\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 1.4489906163817068e-07\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 8.468290616292506e-05\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 2.4177272734959843e-06\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 7.54494612920098e-05\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 7.6635018331217e-07\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.0005325471283867955\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 1.1609100738496636e-06\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.00010501828364795074\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 8.750270353630185e-05\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.00041713437531143427\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.00011841574450954795\nLayer: transformer_blocks.3.input_self_attention.in_proj_weight | Grad Norm: 0.005047755315899849\nLayer: transformer_blocks.3.input_self_attention.in_proj_bias | Grad Norm: 0.00019331226940266788\nLayer: transformer_blocks.3.input_self_attention.out_proj.weight | Grad Norm: 0.0026899133808910847\nLayer: transformer_blocks.3.input_self_attention.out_proj.bias | Grad Norm: 0.000235618936130777\nLayer: transformer_blocks.3.context_attn.in_proj_weight | Grad Norm: 0.004253549966961145\nLayer: transformer_blocks.3.context_attn.in_proj_bias | Grad Norm: 0.002611703472211957\nLayer: transformer_blocks.3.context_attn.out_proj.weight | Grad Norm: 0.00407719612121582\nLayer: transformer_blocks.3.context_attn.out_proj.bias | Grad Norm: 0.0040046172216534615\nLayer: transformer_blocks.3.persona_attn.in_proj_weight | Grad Norm: 0.08779313415288925\nLayer: transformer_blocks.3.persona_attn.in_proj_bias | Grad Norm: 0.003673309925943613\nLayer: transformer_blocks.3.persona_attn.out_proj.weight | Grad Norm: 0.03937783092260361\nLayer: transformer_blocks.3.persona_attn.out_proj.bias | Grad Norm: 0.002649590140208602\nLayer: transformer_blocks.3.paa_layer.fc.weight | Grad Norm: 0.026180747896432877\nLayer: transformer_blocks.3.paa_layer.fc.bias | Grad Norm: 0.0005664229393005371\nLayer: transformer_blocks.3.paa_layer.fc_out.weight | Grad Norm: 0.10485431551933289\nLayer: transformer_blocks.3.paa_layer.fc_out.bias | Grad Norm: 0.00559160765260458\nLayer: transformer_blocks.3.mlp.0.weight | Grad Norm: 0.16398268938064575\nLayer: transformer_blocks.3.mlp.0.bias | Grad Norm: 0.008976385928690434\nLayer: transformer_blocks.3.mlp.3.weight | Grad Norm: 0.3207716643810272\nLayer: transformer_blocks.3.mlp.3.bias | Grad Norm: 0.0355672650039196\nLayer: transformer_blocks.3.mlp.4.weight | Grad Norm: 0.01256292313337326\nLayer: transformer_blocks.3.mlp.4.bias | Grad Norm: 0.014003760181367397\nLayer: transformer_blocks.3.layer_norm2.weight | Grad Norm: 0.014493062160909176\nLayer: transformer_blocks.3.layer_norm2.bias | Grad Norm: 0.015173275023698807\nLayer: final_fc.weight | Grad Norm: 0.9196637868881226\nLayer: final_fc.bias | Grad Norm: 0.034793417900800705\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [31/200] completed | Average Loss: 4.6403\nTraining started for epoch 32/200\nEpoch [32/200] completed | Average Loss: 4.6386\nTraining started for epoch 33/200\nEpoch [33/200] completed | Average Loss: 4.6372\nTraining started for epoch 34/200\nEpoch [34/200] completed | Average Loss: 4.6357\nTraining started for epoch 35/200\nEpoch [35/200] completed | Average Loss: 4.6345\nTraining started for epoch 36/200\nEpoch [36/200] completed | Average Loss: 4.6333\nTraining started for epoch 37/200\nEpoch [37/200] completed | Average Loss: 4.6321\nTraining started for epoch 38/200\nEpoch [38/200] completed | Average Loss: 4.6311\nTraining started for epoch 39/200\nEpoch [39/200] completed | Average Loss: 4.6296\nTraining started for epoch 40/200\nCheckpoint saved at ./checkpoints/model_epoch_40.pth\nEpoch [40/200] completed | Average Loss: 4.6287\nTraining started for epoch 41/200\nLayer: token_embedding.weight | Grad Norm: 5.452620234665251e-13\nLayer: position_embedding.weight | Grad Norm: 5.651246072664595e-13\nLayer: self_attention.in_proj_weight | Grad: None\nLayer: self_attention.in_proj_bias | Grad: None\nLayer: self_attention.out_proj.weight | Grad: None\nLayer: self_attention.out_proj.bias | Grad: None\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 3.025233882847367e-11\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 8.157778380764313e-13\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 3.6768244698892616e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 1.4661784156558277e-12\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 1.7540226215917443e-11\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 1.1027172230893001e-11\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 2.1737353236850332e-11\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 1.9198373793494206e-11\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 2.769524609647078e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 1.0698856731106154e-11\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 3.5684952215397914e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 1.902310080292846e-11\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 6.848903388867456e-11\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 3.69416736784034e-12\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 4.1451500565337085e-10\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 6.732094742778472e-11\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 5.36092115144271e-10\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 1.1939840782737576e-10\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 1.1688329193404456e-09\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 3.223376565664182e-10\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 2.532079407258081e-11\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 2.660331156756346e-11\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 2.5575314435699603e-11\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 2.66567514589644e-11\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 1.020476481095045e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 3.8376902350423236e-11\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 1.8422029457454414e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 6.604736896287378e-11\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 4.984869739210751e-10\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 3.133882875427929e-10\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 6.614315206654453e-10\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 5.508760114736333e-10\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 6.656128537230188e-09\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 2.5750357401932433e-10\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 1.1707512292957745e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 4.716187440578778e-10\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 6.641232896953397e-09\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 1.667782162373399e-10\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 4.09096934106401e-08\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 1.702577745454903e-09\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 1.122954031984591e-07\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 3.1601323779995028e-09\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 9.034621371029061e-07\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 1.2390161785447162e-08\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 1.7028793308782042e-07\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 9.8648150981262e-08\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 1.1269622746112873e-06\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 2.124925231328234e-07\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 1.2638219232030679e-05\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 4.776170499098953e-07\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 4.790754246641882e-06\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 4.2075950545950036e-07\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 4.530122623691568e-06\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 2.8479930733738e-06\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 2.8028193810314406e-06\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 3.6600479234039085e-06\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.0008450274472124875\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 3.312415719847195e-05\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.0001376452128170058\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 8.558079571230337e-06\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 4.507978155743331e-05\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 6.863156158942729e-07\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.00010224512516288087\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 2.5168894808302866e-06\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 7.97787361079827e-05\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 6.310116873464722e-07\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.0003070914826821536\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 3.897568490174308e-07\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 9.93739377008751e-05\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 5.26382755197119e-05\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.0004464270896278322\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 9.202303044730797e-05\nLayer: transformer_blocks.3.input_self_attention.in_proj_weight | Grad Norm: 0.004280136898159981\nLayer: transformer_blocks.3.input_self_attention.in_proj_bias | Grad Norm: 0.00016594791668467224\nLayer: transformer_blocks.3.input_self_attention.out_proj.weight | Grad Norm: 0.0016858302988111973\nLayer: transformer_blocks.3.input_self_attention.out_proj.bias | Grad Norm: 0.00014204297622200102\nLayer: transformer_blocks.3.context_attn.in_proj_weight | Grad Norm: 0.0017632333328947425\nLayer: transformer_blocks.3.context_attn.in_proj_bias | Grad Norm: 0.0011085077421739697\nLayer: transformer_blocks.3.context_attn.out_proj.weight | Grad Norm: 0.0025195609778165817\nLayer: transformer_blocks.3.context_attn.out_proj.bias | Grad Norm: 0.002324191154912114\nLayer: transformer_blocks.3.persona_attn.in_proj_weight | Grad Norm: 0.03628924489021301\nLayer: transformer_blocks.3.persona_attn.in_proj_bias | Grad Norm: 0.0015086052007973194\nLayer: transformer_blocks.3.persona_attn.out_proj.weight | Grad Norm: 0.0226778332144022\nLayer: transformer_blocks.3.persona_attn.out_proj.bias | Grad Norm: 0.0014104569563642144\nLayer: transformer_blocks.3.paa_layer.fc.weight | Grad Norm: 0.015079459175467491\nLayer: transformer_blocks.3.paa_layer.fc.bias | Grad Norm: 0.0002895100915338844\nLayer: transformer_blocks.3.paa_layer.fc_out.weight | Grad Norm: 0.05667221546173096\nLayer: transformer_blocks.3.paa_layer.fc_out.bias | Grad Norm: 0.0027543101459741592\nLayer: transformer_blocks.3.mlp.0.weight | Grad Norm: 0.07848580926656723\nLayer: transformer_blocks.3.mlp.0.bias | Grad Norm: 0.0037393742240965366\nLayer: transformer_blocks.3.mlp.3.weight | Grad Norm: 0.25262871384620667\nLayer: transformer_blocks.3.mlp.3.bias | Grad Norm: 0.018757104873657227\nLayer: transformer_blocks.3.mlp.4.weight | Grad Norm: 0.01077899057418108\nLayer: transformer_blocks.3.mlp.4.bias | Grad Norm: 0.011383112519979477\nLayer: transformer_blocks.3.layer_norm2.weight | Grad Norm: 0.012695700861513615\nLayer: transformer_blocks.3.layer_norm2.bias | Grad Norm: 0.011533510871231556\nLayer: final_fc.weight | Grad Norm: 0.7911846041679382\nLayer: final_fc.bias | Grad Norm: 0.03060165047645569\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [41/200] completed | Average Loss: 4.6278\nTraining started for epoch 42/200\nEpoch [42/200] completed | Average Loss: 4.6269\nTraining started for epoch 43/200\nEpoch [43/200] completed | Average Loss: 4.6262\nTraining started for epoch 44/200\nEpoch [44/200] completed | Average Loss: 4.6254\nTraining started for epoch 45/200\nEpoch [45/200] completed | Average Loss: 4.6244\nTraining started for epoch 46/200\nEpoch [46/200] completed | Average Loss: 4.6237\nTraining started for epoch 47/200\nEpoch [47/200] completed | Average Loss: 4.6229\nTraining started for epoch 48/200\nEpoch [48/200] completed | Average Loss: 4.6222\nTraining started for epoch 49/200\nEpoch [49/200] completed | Average Loss: 4.6218\nTraining started for epoch 50/200\nEpoch [50/200] completed | Average Loss: 4.6209\nTraining started for epoch 51/200\nLayer: token_embedding.weight | Grad Norm: 3.265082160804761e-13\nLayer: position_embedding.weight | Grad Norm: 3.394116313958778e-13\nLayer: self_attention.in_proj_weight | Grad: None\nLayer: self_attention.in_proj_bias | Grad: None\nLayer: self_attention.out_proj.weight | Grad: None\nLayer: self_attention.out_proj.bias | Grad: None\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 1.837866317466741e-11\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 4.981513790879022e-13\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 2.2716115447019014e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 9.155617887099399e-13\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 1.2033112353759812e-11\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 7.040188744478204e-12\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 1.535283011833144e-11\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 1.2676251541499095e-11\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 1.5565450317556184e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 6.798679241110106e-12\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 2.0535381728414848e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 1.2746793204287954e-11\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 3.936978867913332e-11\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 2.2518211253985676e-12\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 2.423151956865155e-10\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 4.523829227887077e-11\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 3.098600542816854e-10\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 8.002453560917999e-11\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 6.75281053297283e-10\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 2.181650970989324e-10\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 1.5475478537529952e-11\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 1.546682573683178e-11\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 1.6162411686782008e-11\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 1.5656701629618297e-11\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 5.999631347286538e-10\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 2.272796187363646e-11\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 1.1459724280626915e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 4.150925783652504e-11\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 3.7356959192713646e-10\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 2.185636116536216e-10\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 4.792590768687433e-10\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 3.830074035704456e-10\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 4.185775281939641e-09\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 1.8295814863122928e-10\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 7.158677206575703e-09\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 3.336016740629333e-10\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 3.725375563590205e-09\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 1.0291170693399465e-10\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 2.4582423208130422e-08\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 1.2127165938125017e-09\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 6.615484693384133e-08\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 2.183355718443636e-09\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 5.507853302333388e-07\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 8.609424284600209e-09\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 7.611112096128636e-08\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 6.503924510070647e-08\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 3.7559959764621453e-07\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 9.592515937129065e-08\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 4.9120963012683205e-06\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 1.8732576734237227e-07\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 3.0450441954599228e-06\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 1.642506788357423e-07\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 9.082665428650216e-07\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 5.313976316756452e-07\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 1.6684927004462224e-06\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 9.334357855550479e-07\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.00015338831872213632\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 6.544326879520668e-06\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 3.3644235372776166e-05\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 1.7940678844752256e-06\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 1.0177911462960765e-05\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 1.723665405961583e-07\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 2.3146580133470707e-05\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 6.850649469924974e-07\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 5.483599306899123e-05\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 5.218760747993656e-07\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.0005981802241876721\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 1.3303807691045222e-06\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 9.400083945365623e-05\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.00011511050979606807\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.0002737220493145287\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.00014085987641010433\nLayer: transformer_blocks.3.input_self_attention.in_proj_weight | Grad Norm: 0.0038696189876645803\nLayer: transformer_blocks.3.input_self_attention.in_proj_bias | Grad Norm: 0.00015152037667576224\nLayer: transformer_blocks.3.input_self_attention.out_proj.weight | Grad Norm: 0.002119018929079175\nLayer: transformer_blocks.3.input_self_attention.out_proj.bias | Grad Norm: 0.00017448372091166675\nLayer: transformer_blocks.3.context_attn.in_proj_weight | Grad Norm: 0.002094399183988571\nLayer: transformer_blocks.3.context_attn.in_proj_bias | Grad Norm: 0.0012253659078851342\nLayer: transformer_blocks.3.context_attn.out_proj.weight | Grad Norm: 0.003013398265466094\nLayer: transformer_blocks.3.context_attn.out_proj.bias | Grad Norm: 0.002204356249421835\nLayer: transformer_blocks.3.persona_attn.in_proj_weight | Grad Norm: 0.038159746676683426\nLayer: transformer_blocks.3.persona_attn.in_proj_bias | Grad Norm: 0.0016333183739334345\nLayer: transformer_blocks.3.persona_attn.out_proj.weight | Grad Norm: 0.030639728531241417\nLayer: transformer_blocks.3.persona_attn.out_proj.bias | Grad Norm: 0.0014480699319392443\nLayer: transformer_blocks.3.paa_layer.fc.weight | Grad Norm: 0.02728200890123844\nLayer: transformer_blocks.3.paa_layer.fc.bias | Grad Norm: 0.0003716582723427564\nLayer: transformer_blocks.3.paa_layer.fc_out.weight | Grad Norm: 0.07506183534860611\nLayer: transformer_blocks.3.paa_layer.fc_out.bias | Grad Norm: 0.002418443327769637\nLayer: transformer_blocks.3.mlp.0.weight | Grad Norm: 0.10461743921041489\nLayer: transformer_blocks.3.mlp.0.bias | Grad Norm: 0.0034096885938197374\nLayer: transformer_blocks.3.mlp.3.weight | Grad Norm: 0.26860111951828003\nLayer: transformer_blocks.3.mlp.3.bias | Grad Norm: 0.010260037146508694\nLayer: transformer_blocks.3.mlp.4.weight | Grad Norm: 0.010822033509612083\nLayer: transformer_blocks.3.mlp.4.bias | Grad Norm: 0.009687387384474277\nLayer: transformer_blocks.3.layer_norm2.weight | Grad Norm: 0.013826919719576836\nLayer: transformer_blocks.3.layer_norm2.bias | Grad Norm: 0.010601908899843693\nLayer: final_fc.weight | Grad Norm: 0.9518172144889832\nLayer: final_fc.bias | Grad Norm: 0.038669973611831665\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [51/200] completed | Average Loss: 4.6204\nTraining started for epoch 52/200\nEpoch [52/200] completed | Average Loss: 4.6198\nTraining started for epoch 53/200\nEpoch [53/200] completed | Average Loss: 4.6192\nTraining started for epoch 54/200\nEpoch [54/200] completed | Average Loss: 4.6188\nTraining started for epoch 55/200\nEpoch [55/200] completed | Average Loss: 4.6181\nTraining started for epoch 56/200\nEpoch [56/200] completed | Average Loss: 4.6179\nTraining started for epoch 57/200\nEpoch [57/200] completed | Average Loss: 4.6173\nTraining started for epoch 58/200\nEpoch [58/200] completed | Average Loss: 4.6168\nTraining started for epoch 59/200\nEpoch [59/200] completed | Average Loss: 4.6164\nTraining started for epoch 60/200\nCheckpoint saved at ./checkpoints/model_epoch_60.pth\nEpoch [60/200] completed | Average Loss: 4.6157\nTraining started for epoch 61/200\nLayer: token_embedding.weight | Grad Norm: 5.778014243278118e-13\nLayer: position_embedding.weight | Grad Norm: 6.003501732201877e-13\nLayer: self_attention.in_proj_weight | Grad: None\nLayer: self_attention.in_proj_bias | Grad: None\nLayer: self_attention.out_proj.weight | Grad: None\nLayer: self_attention.out_proj.bias | Grad: None\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 3.107075013941696e-11\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 8.469324959230196e-13\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 3.575234205910327e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 1.4579264445008233e-12\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 1.7105766456082527e-11\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 1.0445977416395635e-11\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 2.143888191974419e-11\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 1.8735308443540433e-11\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 2.7696886450989666e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 1.0794006313763482e-11\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 3.5638281215000234e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 1.9445112187099767e-11\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 6.809453001466181e-11\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 3.7900745909758005e-12\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 3.9809458507455986e-10\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 6.694714227428733e-11\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 5.081358667169411e-10\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 1.196179544304954e-10\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 1.0137107819829794e-09\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 3.017740224375842e-10\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 2.3090244993806408e-11\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 2.3049491135185285e-11\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 2.304667741370725e-11\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 2.3072370403109943e-11\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 9.265641032207839e-10\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 3.522244301779054e-11\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 1.6936599900319038e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 6.211477165951607e-11\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 4.679563403442444e-10\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 2.8576685462411433e-10\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 6.185975620631723e-10\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 5.106460809756186e-10\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 6.310562294942201e-09\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 2.446363667196749e-10\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 1.0971874075949017e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 4.571724665503041e-10\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 6.054938328503567e-09\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 1.5861334468070254e-10\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 3.7507664529812246e-08\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 1.6579200234900782e-09\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 1.0117007320786797e-07\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 2.9797402323339384e-09\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 8.756338161219901e-07\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 1.2063634535763867e-08\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 1.7133818630554742e-07\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 1.0802131811260551e-07\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 1.010815324775649e-07\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 1.1860712589850664e-07\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 4.487787919060793e-06\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 1.7267788621211366e-07\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 1.641883159209101e-06\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 1.2494638212956488e-07\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 2.0901519803828705e-07\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 1.276392822546768e-07\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 2.4070317294899723e-07\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 1.3613552596325462e-07\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 1.1301509402983356e-05\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 2.8813661856474937e-07\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 6.012261565047083e-06\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 1.412093268982062e-07\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 1.864320074673742e-05\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 1.334030770294703e-07\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 4.737098606710788e-06\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 5.0594167078088503e-08\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 1.6347019482054748e-05\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 6.008723119066417e-08\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.0003045399207621813\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 2.4424841171821754e-07\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 8.791650179773569e-05\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 5.941066410741769e-05\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.0005203750915825367\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 9.770606266101822e-05\nLayer: transformer_blocks.3.input_self_attention.in_proj_weight | Grad Norm: 0.004005168564617634\nLayer: transformer_blocks.3.input_self_attention.in_proj_bias | Grad Norm: 0.0001572756445966661\nLayer: transformer_blocks.3.input_self_attention.out_proj.weight | Grad Norm: 0.0016668277094140649\nLayer: transformer_blocks.3.input_self_attention.out_proj.bias | Grad Norm: 0.00013109850988257676\nLayer: transformer_blocks.3.context_attn.in_proj_weight | Grad Norm: 0.002033947966992855\nLayer: transformer_blocks.3.context_attn.in_proj_bias | Grad Norm: 0.0012420708080753684\nLayer: transformer_blocks.3.context_attn.out_proj.weight | Grad Norm: 0.0031665214337408543\nLayer: transformer_blocks.3.context_attn.out_proj.bias | Grad Norm: 0.002016187645494938\nLayer: transformer_blocks.3.persona_attn.in_proj_weight | Grad Norm: 0.0503324419260025\nLayer: transformer_blocks.3.persona_attn.in_proj_bias | Grad Norm: 0.002051346004009247\nLayer: transformer_blocks.3.persona_attn.out_proj.weight | Grad Norm: 0.022043056786060333\nLayer: transformer_blocks.3.persona_attn.out_proj.bias | Grad Norm: 0.0012788436142727733\nLayer: transformer_blocks.3.paa_layer.fc.weight | Grad Norm: 0.016635475680232048\nLayer: transformer_blocks.3.paa_layer.fc.bias | Grad Norm: 0.00026223238091915846\nLayer: transformer_blocks.3.paa_layer.fc_out.weight | Grad Norm: 0.05428975820541382\nLayer: transformer_blocks.3.paa_layer.fc_out.bias | Grad Norm: 0.0020686679054051638\nLayer: transformer_blocks.3.mlp.0.weight | Grad Norm: 0.06867900490760803\nLayer: transformer_blocks.3.mlp.0.bias | Grad Norm: 0.0028261863626539707\nLayer: transformer_blocks.3.mlp.3.weight | Grad Norm: 0.2136978954076767\nLayer: transformer_blocks.3.mlp.3.bias | Grad Norm: 0.010688388720154762\nLayer: transformer_blocks.3.mlp.4.weight | Grad Norm: 0.0105066429823637\nLayer: transformer_blocks.3.mlp.4.bias | Grad Norm: 0.011031899601221085\nLayer: transformer_blocks.3.layer_norm2.weight | Grad Norm: 0.01090945303440094\nLayer: transformer_blocks.3.layer_norm2.bias | Grad Norm: 0.010517394170165062\nLayer: final_fc.weight | Grad Norm: 0.73748779296875\nLayer: final_fc.bias | Grad Norm: 0.026149947196245193\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [61/200] completed | Average Loss: 4.6155\nTraining started for epoch 62/200\nEpoch [62/200] completed | Average Loss: 4.6149\nTraining started for epoch 63/200\nEpoch [63/200] completed | Average Loss: 4.6149\nTraining started for epoch 64/200\nEpoch [64/200] completed | Average Loss: 4.6146\nTraining started for epoch 65/200\nEpoch [65/200] completed | Average Loss: 4.6142\nTraining started for epoch 66/200\nEpoch [66/200] completed | Average Loss: 4.6139\nTraining started for epoch 67/200\nEpoch [67/200] completed | Average Loss: 4.6133\nTraining started for epoch 68/200\nBatch 621/1250 | Loss: 4.2676\r","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"export CUDA_LAUNCH_BLOCKING=1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T14:16:16.118335Z","iopub.execute_input":"2025-01-08T14:16:16.118626Z","iopub.status.idle":"2025-01-08T14:16:16.123946Z","shell.execute_reply.started":"2025-01-08T14:16:16.118603Z","shell.execute_reply":"2025-01-08T14:16:16.122821Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-66-56efdc3ca8f4>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    export CUDA_LAUNCH_BLOCKING=1\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-66-56efdc3ca8f4>, line 1)","output_type":"error"}],"execution_count":66},{"cell_type":"code","source":"torch.save(model.state_dict(), 'paamodel.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T01:18:23.961207Z","iopub.execute_input":"2025-01-10T01:18:23.961519Z","iopub.status.idle":"2025-01-10T01:18:24.035080Z","shell.execute_reply.started":"2025-01-10T01:18:23.961486Z","shell.execute_reply":"2025-01-10T01:18:24.033767Z"},"id":"wkDGQ4eRa7JH"},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-4f3f5573ee9e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'paamodel.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"],"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"model = PAAModel()\n\n# Load the saved state dict\nmodel.load_state_dict(torch.load('paamodel.pth'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:15:38.044438Z","iopub.execute_input":"2025-01-02T09:15:38.044677Z","iopub.status.idle":"2025-01-02T09:15:43.200276Z","shell.execute_reply.started":"2025-01-02T09:15:38.044656Z","shell.execute_reply":"2025-01-02T09:15:43.199391Z"},"id":"7TcPgRsja7JH","outputId":"9367d669-eabf-48fb-e834-0d05129193bf"},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n<ipython-input-100-576f8403c810>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('paamodel.pth'))\n","output_type":"stream"},{"execution_count":100,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"df_eval = df[9999:10000].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:13:31.90417Z","iopub.execute_input":"2025-01-02T09:13:31.904508Z","iopub.status.idle":"2025-01-02T09:13:31.909558Z","shell.execute_reply.started":"2025-01-02T09:13:31.904481Z","shell.execute_reply":"2025-01-02T09:13:31.908551Z"},"id":"K1MXJesua7JI"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:13:33.152773Z","iopub.execute_input":"2025-01-02T09:13:33.153078Z","iopub.status.idle":"2025-01-02T09:13:33.167906Z","shell.execute_reply.started":"2025-01-02T09:13:33.153052Z","shell.execute_reply":"2025-01-02T09:13:33.166951Z"},"id":"9D4sb3pxa7JI","outputId":"114e6bf8-46fd-487a-d0d7-713c7e4ec370"},"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"                                             Problem  \\\n0  Amicable numbers are pairs of numbers for whic...   \n\n                                        Student_code  \\\n0  def named_are_amicable():                for n...   \n\n                                       Expected_code            Q01  \\\n0  def sum_of_divisors(number):          divisors...  2 : Sometimes   \n\n             Q02            Q03        Q04               Q05            Q06  \\\n0  2 : Sometimes  2 : Sometimes  3 : Often  1 : Almost Never  2 : Sometimes   \n\n             Q07  ...            Q11            Q12               Q13  \\\n0  2 : Sometimes  ...  2 : Sometimes  2 : Sometimes  1 : Almost Never   \n\n             Q14        Q15            Q16  \\\n0  2 : Sometimes  3 : Often  2 : Sometimes   \n\n                                metacognitive_vector  \\\n0  ['2 ', '2 ', '2 ', '3 ', '1 ', '2 ', '2 ', '2 ...   \n\n                              metacognitive_feedback  \\\n0  Your implementation has a few significant issu...   \n\n                            combined_problem_student  \\\n0  Amicable numbers are pairs of numbers for whic...   \n\n                           combined_problem_expected  \n0  Amicable numbers are pairs of numbers for whic...  \n\n[1 rows x 23 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Problem</th>\n      <th>Student_code</th>\n      <th>Expected_code</th>\n      <th>Q01</th>\n      <th>Q02</th>\n      <th>Q03</th>\n      <th>Q04</th>\n      <th>Q05</th>\n      <th>Q06</th>\n      <th>Q07</th>\n      <th>...</th>\n      <th>Q11</th>\n      <th>Q12</th>\n      <th>Q13</th>\n      <th>Q14</th>\n      <th>Q15</th>\n      <th>Q16</th>\n      <th>metacognitive_vector</th>\n      <th>metacognitive_feedback</th>\n      <th>combined_problem_student</th>\n      <th>combined_problem_expected</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Amicable numbers are pairs of numbers for whic...</td>\n      <td>def named_are_amicable():                for n...</td>\n      <td>def sum_of_divisors(number):          divisors...</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>...</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>['2 ', '2 ', '2 ', '3 ', '1 ', '2 ', '2 ', '2 ...</td>\n      <td>Your implementation has a few significant issu...</td>\n      <td>Amicable numbers are pairs of numbers for whic...</td>\n      <td>Amicable numbers are pairs of numbers for whic...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 23 columns</p>\n</div>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"eval_dataset = CustomDataset(df_eval, t5_tokenizer,gpt2_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:13:36.045686Z","iopub.execute_input":"2025-01-02T09:13:36.046004Z","iopub.status.idle":"2025-01-02T09:13:36.049807Z","shell.execute_reply.started":"2025-01-02T09:13:36.045976Z","shell.execute_reply":"2025-01-02T09:13:36.048902Z"},"id":"p0goTw6Da7JI"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef inference(model,gpt2_tokenizer, t5_tokenizer, eval_dataset, device):\n    model.eval()\n    model.to(device)\n\n    metacognitive_vector_ids, problem_student_code_ids, problem_expected_code_ids,student_code_ids, target_ids = eval_dataset[0]\n\n    metacognitive_tensor = metacognitive_vector_ids.unsqueeze(0).to(device)\n    problem_student_code_tensor = problem_student_code_ids.unsqueeze(0).to(device)\n    problem_expected_code_tensor = problem_expected_code_ids.unsqueeze(0).to(device)\n    target_tensor = target_ids.unsqueeze(0).to(device)\n\n    student_attention_mask = (problem_student_code_tensor != t5_tokenizer.pad_token_id).long().to(device)\n    expected_attention_mask = (problem_expected_code_tensor != t5_tokenizer.pad_token_id).long().to(device)\n\n\n    with torch.no_grad():\n\n        logits = model(\n            metacognitive_vector_ids=metacognitive_tensor,\n            problem_student_code_ids=problem_student_code_tensor,\n            problem_expected_code_ids=problem_expected_code_tensor,\n            expected_attention_mask=expected_attention_mask,\n            student_attention_mask=student_attention_mask\n        )\n\n        predictions = logits.argmax(dim=-1).squeeze().tolist()\n        filtered_tokens = [token for token in predictions if token != 0]\n        #decoded_text = t5_tokenizer.decode(filtered_tokens, skip_special_tokens=False)\n        decoded_text = t5_tokenizer.decode(predictions, skip_special_tokens=True)\n\n\n        return filtered_tokens, decoded_text\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:42:25.839368Z","iopub.execute_input":"2025-01-02T09:42:25.839702Z","iopub.status.idle":"2025-01-02T09:42:25.84611Z","shell.execute_reply.started":"2025-01-02T09:42:25.839676Z","shell.execute_reply":"2025-01-02T09:42:25.845101Z"},"id":"GGKQrOYQa7JJ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions, decoded_text = inference(model, gpt2_tokenizer, t5_tokenizer, eval_dataset, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:42:26.462145Z","iopub.execute_input":"2025-01-02T09:42:26.462503Z","iopub.status.idle":"2025-01-02T09:42:26.555247Z","shell.execute_reply.started":"2025-01-02T09:42:26.462473Z","shell.execute_reply":"2025-01-02T09:42:26.554554Z"},"id":"Nk1Ua-_va7JJ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Predicted Tokens:\", predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:42:26.833831Z","iopub.execute_input":"2025-01-02T09:42:26.834114Z","iopub.status.idle":"2025-01-02T09:42:26.838824Z","shell.execute_reply.started":"2025-01-02T09:42:26.834092Z","shell.execute_reply":"2025-01-02T09:42:26.837971Z"},"id":"0i90o8Nfa7JK","outputId":"f02ea63b-45b7-4fbf-84b4-46ed4860ea3d"},"outputs":[{"name":"stdout","text":"Predicted Tokens: [6, 6, 3, 8, 6, 8, 6, 8, 6, 8, 8, 6, 3, 6, 5, 3, 6, 8, 8, 12, 3, 5, 6, 9, 5, 6, 6, 3, 8, 8, 3, 8, 8, 8, 8, 6, 8, 8, 3, 8, 8, 3, 8, 3, 8, 6, 8, 6, 8, 8, 3, 8, 3, 8, 3, 8, 8, 6, 6, 8, 5, 6, 8, 6, 8, 6, 8, 8, 5, 6, 3, 6, 8, 6, 8, 8, 5, 696, 12, 3, 8, 12, 8, 8, 3, 844, 12, 3, 3, 6, 5, 6, 8, 6, 6, 5, 6, 3, 8, 8, 3, 3, 6, 6, 3, 8, 6, 5, 3, 844, 6, 8, 6, 5, 8, 6, 6, 3, 844, 8, 6, 6, 3, 8, 9, 6, 6, 844, 6, 3, 8, 8, 8, 6, 3, 6, 12, 8, 844, 3, 6, 8, 6, 8, 6, 8, 6, 3, 6, 12, 7, 8, 3, 12, 6, 6, 8, 6, 8, 6, 844, 3, 3, 12, 6, 8, 3, 6, 8, 6, 3, 6, 8, 5, 8, 8, 3, 8, 6, 8, 6, 6, 6, 3, 6, 8, 12, 3, 5, 8, 6, 6, 3, 8, 6, 6, 3, 8, 6, 3, 3, 8, 6, 5, 5, 6, 6, 8, 6, 3, 8, 5, 6, 3, 8, 6, 6, 3, 5, 6, 8, 12, 3, 3]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"print(\"Decoded Text:\", decoded_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:42:27.089445Z","iopub.execute_input":"2025-01-02T09:42:27.089745Z","iopub.status.idle":"2025-01-02T09:42:27.094494Z","shell.execute_reply.started":"2025-01-02T09:42:27.089722Z","shell.execute_reply":"2025-01-02T09:42:27.093656Z"},"id":"Mv7lQhaFa7JK","outputId":"982d3c1f-680d-4721-f878-71bc628e6a92"},"outputs":[{"name":"stdout","text":"Decoded Text: ,,  the, the, the, the the,,., the the to.,a.,,  the the  the the the the, the the  the the  the  the, the, the the  the  the  the the,, the., the, the, the the.,, the, the the. Your to  the to the the  areas to ,., the,,.,  the the ,,  the,.  areas, the,. the,,  areas the,,  thea,, areas,  the the the,, to the areas, the, the, the,, tos the  to,, the, the, areas   to, the, the,, the. the the  the, the,,,, the to. the,,  the,,  the,   the,..,, the,  the.,  the,,., the to  \n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def load_checkpoint(checkpoint_path, model, optimizer=None):\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    loss = checkpoint['loss']\n    print(f\"Checkpoint loaded: Epoch {epoch}, Loss: {loss:.4f}\")\n    return model, optimizer, epoch, loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:14:56.480065Z","iopub.execute_input":"2025-01-02T09:14:56.480394Z","iopub.status.idle":"2025-01-02T09:14:56.484822Z","shell.execute_reply.started":"2025-01-02T09:14:56.480367Z","shell.execute_reply":"2025-01-02T09:14:56.483857Z"},"id":"XRItYXvQa7JN"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint_path = \"./checkpoints/model_epoch_20.pth\"\nmodel, optimizer, start_epoch, _ = load_checkpoint(checkpoint_path, model, optimizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:14:20.031344Z","iopub.execute_input":"2025-01-02T09:14:20.031652Z","iopub.status.idle":"2025-01-02T09:14:21.532472Z","shell.execute_reply.started":"2025-01-02T09:14:20.031628Z","shell.execute_reply":"2025-01-02T09:14:21.531698Z"},"id":"iJ7FRM_wa7JO","outputId":"9cca23dd-209b-4ba0-b713-602052e8deb7"},"outputs":[{"name":"stderr","text":"<ipython-input-78-61464e86af1d>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path)\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint loaded: Epoch 20, Loss: 4.2463\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"checkpoint_path = \"./checkpoints/model_epoch_10.pth\"\ncheckpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))  # Use GPU if available: 'cuda'\nmodel.load_state_dict(checkpoint['model_state_dict'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:13:55.653477Z","iopub.execute_input":"2025-01-02T09:13:55.653715Z","iopub.status.idle":"2025-01-02T09:13:55.676084Z","shell.execute_reply.started":"2025-01-02T09:13:55.653693Z","shell.execute_reply":"2025-01-02T09:13:55.674979Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"id":"tBk9w-l_a7JO","outputId":"046613ba-e750-46b5-8175-67d5f9e61095"},"outputs":[{"name":"stderr","text":"<ipython-input-82-a62cfdc3dc89>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))  # Use GPU if available: 'cuda'\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-82-a62cfdc3dc89>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./checkpoints/model_epoch_10.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use GPU if available: 'cuda'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './checkpoints/model_epoch_10.pth'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './checkpoints/model_epoch_10.pth'","output_type":"error"}],"execution_count":null},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:14:26.196139Z","iopub.execute_input":"2025-01-02T09:14:26.19651Z","iopub.status.idle":"2025-01-02T09:14:26.201901Z","shell.execute_reply.started":"2025-01-02T09:14:26.196481Z","shell.execute_reply":"2025-01-02T09:14:26.200956Z"},"id":"b9Shapm3a7JP","outputId":"84970b93-34af-46a0-e4ab-d333fbfb7627"},"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"16803"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"df_eval1 = df[449:450].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:15:21.71876Z","iopub.execute_input":"2025-01-02T09:15:21.719058Z","iopub.status.idle":"2025-01-02T09:15:21.723418Z","shell.execute_reply.started":"2025-01-02T09:15:21.719035Z","shell.execute_reply":"2025-01-02T09:15:21.722544Z"},"id":"QznZNXd2a7JP"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_dataset1 = CustomDataset(df_eval1, t5_tokenizer,gpt2_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:15:22.79416Z","iopub.execute_input":"2025-01-02T09:15:22.794471Z","iopub.status.idle":"2025-01-02T09:15:22.798035Z","shell.execute_reply.started":"2025-01-02T09:15:22.794449Z","shell.execute_reply":"2025-01-02T09:15:22.79709Z"},"id":"JuJQTbuma7JQ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions, decoded_text = inference(model, gpt2_tokenizer, t5_tokenizer, eval_dataset1, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:15:44.516595Z","iopub.execute_input":"2025-01-02T09:15:44.516895Z","iopub.status.idle":"2025-01-02T09:15:44.999499Z","shell.execute_reply.started":"2025-01-02T09:15:44.516872Z","shell.execute_reply":"2025-01-02T09:15:44.998792Z"},"id":"450TkAAMa7JQ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Decoded Text:\", decoded_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:15:47.030509Z","iopub.execute_input":"2025-01-02T09:15:47.030805Z","iopub.status.idle":"2025-01-02T09:15:47.035393Z","shell.execute_reply.started":"2025-01-02T09:15:47.030783Z","shell.execute_reply":"2025-01-02T09:15:47.034606Z"},"id":"wOV8HjGSa7JQ","outputId":"18b10659-bb6a-4ab0-e8b7-460d63296e53"},"outputs":[{"name":"stdout","text":"Decoded Text: Your to  the good  understanding to are to are, are areas areas,.,.            the  the the    the,  the  the the . the   the     the the   the the  the, the  the,  the the the to the  the ,,.        the      the the the the the    the the the the the the  the   the the. the the the the the the    the  the. the. the the the  the the the  the the the the   the the your the your the. the the the  the the    the  your. the. your \n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"id":"YVP6ZYJXa7JQ"},"outputs":[],"execution_count":null}]}