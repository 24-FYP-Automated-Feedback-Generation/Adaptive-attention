{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10048507,"sourceType":"datasetVersion","datasetId":6190931},{"sourceId":10242967,"sourceType":"datasetVersion","datasetId":6334477}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"jDgsZdJ7MQ11","execution":{"iopub.status.busy":"2024-12-20T06:24:58.390299Z","iopub.execute_input":"2024-12-20T06:24:58.390628Z","iopub.status.idle":"2024-12-20T06:24:58.413174Z","shell.execute_reply.started":"2024-12-20T06:24:58.390604Z","shell.execute_reply":"2024-12-20T06:24:58.412263Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/modified-dataset/modified_dataset.csv\n/kaggle/input/metacognitive-dataset/metacognitive-dataset.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"pip install transformers torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:24:58.863682Z","iopub.execute_input":"2024-12-20T06:24:58.863973Z","iopub.status.idle":"2024-12-20T06:25:03.793842Z","shell.execute_reply.started":"2024-12-20T06:24:58.863951Z","shell.execute_reply":"2024-12-20T06:25:03.792755Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"Lp92ndkSMQ15","outputId":"19f501ee-55f4-4080-9137-8d4f1276b0a3"},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import AutoModel, AutoTokenizer, GPT2Model,GPT2Tokenizer,GPT2LMHeadModel\nfrom transformers import BertModel, BertTokenizer\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:03.795316Z","iopub.execute_input":"2024-12-20T06:25:03.795682Z","iopub.status.idle":"2024-12-20T06:25:03.800351Z","shell.execute_reply.started":"2024-12-20T06:25:03.795633Z","shell.execute_reply":"2024-12-20T06:25:03.799373Z"},"id":"6Ws_ctb4MQ17"},"outputs":[],"execution_count":5},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:03.801759Z","iopub.execute_input":"2024-12-20T06:25:03.802020Z","iopub.status.idle":"2024-12-20T06:25:03.884816Z","shell.execute_reply.started":"2024-12-20T06:25:03.801987Z","shell.execute_reply":"2024-12-20T06:25:03.883797Z"},"id":"Ki-_U9AKMQ17"},"outputs":[],"execution_count":6},{"cell_type":"code","source":"device","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eo6KLza7PFtB","outputId":"ef29d56e-8b2c-47dd-8398-46a30df8a298","trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:03.885906Z","iopub.execute_input":"2024-12-20T06:25:03.886249Z","iopub.status.idle":"2024-12-20T06:25:03.902754Z","shell.execute_reply.started":"2024-12-20T06:25:03.886220Z","shell.execute_reply":"2024-12-20T06:25:03.901885Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"model_name_encoder = \"bert-base-uncased\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:04.997627Z","iopub.execute_input":"2024-12-20T06:25:04.997998Z","iopub.status.idle":"2024-12-20T06:25:05.001842Z","shell.execute_reply.started":"2024-12-20T06:25:04.997971Z","shell.execute_reply":"2024-12-20T06:25:05.000931Z"},"id":"ULNOTaR6MQ18"},"outputs":[],"execution_count":8},{"cell_type":"code","source":"encoder_tokenizer = BertTokenizer.from_pretrained(model_name_encoder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:05.326320Z","iopub.execute_input":"2024-12-20T06:25:05.326696Z","iopub.status.idle":"2024-12-20T06:25:06.062627Z","shell.execute_reply.started":"2024-12-20T06:25:05.326642Z","shell.execute_reply":"2024-12-20T06:25:06.061549Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"z7XtlRvWMQ19","outputId":"461f17ec-1c5d-4909-92ce-53d9f8435e77"},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"022f012aa57f42319bf55ff859f463af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"069968698e934f879a794a5d913bcfea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7bdd5d4be0e4c2bb4c3e1dd07db96d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9da1c573f5c40d396abe89d0ba7c9db"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"file_path = \"/kaggle/input/metacognitive-dataset/metacognitive-dataset.csv\"\ndf = pd.read_csv(file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:06.382193Z","iopub.execute_input":"2024-12-20T06:25:06.382546Z","iopub.status.idle":"2024-12-20T06:25:06.476372Z","shell.execute_reply.started":"2024-12-20T06:25:06.382515Z","shell.execute_reply":"2024-12-20T06:25:06.475614Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"file_path = \"/content/drive/MyDrive/metacognitive-dataset.csv\"\ndf = pd.read_csv(file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T04:45:17.711757Z","iopub.execute_input":"2024-12-19T04:45:17.711981Z","iopub.status.idle":"2024-12-19T04:45:17.774720Z","shell.execute_reply.started":"2024-12-19T04:45:17.711960Z","shell.execute_reply":"2024-12-19T04:45:17.773669Z"},"id":"39x7PUtHMQ19"},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1OSy8M5KNZTM","outputId":"7a5a3cbe-911b-4b0e-840a-5b3e9df193b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"execution_count":10},{"cell_type":"code","source":"df.head(5)","metadata":{"id":"QM5xDR9nN8zT","colab":{"base_uri":"https://localhost:8080/","height":429},"outputId":"bd2d92a5-9152-49e9-bd8b-d98e4ad73c2c","trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:09.817792Z","iopub.execute_input":"2024-12-20T06:25:09.818073Z","iopub.status.idle":"2024-12-20T06:25:09.846626Z","shell.execute_reply.started":"2024-12-20T06:25:09.818052Z","shell.execute_reply":"2024-12-20T06:25:09.845794Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                            question    difficulty  \\\n0  =====Problem Statement=====\\nGiven an integer,...  introductory   \n1  =====Problem Statement=====\\nYou are given a s...  introductory   \n2  =====Problem Statement=====\\nYou are given a s...  introductory   \n3  =====Problem Statement=====\\nYou are given a p...  introductory   \n4  =====Function Descriptions=====\\nsum\\n\\nThe su...  introductory   \n\n                                     prefer_solution  \\\n0  n = int(input().strip())\\nw = len(str(bin(n))[...   \n1  #!/usr/bin/env python3\\n\\ndef __starting_point...   \n2  A  = set(input().split())\\nn = int(input())\\nc...   \n3  for i in range(1,int(input())): #More than 2 l...   \n4  import numpy\\nn,m=list(map(int,input().split()...   \n\n                                       flaw_solution  random_col_1  \\\n0  w = len(str(bin(n))[2:])\\nn = int(input().stri...             2   \n1  #!/usr/bin/env python3\\n\\ndef __starting_point...             1   \n2  s  = set(input().split())\\nm = int(input())\\nc...             1   \n3  for idx in range(1, int(input())):\\n    print(...             2   \n4  Variable Renaming\\n\\n```\\nimport numpy as np\\n...             2   \n\n   random_col_2  random_col_3  random_col_4  random_col_5  random_col_6  ...  \\\n0             2             1             2             2             2  ...   \n1             3             1             2             2             2  ...   \n2             1             2             3             3             3  ...   \n3             1             1             3             1             3  ...   \n4             2             3             2             2             1  ...   \n\n   random_col_9  random_col_10  random_col_11  random_col_12  random_col_13  \\\n0             1              1              1              1              3   \n1             2              3              1              3              3   \n2             1              3              1              1              1   \n3             1              3              1              1              3   \n4             1              2              3              3              3   \n\n   random_col_14  random_col_15  random_col_16  \\\n0              1              2              2   \n1              1              2              1   \n2              2              1              1   \n3              1              3              2   \n4              1              3              2   \n\n                               metacognitive_vector  \\\n0  [2, 2, 1, 2, 2, 2, 3, 2, 1, 1, 1, 1, 3, 1, 2, 2]   \n1  [1, 3, 1, 2, 2, 2, 3, 2, 2, 3, 1, 3, 3, 1, 2, 1]   \n2  [1, 1, 2, 3, 3, 3, 2, 2, 1, 3, 1, 1, 1, 2, 1, 1]   \n3  [2, 1, 1, 3, 1, 3, 1, 2, 1, 3, 1, 1, 3, 1, 3, 2]   \n4  [2, 2, 3, 2, 2, 1, 1, 3, 1, 2, 3, 3, 3, 1, 3, 2]   \n\n                              metacognitive_feedback  \n0  Your solution is very close to being correct, ...  \n1  It's clear that you have a good understanding ...  \n2  Your approach to the problem shows a good unde...  \n3  Your approach to the problem shows a good unde...  \n4  It appears you are on the right track with you...  \n\n[5 rows x 22 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>difficulty</th>\n      <th>prefer_solution</th>\n      <th>flaw_solution</th>\n      <th>random_col_1</th>\n      <th>random_col_2</th>\n      <th>random_col_3</th>\n      <th>random_col_4</th>\n      <th>random_col_5</th>\n      <th>random_col_6</th>\n      <th>...</th>\n      <th>random_col_9</th>\n      <th>random_col_10</th>\n      <th>random_col_11</th>\n      <th>random_col_12</th>\n      <th>random_col_13</th>\n      <th>random_col_14</th>\n      <th>random_col_15</th>\n      <th>random_col_16</th>\n      <th>metacognitive_vector</th>\n      <th>metacognitive_feedback</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>=====Problem Statement=====\\nGiven an integer,...</td>\n      <td>introductory</td>\n      <td>n = int(input().strip())\\nw = len(str(bin(n))[...</td>\n      <td>w = len(str(bin(n))[2:])\\nn = int(input().stri...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>[2, 2, 1, 2, 2, 2, 3, 2, 1, 1, 1, 1, 3, 1, 2, 2]</td>\n      <td>Your solution is very close to being correct, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>=====Problem Statement=====\\nYou are given a s...</td>\n      <td>introductory</td>\n      <td>#!/usr/bin/env python3\\n\\ndef __starting_point...</td>\n      <td>#!/usr/bin/env python3\\n\\ndef __starting_point...</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>[1, 3, 1, 2, 2, 2, 3, 2, 2, 3, 1, 3, 3, 1, 2, 1]</td>\n      <td>It's clear that you have a good understanding ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>=====Problem Statement=====\\nYou are given a s...</td>\n      <td>introductory</td>\n      <td>A  = set(input().split())\\nn = int(input())\\nc...</td>\n      <td>s  = set(input().split())\\nm = int(input())\\nc...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>...</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[1, 1, 2, 3, 3, 3, 2, 2, 1, 3, 1, 1, 1, 2, 1, 1]</td>\n      <td>Your approach to the problem shows a good unde...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>=====Problem Statement=====\\nYou are given a p...</td>\n      <td>introductory</td>\n      <td>for i in range(1,int(input())): #More than 2 l...</td>\n      <td>for idx in range(1, int(input())):\\n    print(...</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>...</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>[2, 1, 1, 3, 1, 3, 1, 2, 1, 3, 1, 1, 3, 1, 3, 2]</td>\n      <td>Your approach to the problem shows a good unde...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>=====Function Descriptions=====\\nsum\\n\\nThe su...</td>\n      <td>introductory</td>\n      <td>import numpy\\nn,m=list(map(int,input().split()...</td>\n      <td>Variable Renaming\\n\\n```\\nimport numpy as np\\n...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>[2, 2, 3, 2, 2, 1, 1, 3, 1, 2, 3, 3, 3, 1, 3, 2]</td>\n      <td>It appears you are on the right track with you...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 22 columns</p>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# from torch.utils.data import DataLoader, Dataset\n\n# class CustomDataset(Dataset):\n#     def __init__(self, df, tokenizer):\n#         self.df = df\n#         self.tokenizer = tokenizer\n\n#     def __len__(self):\n#         return len(self.df)\n\n#     def __getitem__(self, idx):\n#         row = self.df.iloc[idx]\n\n#         description: row['description'],\n#         student_code: row['student_code'],\n#         feedback: row['feedback'],\n#         metacognitive_feedback: row['metacognitive_feedback'],\n#         metacognitive_vector: torch.tensor(\n#             ast.literal_eval(row['metacognitive_profile']), dtype=torch.float\n#         )\n#         context = f\"Description: {description} Student Code: {student_code} Feedback: {feedback}\"\n\n#         encoded_context = self.tokenizer(\n#             context,\n#             return_tensors='pt',\n#             padding='max_length',\n#             truncation=True,\n#             max_length=512\n#         )\n\n\n#         return encoded_context , metacognitive_vector\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:12.157987Z","iopub.execute_input":"2024-12-20T06:25:12.158294Z","iopub.status.idle":"2024-12-20T06:25:12.162209Z","shell.execute_reply.started":"2024-12-20T06:25:12.158270Z","shell.execute_reply":"2024-12-20T06:25:12.161231Z"},"id":"_OZUsxhEMQ1-"},"outputs":[],"execution_count":12},{"cell_type":"code","source":"#dataset = CustomDataset(df, tokenizer)\ndataloader = DataLoader(df, batch_size=1, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T05:04:06.334349Z","iopub.execute_input":"2024-12-19T05:04:06.334655Z","iopub.status.idle":"2024-12-19T05:04:06.338684Z","shell.execute_reply.started":"2024-12-19T05:04:06.334631Z","shell.execute_reply":"2024-12-19T05:04:06.337808Z"},"id":"8Kj7CJzwMQ1_"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataloader.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T05:13:09.844793Z","iopub.execute_input":"2024-12-19T05:13:09.845117Z","iopub.status.idle":"2024-12-19T05:13:09.859283Z","shell.execute_reply.started":"2024-12-19T05:13:09.845088Z","shell.execute_reply":"2024-12-19T05:13:09.858220Z"},"id":"yxf2az8MMQ1_"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder = BertModel.from_pretrained(model_name_encoder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:15.082080Z","iopub.execute_input":"2024-12-20T06:25:15.082399Z","iopub.status.idle":"2024-12-20T06:25:17.375508Z","shell.execute_reply.started":"2024-12-20T06:25:15.082374Z","shell.execute_reply":"2024-12-20T06:25:17.374581Z"},"id":"ivtEzOFWMQ2A"},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30f7794283ec44f584413d00441431d7"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"tokenizer_encoder = BertTokenizer.from_pretrained(model_name_encoder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:17.376687Z","iopub.execute_input":"2024-12-20T06:25:17.377048Z","iopub.status.idle":"2024-12-20T06:25:17.483870Z","shell.execute_reply.started":"2024-12-20T06:25:17.377007Z","shell.execute_reply":"2024-12-20T06:25:17.482724Z"},"id":"K6wMxygeMQ2A"},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"class ContextEncoder(nn.Module):\n    def __init__(self, bert_model_name='bert-base-uncased', output_dim=768):\n        super(ContextEncoder, self).__init__()\n\n        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n        self.bert_encoder = BertModel.from_pretrained(bert_model_name)\n\n        self.fc = nn.Linear(self.bert_encoder.config.hidden_size, output_dim)\n\n    def forward(self, description, student_code, feedback):\n\n        context = f\"Description: {description} Student Code: {student_code} Feedback: {feedback}\"\n\n\n        encoded_inputs = self.tokenizer(\n            context,\n            return_tensors='pt',\n            padding='max_length',\n            truncation=True,\n            max_length=512\n        )\n\n\n        with torch.no_grad():\n            context_hidden_states = self.bert_encoder(**encoded_inputs).last_hidden_state\n\n\n        context_rep = context_hidden_states.mean(dim=1)\n        context_rep = self.fc(context_rep)\n        final_rep = context_rep.unsqueeze(1)\n\n        return final_rep","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:17.485443Z","iopub.execute_input":"2024-12-20T06:25:17.485835Z","iopub.status.idle":"2024-12-20T06:25:17.494488Z","shell.execute_reply.started":"2024-12-20T06:25:17.485799Z","shell.execute_reply":"2024-12-20T06:25:17.493487Z"},"id":"JHnSVTTgMQ2A"},"outputs":[],"execution_count":15},{"cell_type":"code","source":"context_encoder = ContextEncoder()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:18.006692Z","iopub.execute_input":"2024-12-20T06:25:18.007014Z","iopub.status.idle":"2024-12-20T06:25:18.252838Z","shell.execute_reply.started":"2024-12-20T06:25:18.006986Z","shell.execute_reply":"2024-12-20T06:25:18.251886Z"},"id":"BjPT-bo0MQ2A"},"outputs":[],"execution_count":16},{"cell_type":"code","source":"context_encoder","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"45UQsIQDQoka","outputId":"e978ea49-b70c-49b4-fc2b-d4cf2fa61a91","trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:18.303459Z","iopub.execute_input":"2024-12-20T06:25:18.303801Z","iopub.status.idle":"2024-12-20T06:25:18.310670Z","shell.execute_reply.started":"2024-12-20T06:25:18.303770Z","shell.execute_reply":"2024-12-20T06:25:18.309823Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"ContextEncoder(\n  (bert_encoder): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (fc): Linear(in_features=768, out_features=768, bias=True)\n)"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"class PersonaEncoder(nn.Module):\n    def __init__(self, metacognitive_dim=16, output_dim=768):\n        super(PersonaEncoder, self).__init__()\n\n\n        self.metacognitive_fc = nn.Linear(metacognitive_dim, output_dim)  # from 16 to 768\n\n\n        self.final_fc = nn.Linear(output_dim, output_dim)\n\n    def forward(self, metacognitive_vector):\n\n        metacognitive_rep = self.metacognitive_fc(metacognitive_vector)\n\n\n        final_rep = self.final_fc(metacognitive_rep)  # (batch_size, output_dim)\n        persona_rep = final_rep.unsqueeze(1)\n\n        return persona_rep","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:20.597196Z","iopub.execute_input":"2024-12-20T06:25:20.597537Z","iopub.status.idle":"2024-12-20T06:25:20.602659Z","shell.execute_reply.started":"2024-12-20T06:25:20.597507Z","shell.execute_reply":"2024-12-20T06:25:20.601734Z"},"id":"eKtPH9jKMQ2A"},"outputs":[],"execution_count":18},{"cell_type":"code","source":"persona_encoder = PersonaEncoder().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:20.961019Z","iopub.execute_input":"2024-12-20T06:25:20.961388Z","iopub.status.idle":"2024-12-20T06:25:21.161992Z","shell.execute_reply.started":"2024-12-20T06:25:20.961356Z","shell.execute_reply":"2024-12-20T06:25:21.160913Z"},"id":"YdGgtQ1jMQ2B"},"outputs":[],"execution_count":19},{"cell_type":"code","source":"persona_encoder","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6o7Ei5HOQudE","outputId":"e1d7f092-2842-4fa8-d921-44731deda8fa","trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:22.392583Z","iopub.execute_input":"2024-12-20T06:25:22.392946Z","iopub.status.idle":"2024-12-20T06:25:22.398360Z","shell.execute_reply.started":"2024-12-20T06:25:22.392919Z","shell.execute_reply":"2024-12-20T06:25:22.397551Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"PersonaEncoder(\n  (metacognitive_fc): Linear(in_features=16, out_features=768, bias=True)\n  (final_fc): Linear(in_features=768, out_features=768, bias=True)\n)"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"class PAALayer(nn.Module):\n    def __init__(self, hidden_dimension = 768 , tau=0.5):\n        super(PAALayer, self).__init__()\n        self.hidden_dimenstion = hidden_dimension\n        self.tau = tau\n\n        # Linear layers for generating the initial weights (wpersona)\n        self.fc = nn.Linear(2 * hidden_dimension, hidden_dimension)  # Concatenated hR and oP\n        self.sigmoid = nn.Sigmoid()\n\n\n    def forward(self, hR , oP, oC):\n        #cross attention results\n        Mp_input  = torch.cat([hR,oP], dim=-1)#1\n        #print(\"Mp input:\", Mp_input.shape)\n        Mp = self.fc(Mp_input)#2\n        #print(\"Mp:\",Mp.shape)\n        Wp = self.sigmoid(Mp) #3\n        #print(\"Wp:\",Wp.shape)\n\n        #apply weighting = 4\n        Mpersona = (Wp > self.tau).float()\n        #print(\"Mpersona:\",Mpersona.shape)\n        Mcontext = (1 - Wp > self.tau).float()\n        #print(\"Mcontext:\",Mcontext.shape)\n\n        #apply masking to the cross attention / element wise multiplication = 5\n        oP_weighted = Mpersona * oP\n        #print(\"oP_weighted:\",oP_weighted.shape)\n        oC_weighted = Mcontext * oC\n        #print(\"oC_weighted:\",oC_weighted.shape)\n\n        #adding the cross attention results = 6\n        HPAA = oP_weighted + oC_weighted #output resulted from PAA layers\n        #print(\"HPAA:\",HPAA.shape)\n        return HPAA","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:23.441681Z","iopub.execute_input":"2024-12-20T06:25:23.442019Z","iopub.status.idle":"2024-12-20T06:25:23.447981Z","shell.execute_reply.started":"2024-12-20T06:25:23.441992Z","shell.execute_reply":"2024-12-20T06:25:23.447155Z"},"id":"mVXEKf38MQ2B"},"outputs":[],"execution_count":21},{"cell_type":"code","source":"PAA = PAALayer()","metadata":{"id":"DZ0V8nCUQ4JS","trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:24.673627Z","iopub.execute_input":"2024-12-20T06:25:24.674011Z","iopub.status.idle":"2024-12-20T06:25:24.689509Z","shell.execute_reply.started":"2024-12-20T06:25:24.673985Z","shell.execute_reply":"2024-12-20T06:25:24.688748Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"PAA","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UeRPYpIIQ6tR","outputId":"1ac92c99-c80d-4313-cb2d-7f6113708f53","trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:25.019313Z","iopub.execute_input":"2024-12-20T06:25:25.019670Z","iopub.status.idle":"2024-12-20T06:25:25.025687Z","shell.execute_reply.started":"2024-12-20T06:25:25.019616Z","shell.execute_reply":"2024-12-20T06:25:25.024718Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"PAALayer(\n  (fc): Linear(in_features=1536, out_features=768, bias=True)\n  (sigmoid): Sigmoid()\n)"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"class CustomTransformerBlock(nn.Module):\n    def __init__(self, hidden_size, tau):\n        super(CustomTransformerBlock, self).__init__()\n\n\n        self.context_attn = nn.MultiheadAttention(hidden_size, num_heads=12, batch_first=True)\n        self.persona_attn = nn.MultiheadAttention(hidden_size, num_heads=12, batch_first=True)\n\n        self.paa_layer = PAALayer(hidden_dimension=hidden_size, tau=tau)\n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_size, 2048),\n            nn.ReLU(),\n            nn.Dropout(p=0.1),\n            nn.Linear(2048, hidden_size),\n            nn.LayerNorm(hidden_size)\n        )\n\n\n\n    def forward(self, hR, encoded_persona, encoded_context):\n\n        # hR = hR.transpose(0, 1)\n        # encoded_persona = encoded_persona.transpose(0, 1)\n        # encoded_context = encoded_context.transpose(0, 1)\n\n        # transformer_output = self.transformer_decoder_layer(hR, encoded_persona)\n\n\n        oP, _ = self.persona_attn(hR, encoded_persona, encoded_persona)\n        oC, _ = self.context_attn(hR, encoded_context, encoded_context)\n\n\n        HPAA = self.paa_layer(hR, oP, oC)\n        return HPAA\n","metadata":{"id":"WakKFdR69W4G","trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:26.196911Z","iopub.execute_input":"2024-12-20T06:25:26.197218Z","iopub.status.idle":"2024-12-20T06:25:26.203588Z","shell.execute_reply.started":"2024-12-20T06:25:26.197195Z","shell.execute_reply":"2024-12-20T06:25:26.202726Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"custom = CustomTransformerBlock(768,0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:26:02.459891Z","iopub.execute_input":"2024-12-20T06:26:02.460235Z","iopub.status.idle":"2024-12-20T06:26:02.530432Z","shell.execute_reply.started":"2024-12-20T06:26:02.460210Z","shell.execute_reply":"2024-12-20T06:26:02.529632Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"custom","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:26:17.542936Z","iopub.execute_input":"2024-12-20T06:26:17.543246Z","iopub.status.idle":"2024-12-20T06:26:17.548769Z","shell.execute_reply.started":"2024-12-20T06:26:17.543209Z","shell.execute_reply":"2024-12-20T06:26:17.547878Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"CustomTransformerBlock(\n  (context_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (persona_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (paa_layer): PAALayer(\n    (fc): Linear(in_features=1536, out_features=768, bias=True)\n    (sigmoid): Sigmoid()\n  )\n  (mlp): Sequential(\n    (0): Linear(in_features=768, out_features=2048, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.1, inplace=False)\n    (3): Linear(in_features=2048, out_features=768, bias=True)\n    (4): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n)"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"class PAAModel(nn.Module):\n    def __init__(self, hidden_size=768, vocab_size = 50257 ,tau=0.5, max_length=512, num_transformer_blocks=6):\n        super(PAAModel , self).__init__()\n        self.hidden_size = hidden_size\n        self.tau = tau\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n        self.num_transformer_blocks = num_transformer_blocks\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n        #self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.tokenizer.add_special_tokens({'pad_token': '[BOS]'})\n        self.embedding = nn.Embedding(len(self.tokenizer), self.hidden_size)\n\n\n\n        #self.transformer_decoder = nn.TransformerDecoderLayer(d_model = hidden_size , nhead=8)\n        #self.decoder = nn.TransformerDecoder(self.transformer_decoder, num_layers=4)\n        self.gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')\n\n        # for param in self.gpt2.parameters():\n        #     param.requires_grad = False\n\n        self.decoder = self.gpt2.transformer\n        #self.decoder = GPT2LMHeadModel.from_pretrained('gpt2')\n\n\n\n        # self.context_attn = nn.MultiheadAttention(hidden_size, num_heads=12, batch_first=True)\n        # self.persona_attn = nn.MultiheadAttention(hidden_size, num_heads=12, batch_first=True)\n\n\n\n\n\n       # self.paa = PAALayer(hidden_dimension=hidden_size, tau=tau)\n\n        self.transformer_blocks = nn.ModuleList([CustomTransformerBlock(hidden_size, tau) for _ in range(num_transformer_blocks)])\n        self.final_fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, metacognitive_response , metacognitive_vector, encoded_context , encoded_persona):\n\n        tokenized_response = self.tokenizer(\n            metacognitive_response,\n            return_tensors='pt',\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length-16\n        )['input_ids'].to(device)\n\n        #print(\"tokenized response:\" ,tokenized_response.shape)\n        #print(\"tokenizer:\" , self.tokenizer)\n\n        metacognitive_response_emb = self.embedding(tokenized_response)\n        #print(\"metacognitive response embedding shape:\" , metacognitive_response_emb.shape)\n\n        #print(\"metacognitive vector long:\", metacognitive_vector.shape)\n        # metacognitive_vector_ids = torch.tensor(metacognitive_vector)\n        #print(\"metacognitive vector token ids shape:\", tokenized_vector.shape)\n        # metacognitive_vector_ids = metacognitive_vector_ids.expand(1, tokenized_response.shape[1])  # Adjust to the length of tokenized response\n        metacognitive_vector_emb = self.embedding(metacognitive_vector)\n        #print(\"metacognitive vector emb shape:\" , metacognitive_vector_emb.shape)\n        # print(\"metacognitive vector token ids shape:\", metacognitive_vector_ids.shape)\n\n        #deocder input - concatenate reposnse + vector = step 1\n        decoder_input = torch.cat([metacognitive_response_emb, metacognitive_vector_emb], dim=1)\n        #print(\"decoder input shape:\" ,decoder_input.shape)\n\n        #step 2 - self attention on input embedding\n        # Use a dummy memory tensor filled with zeros if no encoder memory is provided\n        #memory = torch.zeros((decoder_input.size(0), 1, self.hidden_size)).to(decoder_input.device)\n\n        inputs_embeds = decoder_input\n\n        transformer_output = inputs_embeds\n\n\n\n        outputs = self.decoder(inputs_embeds=inputs_embeds, output_hidden_states=True)\n        hR = (outputs.hidden_states)[-1]\n        #hR = self.decoder(decoder_input , memory)\n        #print(\"hR shape:\" , hR.shape)\n\n        transformer_output = hR\n        for transformer_block in self.transformer_blocks:\n            transformer_output = transformer_block(transformer_output, encoded_persona, encoded_context)\n\n        #step 3 = cross attention\n        # oP, _ = self.persona_attn(hR, encoded_persona, encoded_persona)\n        # oC, _ = self.context_attn(hR, encoded_context, encoded_context)\n        # #print(\"oP shape:\" , oP.shape)\n        # #print(\"oC shape:\" , oC.shape)\n\n\n        # #step 4 = apply PAA layer\n        # HPAA = self.paa(hR , oP, oC)\n        #print(\"HPAA shape:\"  , HPAA.shape)\n\n        #step 5 = linear output\n        logits = self.final_fc(transformer_output)\n        #print(\"logits shape:\" , logits.shape)\n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:27.383379Z","iopub.execute_input":"2024-12-20T06:25:27.383723Z","iopub.status.idle":"2024-12-20T06:25:27.392671Z","shell.execute_reply.started":"2024-12-20T06:25:27.383695Z","shell.execute_reply":"2024-12-20T06:25:27.391745Z"},"id":"re3yOa5GMQ2B"},"outputs":[],"execution_count":25},{"cell_type":"code","source":"model = PAAModel()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:31.794359Z","iopub.execute_input":"2024-12-20T06:25:31.794709Z","iopub.status.idle":"2024-12-20T06:25:36.549999Z","shell.execute_reply.started":"2024-12-20T06:25:31.794668Z","shell.execute_reply":"2024-12-20T06:25:36.549290Z"},"id":"6TuMeoSwMQ2B"},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4858b17945ec450d94c2e5b8a0108af7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b6e8ca9b5f84392b6f096481d3f3742"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"566a6c003bd14944815d863b47e9a271"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6672e7ffa1e2467dbb66a6869d6c7393"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c37b819b5794b4db23f1d56d83c8b94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f10f95b9d94347a9b802732b9bb48d3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc3b6b6b9bf24657a2466e429fe3f52a"}},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:25:36.551214Z","iopub.execute_input":"2024-12-20T06:25:36.551542Z","iopub.status.idle":"2024-12-20T06:25:36.895524Z","shell.execute_reply.started":"2024-12-20T06:25:36.551511Z","shell.execute_reply":"2024-12-20T06:25:36.894528Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"RlLW7ET3MQ2B","outputId":"651fc418-c189-4ecb-a273-e6e152db8684"},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"PAAModel(\n  (embedding): Embedding(50258, 768)\n  (gpt2): GPT2LMHeadModel(\n    (transformer): GPT2Model(\n      (wte): Embedding(50257, 768)\n      (wpe): Embedding(1024, 768)\n      (drop): Dropout(p=0.1, inplace=False)\n      (h): ModuleList(\n        (0-11): 12 x GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2SdpaAttention(\n            (c_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D()\n            (c_proj): Conv1D()\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n  )\n  (decoder): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2SdpaAttention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (transformer_blocks): ModuleList(\n    (0-5): 6 x CustomTransformerBlock(\n      (context_attn): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n      )\n      (persona_attn): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n      )\n      (paa_layer): PAALayer(\n        (fc): Linear(in_features=1536, out_features=768, bias=True)\n        (sigmoid): Sigmoid()\n      )\n      (mlp): Sequential(\n        (0): Linear(in_features=768, out_features=2048, bias=True)\n        (1): ReLU()\n        (2): Dropout(p=0.1, inplace=False)\n        (3): Linear(in_features=2048, out_features=768, bias=True)\n        (4): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (final_fc): Linear(in_features=768, out_features=50257, bias=True)\n)"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"import torch.optim as optim\noptimizer = optim.AdamW(model.parameters(), lr=5e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:26:34.031134Z","iopub.execute_input":"2024-12-20T06:26:34.031509Z","iopub.status.idle":"2024-12-20T06:26:34.539585Z","shell.execute_reply.started":"2024-12-20T06:26:34.031479Z","shell.execute_reply":"2024-12-20T06:26:34.538792Z"},"id":"Ofr2QksoMQ2C"},"outputs":[],"execution_count":31},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:26:34.540805Z","iopub.execute_input":"2024-12-20T06:26:34.541294Z","iopub.status.idle":"2024-12-20T06:26:34.711608Z","shell.execute_reply.started":"2024-12-20T06:26:34.541253Z","shell.execute_reply":"2024-12-20T06:26:34.710913Z"},"id":"r5UYI-QYMQ2C"},"outputs":[],"execution_count":32},{"cell_type":"code","source":"LOSS = torch.nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:26:34.718521Z","iopub.execute_input":"2024-12-20T06:26:34.718838Z","iopub.status.idle":"2024-12-20T06:26:34.722666Z","shell.execute_reply.started":"2024-12-20T06:26:34.718813Z","shell.execute_reply":"2024-12-20T06:26:34.721809Z"},"id":"VlodtWqdMQ2C"},"outputs":[],"execution_count":33},{"cell_type":"code","source":"num_epochs = 100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T07:18:24.460293Z","iopub.execute_input":"2024-12-20T07:18:24.460666Z","iopub.status.idle":"2024-12-20T07:18:24.464363Z","shell.execute_reply.started":"2024-12-20T07:18:24.460609Z","shell.execute_reply":"2024-12-20T07:18:24.463375Z"},"id":"9tD9sJYpMQ2C"},"outputs":[],"execution_count":47},{"cell_type":"code","source":"import ast","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:26:36.763531Z","iopub.execute_input":"2024-12-20T06:26:36.763889Z","iopub.status.idle":"2024-12-20T06:26:36.768058Z","shell.execute_reply.started":"2024-12-20T06:26:36.763863Z","shell.execute_reply":"2024-12-20T06:26:36.767034Z"},"id":"qSl4FVMmMQ2C"},"outputs":[],"execution_count":35},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:26:37.179290Z","iopub.execute_input":"2024-12-20T06:26:37.179680Z","iopub.status.idle":"2024-12-20T06:26:37.183578Z","shell.execute_reply.started":"2024-12-20T06:26:37.179625Z","shell.execute_reply":"2024-12-20T06:26:37.182626Z"},"id":"FplsCJS5MQ2C"},"outputs":[],"execution_count":36},{"cell_type":"code","source":"for batch_idx, (description, student_code, feedback, metacognitive_vector, target_labels) in enumerate(dataloader):\n    print(description)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T05:12:05.483903Z","iopub.execute_input":"2024-12-19T05:12:05.484237Z","iopub.status.idle":"2024-12-19T05:12:05.527671Z","shell.execute_reply.started":"2024-12-19T05:12:05.484208Z","shell.execute_reply":"2024-12-19T05:12:05.526474Z"},"id":"Y1DrPvM-MQ2C","outputId":"b88ae667-0d30-4815-9798-203c53b4c787","colab":{"base_uri":"https://localhost:8080/","height":158}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'dataloader' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-6bb24012aea9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeedback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetacognitive_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"]}],"execution_count":42},{"cell_type":"code","source":"df.columns","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fz1cHZsgOabJ","outputId":"b1a2393f-441e-4cf5-cad8-15f5f5c40012"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['question', 'difficulty', 'prefer_solution', 'flaw_solution',\n","       'random_col_1', 'random_col_2', 'random_col_3', 'random_col_4',\n","       'random_col_5', 'random_col_6', 'random_col_7', 'random_col_8',\n","       'random_col_9', 'random_col_10', 'random_col_11', 'random_col_12',\n","       'random_col_13', 'random_col_14', 'random_col_15', 'random_col_16',\n","       'metacognitive_vector', 'metacognitive_feedback'],\n","      dtype='object')"]},"metadata":{},"execution_count":33}],"execution_count":33},{"cell_type":"code","source":"df_train = df[:400]","metadata":{"id":"lQjbXIZ5Pt4d","trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:26:41.566957Z","iopub.execute_input":"2024-12-20T06:26:41.567298Z","iopub.status.idle":"2024-12-20T06:26:41.572252Z","shell.execute_reply.started":"2024-12-20T06:26:41.567273Z","shell.execute_reply":"2024-12-20T06:26:41.571190Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"print(\"hhh\",end='\\r')\nprint(\"fff\",end='\\r')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xDfhatunSCZ1","outputId":"f70e3115-dae3-471a-db3e-02cb2cbdde5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["hhh\rfff\r"]}],"execution_count":45},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    print(\"training started:\", epoch)\n    model.train()\n    total_loss = 0\n\n    for idx,row in df_train.iterrows():\n        print(f\"{idx+1} - {len(df_train)}\", end='\\r')\n        description = row['question']\n        student_code = row['flaw_solution']\n        feedback = row['prefer_solution']\n        metacognitive_feedback = row['metacognitive_feedback']\n        metacognitive_vector= row['metacognitive_vector']\n        #print(\"metacognitive_vector:\" ,metacognitive_vector)\n\n        metacognitive_tensor = torch.tensor([\n            ast.literal_eval(metacognitive_vector) ], dtype=torch.float).to(device)\n\n        #print(\"metacognitive vector shape:\", metacognitive_tensor.shape)\n        #print(metacognitive_tensor)\n        #print(\"metacognitive vector shape unsqueeze:\", metacognitive_tensor.unsqueeze(1).shape)\n        target_labels = torch.tensor(\n            tokenizer.encode(metacognitive_feedback, max_length=512, truncation=True, padding=\"max_length\")\n        ).to(device)\n\n        #metacognitive_feedback = ast.literal_eval(metacognitive_feedback)\n        #metacognitive_feedback_tensor = torch.tensor(metacognitive_feedback, dtype=torch.float32).unsqueeze(0)\n\n\n        context_encoding = context_encoder(description, student_code , feedback).to(device)\n        persona_encoding = persona_encoder(metacognitive_tensor).to(device)\n        #print(\"encoded context:\",context_encoding.shape)\n        #print(\"encoded persona:\",persona_encoding.shape)\n\n\n\n        logits = model(feedback, metacognitive_tensor.long(), context_encoding, persona_encoding)\n        #print(\"logits.shape\" ,logits.shape)\n\n        logits = logits.view(-1, logits.size(-1))\n        target_ids = target_labels.view(-1)\n\n        #print(\"target labels:\" , (logits.shape, target_ids.shape))\n\n        #print(\"Target label values:\", target_labels)\n        # print(\"Maximum target value:\", target_labels.max())\n        # print(\"Model vocab size:\", model.vocab_size)\n        # print(\"------------------------------------------------\")\n        # print(\"target:\",target_ids)\n        # print(\"------------------------------------------------\")\n        # print(\"logits:\",logits)\n        loss = LOSS(logits,target_ids)\n\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"\\nEpoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(df_train):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T07:19:14.419728Z","iopub.execute_input":"2024-12-20T07:19:14.420083Z","execution_failed":"2024-12-20T08:55:16.273Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"wNneWHl6MQ2D","outputId":"fc0770e2-c214-43ac-80e4-ef2516af1c22"},"outputs":[{"name":"stdout","text":"training started: 0\n400 - 400\nEpoch [1/100], Loss: 4.4643\ntraining started: 1\n400 - 400\nEpoch [2/100], Loss: 4.4641\ntraining started: 2\n400 - 400\nEpoch [3/100], Loss: 4.4642\ntraining started: 3\n400 - 400\nEpoch [4/100], Loss: 4.4643\ntraining started: 4\n400 - 400\nEpoch [5/100], Loss: 4.4641\ntraining started: 5\n400 - 400\nEpoch [6/100], Loss: 4.4644\ntraining started: 6\n400 - 400\nEpoch [7/100], Loss: 4.4642\ntraining started: 7\n400 - 400\nEpoch [8/100], Loss: 4.4643\ntraining started: 8\n400 - 400\nEpoch [9/100], Loss: 4.4641\ntraining started: 9\n400 - 400\nEpoch [10/100], Loss: 4.4643\ntraining started: 10\n58 - 400\r","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Save model state dict\ntorch.save(model.state_dict(), 'paamodel.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:55:48.683995Z","iopub.status.idle":"2024-12-20T08:55:48.684308Z","shell.execute_reply":"2024-12-20T08:55:48.684184Z"},"id":"Ub58r2MXMQ2D"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the model before loading\nmodel = PAAModel()\n\n# Load the saved state dict\nmodel.load_state_dict(torch.load('paamodel.pth'))\n\n# Set to evaluation mode if needed\n#model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:55:48.685264Z","iopub.status.idle":"2024-12-20T08:55:48.685622Z","shell.execute_reply":"2024-12-20T08:55:48.685481Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"cImrvyu9MQ2D","outputId":"a31425a8-b277-4c11-da10-0c3320c62555"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inference(model, context_encoder, persona_encoder, description, student_code, feedback, metacognitive_feedback,metacognitive_vector):\n    model.eval()\n    model.to(device) # Set the model to evaluation mode\n    with torch.no_grad():  # Disable gradient computation for inference\n        # Step 1: Convert the input data to the necessary tensors\n        metacognitive_tensor = torch.tensor(\n            [ast.literal_eval(metacognitive_vector)], dtype=torch.float\n        ).to(device)\n\n        # Step 2: Encode context and persona using the respective encoders\n        context_encoding = context_encoder(description, student_code, feedback).to(device)  # Shape: [1, 768]\n        persona_encoding = persona_encoder(metacognitive_tensor).to(device)  # Shape: [1, 768]\n\n        # Step 3: Prepare the metacognitive response (this is the input for the decoder)\n        # metacognitive_response = tokenizer.encode(\n        #                         metacognitive_feedback,\n        #                         padding=True,\n        #                         truncation=True,\n        #                         max_length=512,\n        #                         return_tensors='pt'\n        #                     )['input_ids']\n\n        # Step 4: Pass everything through the model (no gradients needed for inference)\n        logits = model(metacognitive_feedback, metacognitive_tensor.long(), context_encoding, persona_encoding)\n\n        # Step 5: Convert logits to predictions (for simplicity, let's take the argmax here)\n        predictions = torch.argmax(logits, dim=-1)  # Taking argmax along the vocabulary dimension\n\n        # Step 6: If needed, convert the predicted tokens back to text (this step depends on your output format)\n        predicted_tokens = predictions.squeeze().tolist()\n        decoded_text = tokenizer.decode(predicted_tokens, skip_special_tokens=True)\n\n        # Convert tokens to text (if using a tokenizer for language generation, replace this step)\n        # For simplicity, assuming the output is a sequence of tokens\n        return predicted_tokens,decoded_text\n\n\n\n","metadata":{"trusted":true,"id":"wvaT28ZUMQ2D","execution":{"iopub.status.busy":"2024-12-20T08:55:48.686331Z","iopub.status.idle":"2024-12-20T08:55:48.686637Z","shell.execute_reply":"2024-12-20T08:55:48.686514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Sb1pj7ZZbEC","outputId":"b45cad96-68ca-4a6d-fdf3-e0772593afbc","trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:55:48.805384Z","iopub.execute_input":"2024-12-20T08:55:48.805804Z","iopub.status.idle":"2024-12-20T08:55:48.816799Z","shell.execute_reply.started":"2024-12-20T08:55:48.805762Z","shell.execute_reply":"2024-12-20T08:55:48.814877Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-b666bf274d0a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"],"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"row = df.iloc[450]\n\ndescription = row['question']\nstudent_code = row['flaw_solution']\nfeedback = row['prefer_solution']\nmetacognitive_feedback = row['metacognitive_feedback']\nmetacognitive_vector= row['metacognitive_vector']","metadata":{"trusted":true,"id":"4Omk6CfoMQ2E","execution":{"iopub.status.busy":"2024-12-20T08:55:48.817584Z","iopub.status.idle":"2024-12-20T08:55:48.817978Z","shell.execute_reply":"2024-12-20T08:55:48.817834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predicted_tokens ,predictions= inference(model, context_encoder, persona_encoder, description, student_code, feedback, metacognitive_feedback,metacognitive_vector)","metadata":{"trusted":true,"id":"uBx1VLteMQ2E","execution":{"iopub.status.busy":"2024-12-20T08:55:48.819312Z","iopub.status.idle":"2024-12-20T08:55:48.819766Z","shell.execute_reply":"2024-12-20T08:55:48.819577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"LVtcsonpMQ2E","outputId":"9345ee91-b115-4881-bcd1-a84c7d8ca0d1","execution":{"iopub.status.busy":"2024-12-20T08:55:48.820659Z","iopub.status.idle":"2024-12-20T08:55:48.821037Z","shell.execute_reply":"2024-12-20T08:55:48.820907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"_YplAx7cZkTW"},"outputs":[],"execution_count":null}]}