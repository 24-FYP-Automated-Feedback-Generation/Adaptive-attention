{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10048507,"sourceType":"datasetVersion","datasetId":6190931},{"sourceId":10242967,"sourceType":"datasetVersion","datasetId":6334477},{"sourceId":10316683,"sourceType":"datasetVersion","datasetId":6386966}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/cross-attentive-adaptaion-1-42e20c40-7d10-4162-a18f-309d0acc6c84.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20250102/auto/storage/goog4_request&X-Goog-Date=20250102T105331Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=92b18c91ac0391ce0f352019d957ea52f7551e139baab2f4c2d081e7b00cb3f45fdac366f022f08f3e38baf8177fd795fc8c3fbd8cad5cf56a5768e1f52ecd6947542b4909e5a9704e2c78165792da412f67a2b5534420e56e9210a50dc822df79a0f3bb4d052e68727023669ed37f2efb85edb1d0d5fdf1d1e463a7004504deba440307442799c3c2b8c08fe8201d4f5c87f259697732dcad9ac39bece48fa8c1893336ef7d31b901d26c6edfc0d56ea8bdc41c37a1801047f9dba70ca1e3df6573910e4c014ed26b38930e479a44a1e54ca15f5838a9a6213b67a6a320eb0377e57092dc5909688f060cd8995b0bfa7bcccc11ea2ad546230a88177d6e0307","timestamp":1736008492951}],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"43da2f47170345cdac7f72b7aa84662d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_89cfdd8672f548ca8d8722b5c39c240e","IPY_MODEL_a4ecd892759c4a759d069e293e15b779","IPY_MODEL_9aa1e481e9ad45678462f48428086ace"],"layout":"IPY_MODEL_f3027b6fbe7048378dd09030b5bd55d5"}},"89cfdd8672f548ca8d8722b5c39c240e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e0f7c835fd643c98686a30e7a7062fe","placeholder":"​","style":"IPY_MODEL_ad0642d1ee6e4d5381fb0fa34a4cf092","value":"config.json: 100%"}},"a4ecd892759c4a759d069e293e15b779":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d36ea6e57e142c1b40a79c8f4b8f6de","max":1208,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26e3066c9eb940c297966090d0d235d8","value":1208}},"9aa1e481e9ad45678462f48428086ace":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_feee58e7de1340e3860c9a95ff4be60e","placeholder":"​","style":"IPY_MODEL_2976f27258574b5d867dfdcbe16f9769","value":" 1.21k/1.21k [00:00&lt;00:00, 53.0kB/s]"}},"f3027b6fbe7048378dd09030b5bd55d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e0f7c835fd643c98686a30e7a7062fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad0642d1ee6e4d5381fb0fa34a4cf092":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d36ea6e57e142c1b40a79c8f4b8f6de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26e3066c9eb940c297966090d0d235d8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"feee58e7de1340e3860c9a95ff4be60e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2976f27258574b5d867dfdcbe16f9769":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"749891e75dea4320a86201f0d4e19c16":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3958951756e54804a17eae58aa6ba0dc","IPY_MODEL_1255530944b14d6099c53a92b2bb2bff","IPY_MODEL_eeb45b3f646a49f0bdee3d80c96685f5"],"layout":"IPY_MODEL_e2f860e4136747159aa03f5d0a63d368"}},"3958951756e54804a17eae58aa6ba0dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e539e4ce810e416986a5fa7f70494395","placeholder":"​","style":"IPY_MODEL_1d3f7e377c3348948b55485857e699f1","value":"spiece.model: 100%"}},"1255530944b14d6099c53a92b2bb2bff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3add81bdfdc2431cb7e774e4fa914292","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f85aef5dcd64053acd7128663657eb0","value":791656}},"eeb45b3f646a49f0bdee3d80c96685f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9fc7544d1fc4848a6161195104e1254","placeholder":"​","style":"IPY_MODEL_ee7e1d8f427d4b698a0ad3fa19904269","value":" 792k/792k [00:00&lt;00:00, 3.72MB/s]"}},"e2f860e4136747159aa03f5d0a63d368":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e539e4ce810e416986a5fa7f70494395":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d3f7e377c3348948b55485857e699f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3add81bdfdc2431cb7e774e4fa914292":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f85aef5dcd64053acd7128663657eb0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a9fc7544d1fc4848a6161195104e1254":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee7e1d8f427d4b698a0ad3fa19904269":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e04aa00ec6bf4c4cac5f87cf82a27c54":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7ec5c7b696ff4d7098aa0681b6ca625c","IPY_MODEL_9d530a87310d4c47bfa18a06fca19e17","IPY_MODEL_35a0e3310300473b97150e41c0b23339"],"layout":"IPY_MODEL_2d05e85d3811459e92522fe7c44cc95a"}},"7ec5c7b696ff4d7098aa0681b6ca625c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_744b2bc8bdb346c79694d3164fb8ceec","placeholder":"​","style":"IPY_MODEL_d03e94b61e1840e3b7f0ec3c6ae37b73","value":"tokenizer.json: 100%"}},"9d530a87310d4c47bfa18a06fca19e17":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_da66806b1f8f4a8faacf076045688d5f","max":1389353,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f000bf67102b4b0bb9322d272114958b","value":1389353}},"35a0e3310300473b97150e41c0b23339":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1770b5d95b514882bdc265bada08474f","placeholder":"​","style":"IPY_MODEL_6f0779e020d44f9396ca5294b3d3975a","value":" 1.39M/1.39M [00:00&lt;00:00, 6.71MB/s]"}},"2d05e85d3811459e92522fe7c44cc95a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"744b2bc8bdb346c79694d3164fb8ceec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d03e94b61e1840e3b7f0ec3c6ae37b73":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da66806b1f8f4a8faacf076045688d5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f000bf67102b4b0bb9322d272114958b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1770b5d95b514882bdc265bada08474f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f0779e020d44f9396ca5294b3d3975a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b4b9829c79e4df2b7f6159d4a54634a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7574a2d7e4984e01ae8043545e7f3afb","IPY_MODEL_b28caa6bf2c745ec87cf3da06ecf4a7a","IPY_MODEL_cd57ad7ae6aa4ced911bf44e8b5437d0"],"layout":"IPY_MODEL_5c3722f465704e12bd93bb12220b42e3"}},"7574a2d7e4984e01ae8043545e7f3afb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c98216f497743dcb8abb7b69be97942","placeholder":"​","style":"IPY_MODEL_9c817b66deb1488b95479ae5667b71cf","value":"tokenizer_config.json: 100%"}},"b28caa6bf2c745ec87cf3da06ecf4a7a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7e2b480b24244919ec20c23720905a7","max":26,"min":0,"orientation":"horizontal","style":"IPY_MODEL_214c9d061b734b03bd0060ef66fb8c02","value":26}},"cd57ad7ae6aa4ced911bf44e8b5437d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_770dcb4cf22d4035a8b8970aed70412b","placeholder":"​","style":"IPY_MODEL_17209e71fc73454b99b23f617c5643d0","value":" 26.0/26.0 [00:00&lt;00:00, 465B/s]"}},"5c3722f465704e12bd93bb12220b42e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c98216f497743dcb8abb7b69be97942":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c817b66deb1488b95479ae5667b71cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7e2b480b24244919ec20c23720905a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"214c9d061b734b03bd0060ef66fb8c02":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"770dcb4cf22d4035a8b8970aed70412b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17209e71fc73454b99b23f617c5643d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3aa1fd17abe44555ad47e0aef4c0f938":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1bd6bdbc2e204509b9c209417733f4a3","IPY_MODEL_b8ac8ee34b73426190aebf20a0035eae","IPY_MODEL_4cf598feee2b4c71938150e26c9cb341"],"layout":"IPY_MODEL_0ceaa855fc724594bf8c64a8a1926e01"}},"1bd6bdbc2e204509b9c209417733f4a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d80c6058f22a47cba18ed8e9eb1918ae","placeholder":"​","style":"IPY_MODEL_13f8d88212184e9889255c7e6f3f6dfd","value":"vocab.json: 100%"}},"b8ac8ee34b73426190aebf20a0035eae":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_de559a29729142c2ae3a9641c96478c4","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6df5febabe3c4062ab1c055fbea5a62b","value":1042301}},"4cf598feee2b4c71938150e26c9cb341":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfa0c3241c3f4d9088aa3a8ea8b45f70","placeholder":"​","style":"IPY_MODEL_ebcecc565e484bf0933b3910d4566726","value":" 1.04M/1.04M [00:00&lt;00:00, 5.02MB/s]"}},"0ceaa855fc724594bf8c64a8a1926e01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d80c6058f22a47cba18ed8e9eb1918ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13f8d88212184e9889255c7e6f3f6dfd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de559a29729142c2ae3a9641c96478c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6df5febabe3c4062ab1c055fbea5a62b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dfa0c3241c3f4d9088aa3a8ea8b45f70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebcecc565e484bf0933b3910d4566726":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3809873b8cf045c2a00ba0d62261f6af":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3d86694625d04af094c9beef8a589b16","IPY_MODEL_d5f9e809db154bbfaf908195b325a34a","IPY_MODEL_fc953cc050dd4280846cd6e1219019e7"],"layout":"IPY_MODEL_9c27818ce29f4d4c97bc4b55efad78e5"}},"3d86694625d04af094c9beef8a589b16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_617a9be3796a4ec8a0377b82f497635e","placeholder":"​","style":"IPY_MODEL_8309e9644c074f83ab4f4fd9a99582c6","value":"merges.txt: 100%"}},"d5f9e809db154bbfaf908195b325a34a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d533ecc7ab649aaa98a0712db11376c","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_357500baa89547c39282e086ab7a7999","value":456318}},"fc953cc050dd4280846cd6e1219019e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_942420c6462c47ca8a0e09321e60a1b8","placeholder":"​","style":"IPY_MODEL_9f5ed38aa7a94a00b373febd383104ca","value":" 456k/456k [00:00&lt;00:00, 13.6MB/s]"}},"9c27818ce29f4d4c97bc4b55efad78e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"617a9be3796a4ec8a0377b82f497635e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8309e9644c074f83ab4f4fd9a99582c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d533ecc7ab649aaa98a0712db11376c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"357500baa89547c39282e086ab7a7999":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"942420c6462c47ca8a0e09321e60a1b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f5ed38aa7a94a00b373febd383104ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e801240ba7b42378f27c301c0fcac29":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7b0cabdc4da749228879868a6f7e34e3","IPY_MODEL_9fabc1f28c5248d8a9ced22a2f37890c","IPY_MODEL_95879c33ac2c4568a8b1fbe4f4004009"],"layout":"IPY_MODEL_421fb5dee92b40e0b94ac250bdc13adc"}},"7b0cabdc4da749228879868a6f7e34e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbf744dc7b674882b25a67ececf38fe3","placeholder":"​","style":"IPY_MODEL_ff2f0600478f4801a2243cb634722196","value":"tokenizer.json: 100%"}},"9fabc1f28c5248d8a9ced22a2f37890c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0bffe6826a9406dad9fbc75e1dc99e2","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e1c977ccc074a42b090867b860297b9","value":1355256}},"95879c33ac2c4568a8b1fbe4f4004009":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f73dfe68ceb4c2689ad142e0664340a","placeholder":"​","style":"IPY_MODEL_ebeab6709a6e40e58a661585be171203","value":" 1.36M/1.36M [00:00&lt;00:00, 6.66MB/s]"}},"421fb5dee92b40e0b94ac250bdc13adc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbf744dc7b674882b25a67ececf38fe3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff2f0600478f4801a2243cb634722196":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0bffe6826a9406dad9fbc75e1dc99e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e1c977ccc074a42b090867b860297b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3f73dfe68ceb4c2689ad142e0664340a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebeab6709a6e40e58a661585be171203":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44d5dfde2cea49ffb47e9fa82d182502":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_78c2f42c86e644469f1697ace4f1e0d1","IPY_MODEL_bde9128d864947c7918a2192aa8bbdcb","IPY_MODEL_cf1a5f16f250435082e836611374688a"],"layout":"IPY_MODEL_c9185e9b796c4e14be13dc4ebb223e1b"}},"78c2f42c86e644469f1697ace4f1e0d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e24b589fab34b389b4b7b1bab26f624","placeholder":"​","style":"IPY_MODEL_5a6158b3e2934c5d9e2b15c2f6951031","value":"config.json: 100%"}},"bde9128d864947c7918a2192aa8bbdcb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aaa6919585e543969a70349ec8f870da","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9140609d187f48869561947744cda081","value":665}},"cf1a5f16f250435082e836611374688a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bb1f013da664d34a26d042c062d8336","placeholder":"​","style":"IPY_MODEL_ee1033c0ac7245358b4aaeb3ae3e2502","value":" 665/665 [00:00&lt;00:00, 15.5kB/s]"}},"c9185e9b796c4e14be13dc4ebb223e1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e24b589fab34b389b4b7b1bab26f624":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a6158b3e2934c5d9e2b15c2f6951031":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aaa6919585e543969a70349ec8f870da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9140609d187f48869561947744cda081":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7bb1f013da664d34a26d042c062d8336":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee1033c0ac7245358b4aaeb3ae3e2502":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n# THEN FEEL FREE TO DELETE THIS CELL.\n# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# NOTEBOOK.\nimport kagglehub\nuom200407h_modified_dataset_path = kagglehub.dataset_download('uom200407h/modified-dataset')\nuom200644f_metacognitive_dataset_path = kagglehub.dataset_download('uom200644f/metacognitive-dataset')\nbashadithennakoon_metacognitive_feedback_for_algorithm_solving_path = kagglehub.dataset_download('bashadithennakoon/metacognitive-feedback-for-algorithm-solving')\n\nprint('Data source import complete.')\n","metadata":{"id":"WZPr7xkva7Id"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"XOEjCcHQa7Ii","execution":{"iopub.status.busy":"2025-01-10T08:41:16.945479Z","iopub.execute_input":"2025-01-10T08:41:16.945780Z","iopub.status.idle":"2025-01-10T08:41:17.237950Z","shell.execute_reply.started":"2025-01-10T08:41:16.945752Z","shell.execute_reply":"2025-01-10T08:41:17.237016Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/metacognitive-dataset/metacognitive-dataset.csv\n/kaggle/input/modified-dataset/modified_dataset.csv\n/kaggle/input/metacognitive-feedback-for-algorithm-solving/final_dataset_with_annotated_metacognitive_feedback_gpt-4o-mini.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install transformers torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:19.274393Z","iopub.execute_input":"2025-01-10T08:41:19.274823Z","iopub.status.idle":"2025-01-10T08:41:23.674720Z","shell.execute_reply.started":"2025-01-10T08:41:19.274798Z","shell.execute_reply":"2025-01-10T08:41:23.673610Z"},"id":"TYCjdbZUa7Ij","executionInfo":{"status":"ok","timestamp":1736007590391,"user_tz":-330,"elapsed":3960,"user":{"displayName":"","userId":""}},"outputId":"1854ce93-b297-4c35-e5fc-59e5bbd37f61"},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import AutoModel, AutoTokenizer, GPT2Model,GPT2Tokenizer,GPT2LMHeadModel\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration , T5Tokenizer , T5Model\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:23.676340Z","iopub.execute_input":"2025-01-10T08:41:23.676649Z","iopub.status.idle":"2025-01-10T08:41:28.598474Z","shell.execute_reply.started":"2025-01-10T08:41:23.676627Z","shell.execute_reply":"2025-01-10T08:41:28.597727Z"},"id":"IgJFcjXoa7Il"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:28.599923Z","iopub.execute_input":"2025-01-10T08:41:28.600346Z","iopub.status.idle":"2025-01-10T08:41:28.653213Z","shell.execute_reply.started":"2025-01-10T08:41:28.600323Z","shell.execute_reply":"2025-01-10T08:41:28.652355Z"},"id":"DpWbDwTja7Im"},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:28.654450Z","iopub.execute_input":"2025-01-10T08:41:28.654669Z","iopub.status.idle":"2025-01-10T08:41:28.673965Z","shell.execute_reply.started":"2025-01-10T08:41:28.654648Z","shell.execute_reply":"2025-01-10T08:41:28.673166Z"},"id":"kdvnV1Efa7Im","executionInfo":{"status":"ok","timestamp":1736007616811,"user_tz":-330,"elapsed":5,"user":{"displayName":"","userId":""}},"outputId":"802e1d3c-dc5d-41ac-d16a-f50ffb3dacf8"},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"checkpoint = \"t5-base\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:28.674895Z","iopub.execute_input":"2025-01-10T08:41:28.675188Z","iopub.status.idle":"2025-01-10T08:41:28.688943Z","shell.execute_reply.started":"2025-01-10T08:41:28.675159Z","shell.execute_reply":"2025-01-10T08:41:28.688333Z"},"id":"OvfQPyhKa7In"},"outputs":[],"execution_count":6},{"cell_type":"code","source":"t5_tokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:28.689713Z","iopub.execute_input":"2025-01-10T08:41:28.690010Z","iopub.status.idle":"2025-01-10T08:41:29.526736Z","shell.execute_reply.started":"2025-01-10T08:41:28.689981Z","shell.execute_reply":"2025-01-10T08:41:29.525763Z"},"id":"EPXot-9Sa7In","executionInfo":{"status":"ok","timestamp":1736007620230,"user_tz":-330,"elapsed":3422,"user":{"displayName":"","userId":""}},"outputId":"c202dd01-1b26-4400-cd53-9c6f676a8c2a"},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b7e83d7d27b49a698e61a2d145fe676"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cd0279fd43349ff90ccea2d1a1af33c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33a5304519034065b2a848de8db6518b"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"t5_tokenizer.vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:29.528609Z","iopub.execute_input":"2025-01-10T08:41:29.528820Z","iopub.status.idle":"2025-01-10T08:41:29.533409Z","shell.execute_reply.started":"2025-01-10T08:41:29.528802Z","shell.execute_reply":"2025-01-10T08:41:29.532639Z"},"id":"qX1l99_3a7Io","executionInfo":{"status":"ok","timestamp":1736007620230,"user_tz":-330,"elapsed":3,"user":{"displayName":"","userId":""}},"outputId":"803d67af-d77e-4ecf-8be7-b3813bdd5551"},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"32100"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"#set the max length to model's default present max length\nt5_tokenizer.model_max_length = t5_tokenizer.model_max_length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:29.534313Z","iopub.execute_input":"2025-01-10T08:41:29.534511Z","iopub.status.idle":"2025-01-10T08:41:29.548321Z","shell.execute_reply.started":"2025-01-10T08:41:29.534494Z","shell.execute_reply":"2025-01-10T08:41:29.547542Z"},"id":"9XRWaOWWa7Ip"},"outputs":[],"execution_count":9},{"cell_type":"code","source":"gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:29.549039Z","iopub.execute_input":"2025-01-10T08:41:29.549354Z","iopub.status.idle":"2025-01-10T08:41:30.418151Z","shell.execute_reply.started":"2025-01-10T08:41:29.549313Z","shell.execute_reply":"2025-01-10T08:41:30.417486Z"},"id":"tLqjpzCxa7Ip","executionInfo":{"status":"ok","timestamp":1736007623206,"user_tz":-330,"elapsed":2269,"user":{"displayName":"","userId":""}},"outputId":"fd776e6d-245d-4ab9-cc78-99e969b0a830"},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfb2a587745b4f0182d68d485b53a08a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"633fe29b8521477399b0d175392c4d8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"229ab741aa1641f2a69ac324d096722e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a5d2018f2a94fd4941d27d9392236ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faeb17487dfa4280a4247395e133ff70"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:30.418890Z","iopub.execute_input":"2025-01-10T08:41:30.419091Z","iopub.status.idle":"2025-01-10T08:41:30.422588Z","shell.execute_reply.started":"2025-01-10T08:41:30.419073Z","shell.execute_reply":"2025-01-10T08:41:30.421928Z"},"id":"LVVTv7zfa7Iq"},"outputs":[],"execution_count":11},{"cell_type":"code","source":"file_path = \"/kaggle/input/metacognitive-feedback-for-algorithm-solving/final_dataset_with_annotated_metacognitive_feedback_gpt-4o-mini.csv\"\ndf = pd.read_csv(file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:30.423985Z","iopub.execute_input":"2025-01-10T08:41:30.424304Z","iopub.status.idle":"2025-01-10T08:41:32.299950Z","shell.execute_reply.started":"2025-01-10T08:41:30.424258Z","shell.execute_reply":"2025-01-10T08:41:32.299214Z"},"id":"XrY_OzWca7Iq","executionInfo":{"status":"error","timestamp":1736007624024,"user_tz":-330,"elapsed":822,"user":{"displayName":"","userId":""}},"outputId":"3eb44ee1-99fa-46b6-becc-3de1a7578cb1"},"outputs":[],"execution_count":12},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:32.301132Z","iopub.execute_input":"2025-01-10T08:41:32.301393Z","iopub.status.idle":"2025-01-10T08:41:32.307470Z","shell.execute_reply.started":"2025-01-10T08:41:32.301370Z","shell.execute_reply":"2025-01-10T08:41:32.306653Z"},"id":"nqzlVJm_a7Ir"},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Index(['Question 1', 'Response 1', 'Right answer 1', 'Q01', 'Q02', 'Q03',\n       'Q04', 'Q05', 'Q06', 'Q07', 'Q08', 'Q09', 'Q10', 'Q11', 'Q12', 'Q13',\n       'Q14', 'Q15', 'Q16', 'metacognitive_vector', 'metacognitive_feedback'],\n      dtype='object')"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"df.rename(\n    columns={\n        'Question 1': 'Problem',\n        'Response 1': 'Student_code',\n        'Right answer 1': 'Expected_code'\n    },\n    inplace=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:32.308342Z","iopub.execute_input":"2025-01-10T08:41:32.308595Z","iopub.status.idle":"2025-01-10T08:41:32.328380Z","shell.execute_reply.started":"2025-01-10T08:41:32.308574Z","shell.execute_reply":"2025-01-10T08:41:32.327578Z"},"id":"ugJYy8pVa7Ir"},"outputs":[],"execution_count":14},{"cell_type":"code","source":"df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:32.524863Z","iopub.execute_input":"2025-01-10T08:41:32.525142Z","iopub.status.idle":"2025-01-10T08:41:32.543890Z","shell.execute_reply.started":"2025-01-10T08:41:32.525121Z","shell.execute_reply":"2025-01-10T08:41:32.543234Z"},"id":"mcjarOz3a7Is"},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                             Problem  \\\n0  Develop a Python program that takes the name o...   \n1  Develop a Python program that takes the name o...   \n2  Develop a Python program that takes the name o...   \n\n                                        Student_code  \\\n0  file_input = input()      file_open = open(fil...   \n1  file_input = input()      file_open = open(fil...   \n2  file_input = input()      file_open = open(fil...   \n\n                                       Expected_code        Q01  \\\n0  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n1  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n2  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n\n             Q02               Q03        Q04            Q05            Q06  \\\n0  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n1  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n2  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n\n                Q07  ...        Q09        Q10            Q11            Q12  \\\n0  1 : Almost Never  ...  3 : Often  3 : Often  2 : Sometimes  2 : Sometimes   \n1  1 : Almost Never  ...  3 : Often  3 : Often  2 : Sometimes  2 : Sometimes   \n2  1 : Almost Never  ...  3 : Often  3 : Often  2 : Sometimes  2 : Sometimes   \n\n             Q13        Q14        Q15               Q16  \\\n0  2 : Sometimes  3 : Often  3 : Often  1 : Almost Never   \n1  2 : Sometimes  3 : Often  3 : Often  1 : Almost Never   \n2  2 : Sometimes  3 : Often  3 : Often  1 : Almost Never   \n\n                                metacognitive_vector  \\\n0  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n1  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n2  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n\n                              metacognitive_feedback  \n0  Your initial code serves as a starting point, ...  \n1  Your code exhibits a solid attempt at reading ...  \n2  It looks like you're in a good place with some...  \n\n[3 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Problem</th>\n      <th>Student_code</th>\n      <th>Expected_code</th>\n      <th>Q01</th>\n      <th>Q02</th>\n      <th>Q03</th>\n      <th>Q04</th>\n      <th>Q05</th>\n      <th>Q06</th>\n      <th>Q07</th>\n      <th>...</th>\n      <th>Q09</th>\n      <th>Q10</th>\n      <th>Q11</th>\n      <th>Q12</th>\n      <th>Q13</th>\n      <th>Q14</th>\n      <th>Q15</th>\n      <th>Q16</th>\n      <th>metacognitive_vector</th>\n      <th>metacognitive_feedback</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>Your initial code serves as a starting point, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>Your code exhibits a solid attempt at reading ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>It looks like you're in a good place with some...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 21 columns</p>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"student_prompt = \"This is the combination of the problem desccription and the student provided code.\"\ncontext_prompt = \"This is the combination of the problem description and the expected correct answer for that algorithm design question.\"\nmetacognitive_components = [\n                \"Read\", \"Identify\", \"Rephrase\", \"Examples\", \"Breakdown\", \"Estimate\", \"Plan\",\n                \"Revise\", \"Verify\", \"AvoidMistakes\", \"MonitorSteps\", \"MonitorProcess\",\n                \"ValidateConstraints\", \"Confirm\", \"CheckRequirements\", \"Reflect\"\n            ]\n\nmetacognition_prompt = (\n            f\"Metacognitive feedback helps students reflect on their algorithm-solving strategies. \"\n            f\"The passed metacognitive vector is a 16-dimensional vector describing the student's metacognition profile:  \"\n            f\"where each dimension represents one of 16 metacognitive components, rated as 1 (Almost never), \"\n            f\"2 (Sometimes), or 3 (Often). These components include: {', '.join(metacognitive_components)}.\"\n        )\ndecoder_prompt = \"This is the combination student's metacognition profile and the student's code for the algorithm problem with the problem description itself.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:32.984746Z","iopub.execute_input":"2025-01-10T08:41:32.985023Z","iopub.status.idle":"2025-01-10T08:41:32.989191Z","shell.execute_reply.started":"2025-01-10T08:41:32.984998Z","shell.execute_reply":"2025-01-10T08:41:32.988351Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"df['combined_problem_student'] = student_prompt + \"  Problem:  \" + df['Problem'] + \".  Student provided code:  \" + df['Student_code']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:34.974543Z","iopub.execute_input":"2025-01-10T08:41:34.974823Z","iopub.status.idle":"2025-01-10T08:41:35.102822Z","shell.execute_reply.started":"2025-01-10T08:41:34.974798Z","shell.execute_reply":"2025-01-10T08:41:35.101905Z"},"id":"Z7eLV0EQa7Is"},"outputs":[],"execution_count":17},{"cell_type":"code","source":"df['combined_problem_expected'] = context_prompt + \" Problem:  \" + df['Problem'] + \".  Expected correct ansswer for the problem:  \" + df['Expected_code']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:35.369632Z","iopub.execute_input":"2025-01-10T08:41:35.369912Z","iopub.status.idle":"2025-01-10T08:41:35.464073Z","shell.execute_reply.started":"2025-01-10T08:41:35.369885Z","shell.execute_reply":"2025-01-10T08:41:35.463376Z"},"id":"7GpCPwv8a7Is"},"outputs":[],"execution_count":18},{"cell_type":"code","source":"df['combined_metacogntion_prompt'] =  metacognition_prompt + \"Here is the student's metacognition vector: \" + df['metacognitive_vector']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:35.750843Z","iopub.execute_input":"2025-01-10T08:41:35.751120Z","iopub.status.idle":"2025-01-10T08:41:35.759232Z","shell.execute_reply.started":"2025-01-10T08:41:35.751099Z","shell.execute_reply":"2025-01-10T08:41:35.758372Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# df['combined_persona_student'] = ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:36.114184Z","iopub.execute_input":"2025-01-10T08:41:36.114500Z","iopub.status.idle":"2025-01-10T08:41:36.117999Z","shell.execute_reply.started":"2025-01-10T08:41:36.114478Z","shell.execute_reply":"2025-01-10T08:41:36.117124Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"df['combined_metacogntion_prompt'][20]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:36.468812Z","iopub.execute_input":"2025-01-10T08:41:36.469117Z","iopub.status.idle":"2025-01-10T08:41:36.474068Z","shell.execute_reply.started":"2025-01-10T08:41:36.469091Z","shell.execute_reply":"2025-01-10T08:41:36.473299Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"\"Metacognitive feedback helps students reflect on their algorithm-solving strategies. The passed metacognitive vector is a 16-dimensional vector describing the student's metacognition profile:  where each dimension represents one of 16 metacognitive components, rated as 1 (Almost never), 2 (Sometimes), or 3 (Often). These components include: Read, Identify, Rephrase, Examples, Breakdown, Estimate, Plan, Revise, Verify, AvoidMistakes, MonitorSteps, MonitorProcess, ValidateConstraints, Confirm, CheckRequirements, Reflect.Here is the student's metacognition vector: ['2 ', '3 ', '2 ', '3 ', '2 ', '2 ', '1 ', '2 ', '3 ', '3 ', '3 ', '3 ', '3 ', '3 ', '2 ', '2 ']\""},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:38.878023Z","iopub.execute_input":"2025-01-10T08:41:38.878359Z","iopub.status.idle":"2025-01-10T08:41:38.883218Z","shell.execute_reply.started":"2025-01-10T08:41:38.878327Z","shell.execute_reply":"2025-01-10T08:41:38.882510Z"},"id":"-EMODMJCa7It"},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"Index(['Problem', 'Student_code', 'Expected_code', 'Q01', 'Q02', 'Q03', 'Q04',\n       'Q05', 'Q06', 'Q07', 'Q08', 'Q09', 'Q10', 'Q11', 'Q12', 'Q13', 'Q14',\n       'Q15', 'Q16', 'metacognitive_vector', 'metacognitive_feedback',\n       'combined_problem_student', 'combined_problem_expected',\n       'combined_metacogntion_prompt'],\n      dtype='object')"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"df.dropna(subset=['Problem', 'metacognitive_feedback', 'combined_problem_student','Student_code'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:39.203634Z","iopub.execute_input":"2025-01-10T08:41:39.203893Z","iopub.status.idle":"2025-01-10T08:41:39.227639Z","shell.execute_reply.started":"2025-01-10T08:41:39.203873Z","shell.execute_reply":"2025-01-10T08:41:39.226743Z"},"id":"94NeCGFua7It"},"outputs":[],"execution_count":23},{"cell_type":"code","source":"df.reset_index(drop=True, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:39.524990Z","iopub.execute_input":"2025-01-10T08:41:39.525319Z","iopub.status.idle":"2025-01-10T08:41:39.528796Z","shell.execute_reply.started":"2025-01-10T08:41:39.525260Z","shell.execute_reply":"2025-01-10T08:41:39.528063Z"},"id":"9wKnq90Ua7Iu"},"outputs":[],"execution_count":24},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:41.604468Z","iopub.execute_input":"2025-01-10T08:41:41.604756Z","iopub.status.idle":"2025-01-10T08:41:41.633127Z","shell.execute_reply.started":"2025-01-10T08:41:41.604734Z","shell.execute_reply":"2025-01-10T08:41:41.632297Z"},"id":"ZzXH3PICa7Iu"},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"Problem                         0\nStudent_code                    0\nExpected_code                   0\nQ01                             0\nQ02                             0\nQ03                             0\nQ04                             0\nQ05                             0\nQ06                             0\nQ07                             0\nQ08                             0\nQ09                             0\nQ10                             0\nQ11                             0\nQ12                             0\nQ13                             0\nQ14                             0\nQ15                             0\nQ16                             0\nmetacognitive_vector            0\nmetacognitive_feedback          0\ncombined_problem_student        0\ncombined_problem_expected       0\ncombined_metacogntion_prompt    0\ndtype: int64"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"df['metacognitive_feedback'][100]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:43.559187Z","iopub.execute_input":"2025-01-10T08:41:43.559537Z","iopub.status.idle":"2025-01-10T08:41:43.564871Z","shell.execute_reply.started":"2025-01-10T08:41:43.559510Z","shell.execute_reply":"2025-01-10T08:41:43.564073Z"},"id":"WtKweTkea7Iu"},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"\"Your current implementation shows a good effort in structuring your code with functions, but there are some areas where further refinement is necessary to meet the problem requirements. First, consider how you're capturing the relationships between birth years and heights. While you are correctly gathering names, birthdates, and heights into a dictionary, think about how you can aggregate heights by decade instead of year. This will streamline the calculation of average heights. It’s crucial to loop through your input list once to comprehend and process the data before beginning any calculations for averaging—possibly consolidating this logic in your `calculate_average_height` function. Also, ensure that you are converting heights to the correct data type before performing any arithmetic operations. Additionally, take a closer look at how you’re determining the range of decades; right now it seems like you may be focusing on unique years instead of decades. Consider creating a systematic structure that easily categorizes each height into its corresponding decade bucket. Furthermore, as you develop your code, remember to monitor your program's flow and adjust accordingly—this is especially important when it comes to function calls and data structure manipulations. Establishing a plan before implementation can significantly enhance your problem-solving process and avoid errors stemming from assumptions. Overall, maintain momentum and continue refining your approach by checking each component against the problem's requirements, thus ensuring coherence and completeness in your solution.\""},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:43.893784Z","iopub.execute_input":"2025-01-10T08:41:43.894085Z","iopub.status.idle":"2025-01-10T08:41:43.913757Z","shell.execute_reply.started":"2025-01-10T08:41:43.894058Z","shell.execute_reply":"2025-01-10T08:41:43.913083Z"},"id":"t_EBOaYQa7Iv"},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"                                             Problem  \\\n0  Develop a Python program that takes the name o...   \n1  Develop a Python program that takes the name o...   \n2  Develop a Python program that takes the name o...   \n3  Develop a Python program that takes the name o...   \n4  Develop a Python program that takes the name o...   \n\n                                        Student_code  \\\n0  file_input = input()      file_open = open(fil...   \n1  file_input = input()      file_open = open(fil...   \n2  file_input = input()      file_open = open(fil...   \n3  file_input = input()      file_open = open(fil...   \n4  file_input = input()      file_open = open(fil...   \n\n                                       Expected_code        Q01  \\\n0  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n1  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n2  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n3  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n4  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n\n             Q02               Q03        Q04            Q05            Q06  \\\n0  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n1  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n2  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n3  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n4  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n\n                Q07  ...            Q12            Q13        Q14        Q15  \\\n0  1 : Almost Never  ...  2 : Sometimes  2 : Sometimes  3 : Often  3 : Often   \n1  1 : Almost Never  ...  2 : Sometimes  2 : Sometimes  3 : Often  3 : Often   \n2  1 : Almost Never  ...  2 : Sometimes  2 : Sometimes  3 : Often  3 : Often   \n3  1 : Almost Never  ...  2 : Sometimes  2 : Sometimes  3 : Often  3 : Often   \n4  1 : Almost Never  ...  2 : Sometimes  2 : Sometimes  3 : Often  3 : Often   \n\n                Q16                               metacognitive_vector  \\\n0  1 : Almost Never  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n1  1 : Almost Never  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n2  1 : Almost Never  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n3  1 : Almost Never  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n4  1 : Almost Never  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n\n                              metacognitive_feedback  \\\n0  Your initial code serves as a starting point, ...   \n1  Your code exhibits a solid attempt at reading ...   \n2  It looks like you're in a good place with some...   \n3  Your approach to reading the file and splittin...   \n4  Your initial approach to the problem is a good...   \n\n                            combined_problem_student  \\\n0  This is the combination of the problem desccri...   \n1  This is the combination of the problem desccri...   \n2  This is the combination of the problem desccri...   \n3  This is the combination of the problem desccri...   \n4  This is the combination of the problem desccri...   \n\n                           combined_problem_expected  \\\n0  This is the combination of the problem descrip...   \n1  This is the combination of the problem descrip...   \n2  This is the combination of the problem descrip...   \n3  This is the combination of the problem descrip...   \n4  This is the combination of the problem descrip...   \n\n                        combined_metacogntion_prompt  \n0  Metacognitive feedback helps students reflect ...  \n1  Metacognitive feedback helps students reflect ...  \n2  Metacognitive feedback helps students reflect ...  \n3  Metacognitive feedback helps students reflect ...  \n4  Metacognitive feedback helps students reflect ...  \n\n[5 rows x 24 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Problem</th>\n      <th>Student_code</th>\n      <th>Expected_code</th>\n      <th>Q01</th>\n      <th>Q02</th>\n      <th>Q03</th>\n      <th>Q04</th>\n      <th>Q05</th>\n      <th>Q06</th>\n      <th>Q07</th>\n      <th>...</th>\n      <th>Q12</th>\n      <th>Q13</th>\n      <th>Q14</th>\n      <th>Q15</th>\n      <th>Q16</th>\n      <th>metacognitive_vector</th>\n      <th>metacognitive_feedback</th>\n      <th>combined_problem_student</th>\n      <th>combined_problem_expected</th>\n      <th>combined_metacogntion_prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>Your initial code serves as a starting point, ...</td>\n      <td>This is the combination of the problem desccri...</td>\n      <td>This is the combination of the problem descrip...</td>\n      <td>Metacognitive feedback helps students reflect ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>Your code exhibits a solid attempt at reading ...</td>\n      <td>This is the combination of the problem desccri...</td>\n      <td>This is the combination of the problem descrip...</td>\n      <td>Metacognitive feedback helps students reflect ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>It looks like you're in a good place with some...</td>\n      <td>This is the combination of the problem desccri...</td>\n      <td>This is the combination of the problem descrip...</td>\n      <td>Metacognitive feedback helps students reflect ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>Your approach to reading the file and splittin...</td>\n      <td>This is the combination of the problem desccri...</td>\n      <td>This is the combination of the problem descrip...</td>\n      <td>Metacognitive feedback helps students reflect ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>Your initial approach to the problem is a good...</td>\n      <td>This is the combination of the problem desccri...</td>\n      <td>This is the combination of the problem descrip...</td>\n      <td>Metacognitive feedback helps students reflect ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 24 columns</p>\n</div>"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport ast\nclass CustomDataset(Dataset):\n    def __init__(self, dataset, t5_tokenizer,gpt2_tokenizer, max_length=512):\n        self.t5_tokenizer = t5_tokenizer\n        self.gpt2_tokenizer = gpt2_tokenizer\n        self.data = dataset\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        metacognitive_prompt = self.data['combined_metacogntion_prompt'][idx]\n        problem_student_code = self.data['combined_problem_student'][idx]\n        problem_expected_code = self.data['combined_problem_expected'][idx]\n        student_code = self.data['Student_code'][idx]\n        target = self.data['metacognitive_feedback'][idx]\n\n        # metacognitive_vector_float = [\n        # float(item.strip()) for item in ast.literal_eval(metacognitive_vector)]\n        # metacognition_vector_ids = torch.tensor(metacognitive_vector_float, dtype=torch.float)\n        metacognition_prompt_ids = torch.tensor(\n            self.t5_tokenizer.encode(metacognitive_prompt, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n\n        problem_student_code_ids = torch.tensor(\n            self.t5_tokenizer.encode(problem_student_code, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n        problem_expected_code_ids = torch.tensor(\n            self.t5_tokenizer.encode(problem_expected_code, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n\n        student_code_ids = torch.tensor(\n            self.t5_tokenizer.encode(student_code, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n        target_ids = torch.tensor(\n            self.t5_tokenizer.encode(target, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n\n        return metacognition_prompt_ids, problem_student_code_ids, problem_expected_code_ids, student_code_ids, target_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:45.509256Z","iopub.execute_input":"2025-01-10T08:41:45.509599Z","iopub.status.idle":"2025-01-10T08:41:45.516411Z","shell.execute_reply.started":"2025-01-10T08:41:45.509574Z","shell.execute_reply":"2025-01-10T08:41:45.515643Z"},"id":"nNEuM_O3a7Iv"},"outputs":[],"execution_count":28},{"cell_type":"code","source":"dataset = CustomDataset(df, t5_tokenizer, gpt2_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:47.343624Z","iopub.execute_input":"2025-01-10T08:41:47.343928Z","iopub.status.idle":"2025-01-10T08:41:47.347988Z","shell.execute_reply.started":"2025-01-10T08:41:47.343900Z","shell.execute_reply":"2025-01-10T08:41:47.347087Z"},"id":"Xjf6vXzJa7Iv"},"outputs":[],"execution_count":29},{"cell_type":"code","source":"len(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:47.654027Z","iopub.execute_input":"2025-01-10T08:41:47.654333Z","iopub.status.idle":"2025-01-10T08:41:47.659039Z","shell.execute_reply.started":"2025-01-10T08:41:47.654269Z","shell.execute_reply":"2025-01-10T08:41:47.658178Z"},"id":"qmSPRdvNa7Iw"},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"16803"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"metacognition_prompt_ids, problem_student_code_ids, problem_expected_code_ids, student_code_ids, target_ids = dataset[4000]\nprint(f\"Metacognition vector IDs: {metacognition_prompt_ids}\")\nprint(f\"Expected feedback IDs: {problem_student_code_ids}\")\nprint(f\"Expected encoded feedback IDs: {problem_expected_code_ids}\")\nprint(f\"Student Answer IDs: {student_code_ids.shape}\")\nprint(f\"Target IDs: {target_ids.shape}\")\nprint(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:47.993602Z","iopub.execute_input":"2025-01-10T08:41:47.993820Z","iopub.status.idle":"2025-01-10T08:41:48.052453Z","shell.execute_reply.started":"2025-01-10T08:41:47.993801Z","shell.execute_reply":"2025-01-10T08:41:48.051712Z"},"id":"n5JGeIpza7Ix","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Metacognition vector IDs: tensor([14204,    75, 12905,  3268,  3160,  1691,   481,  3548,    30,    70,\n        12628,    18,  6065,    53,  3266,     5,    37,  2804, 10531,    75,\n        12905,  3268, 12938,    19,     3,     9,   898,    18, 11619, 12938,\n            3, 16012,     8,  1236,    31,     7, 10531,    75, 12905,  1575,\n         3278,    10,   213,   284,  9340,  5475,    80,    13,   898, 10531,\n           75, 12905,  3268,  3379,     6,     3,  4094,    38,   209,    41,\n        13283,   470,   201,   204,    41, 19055,   715,     7,   201,    42,\n          220,    41, 10084,   137,   506,  3379,   560,    10,  3403,     6,\n            3, 23393,     6,   419, 27111,     6, 19119,     6, 11429,  3035,\n            6, 23621,    15,     6,  2926,     6,  6342,   159,    15,     6,\n          781,  4921,     6, 15856,   329,   159,  4914,     7,     6, 15192,\n        14337,   102,     7,     6, 15192,  3174,  2319,     7,     6, 23545,\n          342,  4302,     7,  9719,    17,     7,     6,  1193,  7001,     6,\n         1972,  1649,  1169,    60,  4128,     6, 23966,     5, 12636,    15,\n           19,     8,  1236,    31,     7, 10531,    75, 12905,  1575, 12938,\n           10,   784,    31,   519,     3,    31,     6,     3,    31,   519,\n            3,    31,     6,     3,    31,   357,     3,    31,     6,     3,\n           31,   519,     3,    31,     6,     3,    31,   519,     3,    31,\n            6,     3,    31,   519,     3,    31,     6,     3,    31,   536,\n            3,    31,     6,     3,    31,   536,     3,    31,     6,     3,\n           31,   519,     3,    31,     6,     3,    31,   357,     3,    31,\n            6,     3,    31,   519,     3,    31,     6,     3,    31,   519,\n            3,    31,     6,     3,    31,   519,     3,    31,     6,     3,\n           31,   519,     3,    31,     6,     3,    31,   519,     3,    31,\n            6,     3,    31,   519,     3,    31,   908,     1,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0])\nExpected feedback IDs: tensor([  100,    19,     8,  2711,    13,     8,   682,    93,    75, 29771,\n           23,   106,    11,     8,  1236,   937,  1081,     5,  5289,    10,\n        24305,     3,     9, 20737,   478,    24,  1217,  1236,  3056,    11,\n           70,  7586,    16,  1317,  7404,    38,  3785,    45,     3,     9,\n         1042,    11,  3911,     7,   284,  1236,    31,     7,  1348,  2604,\n           12,     8,  8990,    41, 10475,  5595,   137,  5433,     6,     8,\n          478,   398,  3911,     8,   564,   599,     7,    61,    13,     8,\n         1236,   599,     7,    61,    28,     8,  2030,    11,  7402,  1348,\n         7586,     5,   696,   478,   398,  6634,    11,   169,    44,   709,\n           80,  1681,     5,    27,  9082,  6675,  5652, 18169,    10,  1429,\n           71,  1499,  1042,   213,   284,   689,  2579,     3,     9,  1236,\n           31,     7,   564,  2348,    57,    70,  7586,    16,  1317,  7404,\n            6,    66, 12494,    57,  4856,     5,    37,   381,    13,  7404,\n           54,  5215,   344,   481,     6,    68,   284,  1236,    56,    43,\n           44,   709,   220,  7404,     5,    37,   689,  1910,    19,    10,\n         6341, 23954, 17763,   536, 17763,   357,     3,   233, 17763,   567,\n            5,  1698,  2604,    19,    46, 30278,    16,     8,   620,     3,\n         9498,  2915,     5,   290,    56,    36,    44,   709,    80,  1236,\n           11,   112,    87,   760,  7586,    16,     8,  1042,     5,     3,\n         9744,   345,  6675,  5652, 18169,    10,  1429,    37,   478,   225,\n          166,  3911,     8,   564,    11,  1348,  2604,     3, 12279,    12,\n          192,  7908,  1982,  1747,    13,   334,  1236,    30,    80,   689,\n            6,    16,     8,   455,    79,  4283,    16,     8,  3785,     5,\n         1429,    37,   478,   225,   416,  3911,     8,   564,   599,     7,\n           61,    13,     8,  1236,   599,     7,    61,    28,     8,  2030,\n           11,  7402,  1348,  7586,     6,   590,    28,    70,  1348,  7586,\n            5,   156,   132,    19,   163,    80,  1236,     6,    24,  1236,\n          225,    36,  2008,    21,   321,     8,  2030,    11,  7402,  1348,\n            5,   156,   132,    33,  1317,   481,    28,     8,   337,  2030,\n           42,  7402,  1348,     6,   258,   570,    66,   224,   481,    16,\n            8,   455,    79,  4283,    16,     8,  3785,     5,   262,     4,\n        15837,  3765,   209,    10,    27,  9082,  6675,    41, 17752, 17779,\n         4578,  3347,   377, 20129,    61,    10, 13390, 11989,  2777,     3,\n         3940,  5762,  2861,  6374,  2775, 11989, 12707,     3,  4060, 11989,\n         2777,     3,  4271,     3,  4508,     3,  9744,   345,  6675,    10,\n        13390,    10,     3,  4608,     5,  4201,  5762,    10,     3,  4013,\n            5,  1752, 12707,    10,     3,  3914,     5,  3328,  1592,   222,\n        23836,    10, 12707,     6,     3,  3914,     5,  3328,  5586,   222,\n        23836,    10,  5762,     6,     3,  4013,     5,  1752,   262,     4,\n        15837,  3765,   204,    10,    27,  9082,  6675,    41, 17752, 17779,\n         4578,  3347,   377, 20129,    61,    10, 13390, 11989,  2777, 11923,\n         5762,  6374,  2775, 11989, 12707, 11923,  2777, 11989, 18821, 11923,\n         2777, 11989, 11566,  1640,  7123,  2861,     3,  9744,   345,  6675,\n           10, 13390,    10,   668, 10667,  5762,    10,   505, 10667, 12707,\n           10,   668, 10667, 18821,    10,   668, 10667, 11566,    10,   431,\n        23577,  1592,   222, 23836,    10, 13390,     6, 12707,     6, 18821,\n            6,   668, 10667,  5586,   222, 23836,    10, 11566,     6,   431,\n        23577,     5,  6341,   937,  1081,    10,  3785,   834, 11966,  2423,\n           77,  2562,  9960,     3,   226,    32,  2423,  8751,   599,    77,\n         2562,   834, 11966,     6,    31,    52,    31,    61,  1713,  8751,\n            8,  1042,   608,   834, 11966,  2423,   226,    32,     5,  5236,\n         9960,     1])\nExpected encoded feedback IDs: tensor([  100,    19,     8,  2711,    13,     8,   682,  4210,    11,     8,\n         1644,  2024,  1525,    21,    24, 12628,   408,   822,     5,  5289,\n           10, 24305,     3,     9, 20737,   478,    24,  1217,  1236,  3056,\n           11,    70,  7586,    16,  1317,  7404,    38,  3785,    45,     3,\n            9,  1042,    11,  3911,     7,   284,  1236,    31,     7,  1348,\n         2604,    12,     8,  8990,    41, 10475,  5595,   137,  5433,     6,\n            8,   478,   398,  3911,     8,   564,   599,     7,    61,    13,\n            8,  1236,   599,     7,    61,    28,     8,  2030,    11,  7402,\n         1348,  7586,     5,   696,   478,   398,  6634,    11,   169,    44,\n          709,    80,  1681,     5,    27,  9082,  6675,  5652, 18169,    10,\n         1429,    71,  1499,  1042,   213,   284,   689,  2579,     3,     9,\n         1236,    31,     7,   564,  2348,    57,    70,  7586,    16,  1317,\n         7404,     6,    66, 12494,    57,  4856,     5,    37,   381,    13,\n         7404,    54,  5215,   344,   481,     6,    68,   284,  1236,    56,\n           43,    44,   709,   220,  7404,     5,    37,   689,  1910,    19,\n           10,  6341, 23954, 17763,   536, 17763,   357,     3,   233, 17763,\n          567,     5,  1698,  2604,    19,    46, 30278,    16,     8,   620,\n            3,  9498,  2915,     5,   290,    56,    36,    44,   709,    80,\n         1236,    11,   112,    87,   760,  7586,    16,     8,  1042,     5,\n            3,  9744,   345,  6675,  5652, 18169,    10,  1429,    37,   478,\n          225,   166,  3911,     8,   564,    11,  1348,  2604,     3, 12279,\n           12,   192,  7908,  1982,  1747,    13,   334,  1236,    30,    80,\n          689,     6,    16,     8,   455,    79,  4283,    16,     8,  3785,\n            5,  1429,    37,   478,   225,   416,  3911,     8,   564,   599,\n            7,    61,    13,     8,  1236,   599,     7,    61,    28,     8,\n         2030,    11,  7402,  1348,  7586,     6,   590,    28,    70,  1348,\n         7586,     5,   156,   132,    19,   163,    80,  1236,     6,    24,\n         1236,   225,    36,  2008,    21,   321,     8,  2030,    11,  7402,\n         1348,     5,   156,   132,    33,  1317,   481,    28,     8,   337,\n         2030,    42,  7402,  1348,     6,   258,   570,    66,   224,   481,\n           16,     8,   455,    79,  4283,    16,     8,  3785,     5,   262,\n            4, 15837,  3765,   209,    10,    27,  9082,  6675,    41, 17752,\n        17779,  4578,  3347,   377, 20129,    61,    10, 13390, 11989,  2777,\n            3,  3940,  5762,  2861,  6374,  2775, 11989, 12707,     3,  4060,\n        11989,  2777,     3,  4271,     3,  4508,     3,  9744,   345,  6675,\n           10, 13390,    10,     3,  4608,     5,  4201,  5762,    10,     3,\n         4013,     5,  1752, 12707,    10,     3,  3914,     5,  3328,  1592,\n          222, 23836,    10, 12707,     6,     3,  3914,     5,  3328,  5586,\n          222, 23836,    10,  5762,     6,     3,  4013,     5,  1752,   262,\n            4, 15837,  3765,   204,    10,    27,  9082,  6675,    41, 17752,\n        17779,  4578,  3347,   377, 20129,    61,    10, 13390, 11989,  2777,\n        11923,  5762,  6374,  2775, 11989, 12707, 11923,  2777, 11989, 18821,\n        11923,  2777, 11989, 11566,  1640,  7123,  2861,     3,  9744,   345,\n         6675,    10, 13390,    10,   668, 10667,  5762,    10,   505, 10667,\n        12707,    10,   668, 10667, 18821,    10,   668, 10667, 11566,    10,\n          431, 23577,  1592,   222, 23836,    10, 13390,     6, 12707,     6,\n        18821,     6,   668, 10667,  5586,   222, 23836,    10, 11566,     6,\n          431, 23577,     5, 19539,    15,    26,  2024,    46,     7,     7,\n         3321,    21,     8,   682,    10,    20,    89, 11837,   834, 28951,\n          599,     7,  9022,     7,    61,    10,  1205,  4505,   599,     7,\n         9022,     7,    61,     3,    87,    90,    29,   599,     7,  9022,\n            7,     1])\nStudent Answer IDs: torch.Size([512])\nTarget IDs: torch.Size([512])\n\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"gpt2_tokenizer.pad_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:53.094770Z","iopub.execute_input":"2025-01-10T08:41:53.095046Z","iopub.status.idle":"2025-01-10T08:41:53.100209Z","shell.execute_reply.started":"2025-01-10T08:41:53.095024Z","shell.execute_reply":"2025-01-10T08:41:53.099494Z"},"id":"6Wlser8Ba7Iy"},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"'<|endoftext|>'"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"gpt2_pad_token_id = gpt2_tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:53.466804Z","iopub.execute_input":"2025-01-10T08:41:53.467132Z","iopub.status.idle":"2025-01-10T08:41:53.470971Z","shell.execute_reply.started":"2025-01-10T08:41:53.467103Z","shell.execute_reply":"2025-01-10T08:41:53.470078Z"},"id":"cnDxXCGxa7Iy"},"outputs":[],"execution_count":33},{"cell_type":"code","source":"gpt2_pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:55.194021Z","iopub.execute_input":"2025-01-10T08:41:55.194438Z","iopub.status.idle":"2025-01-10T08:41:55.200131Z","shell.execute_reply.started":"2025-01-10T08:41:55.194402Z","shell.execute_reply":"2025-01-10T08:41:55.199362Z"},"id":"5LAI9-sZa7Iy"},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"50256"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"t5_tokenizer.pad_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:55.513927Z","iopub.execute_input":"2025-01-10T08:41:55.514243Z","iopub.status.idle":"2025-01-10T08:41:55.519055Z","shell.execute_reply.started":"2025-01-10T08:41:55.514208Z","shell.execute_reply":"2025-01-10T08:41:55.518189Z"},"id":"hEyynFuNa7Iz"},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"'<pad>'"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"t5_pad_token_id = t5_tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:55.723608Z","iopub.execute_input":"2025-01-10T08:41:55.723931Z","iopub.status.idle":"2025-01-10T08:41:55.727867Z","shell.execute_reply.started":"2025-01-10T08:41:55.723902Z","shell.execute_reply":"2025-01-10T08:41:55.726868Z"},"id":"sGPK7f0ba7Iz"},"outputs":[],"execution_count":36},{"cell_type":"code","source":"t5_pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:41:55.908651Z","iopub.execute_input":"2025-01-10T08:41:55.908949Z","iopub.status.idle":"2025-01-10T08:41:55.914619Z","shell.execute_reply.started":"2025-01-10T08:41:55.908927Z","shell.execute_reply":"2025-01-10T08:41:55.913572Z"},"id":"nWn7op96a7I0"},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"# Context Enocder","metadata":{"id":"HK7ybN62a7I0"}},{"cell_type":"code","source":"# class ContextEncoder(nn.Module):\n#     def __init__(self, t5_model_name='t5-base', output_dim=768 , max_length = 512):\n#         super(ContextEncoder, self).__init__()\n\n#         self.max_length = max_length\n\n#         self.t5_encoder = T5Model.from_pretrained(t5_model_name).encoder\n#         self.t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n#         self.fc = nn.Linear(self.t5_encoder.config.d_model, output_dim)\n\n#     def forward(self, problem_code_ids, attention_masks=None, prompt=\"\"):\n\n#         prompt_ids = torch.tensor(\n#             self.t5_tokenizer.encode(prompt, max_length=self.max_length, truncation=True, padding=\"max_length\")\n#         )\n\n#         problem_code = [self.t5_tokenizer.decode(ids, skip_special_tokens=True) for ids in problem_code_ids]\n\n#         combined_text = prompt + \" \" + \" \".join(problem_code)\n\n#         encoded = self.t5_tokenizer(\n#             combined_text,\n#             max_length=self.max_length, \n#             truncation=True,\n#             padding=\"max_length\",\n#             return_tensors=\"pt\"\n#         ).to(problem_code_ids.device)\n\n#         encoder_outputs = self.t5_encoder(\n#             input_ids=encoded[\"input_ids\"],\n#             attention_mask=encoded[\"attention_mask\"]\n#         )\n\n#         context_hidden_states = encoder_outputs.last_hidden_state\n\n#         context_rep = context_hidden_states.mean(dim=1)\n\n#         # decoded_combined = [\n#         # self.t5_tokenizer.decode(ids, skip_special_tokens=True)\n#         # for ids in encoded_outputs]\n\n#         context_rep = self.fc(context_rep)\n#         final_rep = context_rep.unsqueeze(1)\n\n#         return final_rep","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T07:42:09.352192Z","iopub.execute_input":"2025-01-10T07:42:09.352480Z","iopub.status.idle":"2025-01-10T07:42:09.356085Z","shell.execute_reply.started":"2025-01-10T07:42:09.352458Z","shell.execute_reply":"2025-01-10T07:42:09.355317Z"},"id":"epmjfCO_a7I2"},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# context_encoder = ContextEncoder().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T07:42:10.516269Z","iopub.execute_input":"2025-01-10T07:42:10.516554Z","iopub.status.idle":"2025-01-10T07:42:10.519945Z","shell.execute_reply.started":"2025-01-10T07:42:10.516531Z","shell.execute_reply":"2025-01-10T07:42:10.519153Z"},"id":"Fnjz8vrra7I2"},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# context_encoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T07:42:10.833751Z","iopub.execute_input":"2025-01-10T07:42:10.834074Z","iopub.status.idle":"2025-01-10T07:42:10.837366Z","shell.execute_reply.started":"2025-01-10T07:42:10.834049Z","shell.execute_reply":"2025-01-10T07:42:10.836531Z"},"id":"GCZmPBnSa7I2"},"outputs":[],"execution_count":40},{"cell_type":"code","source":"\n# class MetacognitionLayer(nn.Module):\n#     def __init__(self, metacognitive_dim=16, output_dim=768):\n#         super(MetacognitionLayer, self).__init__()\n#         #16 to 768 mapping\n#         self.metacognitive_fc = nn.Linear(metacognitive_dim, output_dim)\n#         self.final_fc = nn.Linear(output_dim, output_dim)\n\n#         self.tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n#         self.t5_pad_token_id = self.tokenizer.pad_token_id\n#         self.encoder = T5Model.from_pretrained(\"t5-base\").encoder\n\n\n\n#         for param in self.encoder.parameters():\n#             param.requires_grad = False\n\n#     def forward(self, metacognitive_vector):\n\n#         metacognitive_components = [\n#                 \"Read\", \"Identify\", \"Rephrase\", \"Examples\", \"Breakdown\", \"Estimate\", \"Plan\",\n#                 \"Revise\", \"Verify\", \"AvoidMistakes\", \"MonitorSteps\", \"MonitorProcess\",\n#                 \"ValidateConstraints\", \"Confirm\", \"CheckRequirements\", \"Reflect\"\n#             ]\n\n#         prompt_text = (\n#             f\"Metacognitive feedback helps students reflect on their algorithm-solving strategies. \"\n#             f\"The passed metacognitive vector is a 16-dimensional vector describing the student's metacognition profile, \"\n#             f\"where each dimension represents one of 16 metacognitive components, rated as 1 (Almost never), \"\n#             f\"2 (Sometimes), or 3 (Often). These components include: {', '.join(metacognitive_components)}.\"\n#         )\n\n\n#         input_prompt = self.tokenizer(\n#         prompt_text,\n#         return_tensors=\"pt\",\n#         padding=True,\n#         truncation=True,\n#         max_length=512-16\n#         )\n#         input_prompt = {key: value.to(metacognitive_vector.device) for key, value in input_prompt.items()}\n        \n#         outputs = self.encoder(input_ids=input_prompt[\"input_ids\"], attention_mask=input_prompt[\"attention_mask\"])\n#         prompt_embedding = outputs.last_hidden_state.mean(dim=1)\n#         prompt_embedding = prompt_embedding.to(metacognitive_vector.device)\n\n#         metacognitive_rep = self.metacognitive_fc(metacognitive_vector)\n#         final_rep = self.final_fc(metacognitive_rep)\n#         persona_rep = final_rep.unsqueeze(1)\n       \n#         final_persona = persona_rep + prompt_embedding\n\n#         return final_persona","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T07:42:11.101146Z","iopub.execute_input":"2025-01-10T07:42:11.101428Z","iopub.status.idle":"2025-01-10T07:42:11.105145Z","shell.execute_reply.started":"2025-01-10T07:42:11.101402Z","shell.execute_reply":"2025-01-10T07:42:11.104305Z"},"id":"ZX97CJgLa7I2"},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# metacognitive_emb = MetacognitionLayer().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T07:42:12.661334Z","iopub.execute_input":"2025-01-10T07:42:12.661620Z","iopub.status.idle":"2025-01-10T07:42:12.664928Z","shell.execute_reply.started":"2025-01-10T07:42:12.661597Z","shell.execute_reply":"2025-01-10T07:42:12.664080Z"},"id":"x5pQ-96_a7JA"},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# metacognitive_emb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T07:42:12.982547Z","iopub.execute_input":"2025-01-10T07:42:12.982828Z","iopub.status.idle":"2025-01-10T07:42:12.986083Z","shell.execute_reply.started":"2025-01-10T07:42:12.982805Z","shell.execute_reply":"2025-01-10T07:42:12.985264Z"},"id":"BQMfinGKa7JA"},"outputs":[],"execution_count":43},{"cell_type":"code","source":"\nclass PAALayer(nn.Module):\n    def __init__(self, hidden_dimension = 512 , tau=0.8,dropout_rate=0.1):\n        super(PAALayer, self).__init__()\n        self.hidden_dimenstion = hidden_dimension\n        self.tau = tau\n\n\n        self.fc = nn.Linear(2 * hidden_dimension, hidden_dimension)\n        self.sigmoid = nn.Sigmoid()\n        self.fc_out = nn.Linear(hidden_dimension, hidden_dimension)\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n\n    def forward(self, hR , oP, oC):\n\n        Mp_input  = torch.cat([hR,oP], dim=-1)\n        Mp = self.fc(Mp_input)\n        Wp = self.sigmoid(Mp)\n\n        Mpersona = Wp\n        Mcontext = 1 - Wp\n\n        oP_weighted = Mcontext * oP\n        oC_weighted = Mpersona * oC\n\n        HPAA = oP_weighted + oC_weighted\n\n        output = self.fc_out(HPAA)\n\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:00.435152Z","iopub.execute_input":"2025-01-10T08:42:00.435503Z","iopub.status.idle":"2025-01-10T08:42:00.440912Z","shell.execute_reply.started":"2025-01-10T08:42:00.435479Z","shell.execute_reply":"2025-01-10T08:42:00.440023Z"},"id":"2Z_xRbhNa7JA"},"outputs":[],"execution_count":38},{"cell_type":"code","source":"paa = PAALayer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:02.314910Z","iopub.execute_input":"2025-01-10T08:42:02.315226Z","iopub.status.idle":"2025-01-10T08:42:02.332172Z","shell.execute_reply.started":"2025-01-10T08:42:02.315199Z","shell.execute_reply":"2025-01-10T08:42:02.331378Z"},"id":"E6kRw4dea7JB"},"outputs":[],"execution_count":39},{"cell_type":"code","source":"paa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:02.543186Z","iopub.execute_input":"2025-01-10T08:42:02.543531Z","iopub.status.idle":"2025-01-10T08:42:02.548270Z","shell.execute_reply.started":"2025-01-10T08:42:02.543506Z","shell.execute_reply":"2025-01-10T08:42:02.547595Z"},"id":"T3pInXDya7JB","outputId":"66ad94da-6b56-4525-e2b2-459d4aa68c9d"},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"PAALayer(\n  (fc): Linear(in_features=1024, out_features=512, bias=True)\n  (sigmoid): Sigmoid()\n  (fc_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"class CustomTransformerBlock(nn.Module):\n    def __init__(self, hidden_size, tau , dropout_rate=0.1):\n        super(CustomTransformerBlock, self).__init__()\n\n        self.input_self_attention = nn.MultiheadAttention(hidden_size, num_heads=8, batch_first=True)\n        self.context_attn = nn.MultiheadAttention(hidden_size, num_heads=8, batch_first=True)\n        self.persona_attn = nn.MultiheadAttention(hidden_size, num_heads=8, batch_first=True)\n        self.persona_proj = nn.Linear(768, hidden_size)  \n        self.context_proj = nn.Linear(768, hidden_size)\n\n        self.paa_layer = PAALayer(hidden_dimension=hidden_size, tau=tau)\n        \n\n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_size, 2048),\n            nn.ReLU(),\n            nn.Dropout(p=0.1),\n            nn.Linear(2048, hidden_size),\n            nn.LayerNorm(hidden_size)\n        )\n        self.layer_norm2 = nn.LayerNorm(hidden_size)\n\n    def forward(self, student_initial_state, encoded_persona, encoded_context):        \n\n        hR, _ = self.input_self_attention(student_initial_state, student_initial_state, student_initial_state) #query\n        encoded_persona = self.persona_proj(encoded_persona)\n        encoded_context = self.context_proj(encoded_context)\n        # print(\"hr shape\",hR.shape)\n        # print(\"encoded persona shape\", encoded_persona.shape)\n        # print(\"encoded context shape\", encoded_context.shape)\n        oP, _ = self.persona_attn(hR, encoded_persona, encoded_persona )\n       \n        # encoded_context = encoded_context.repeat(hR.size(0), hR.size(1), 1)\n      \n        oC, _ = self.context_attn(hR, encoded_context, encoded_context )\n\n        HPAA = self.paa_layer(hR, oP, oC)\n\n        mlp_output = self.mlp(HPAA)\n        output = self.layer_norm2(mlp_output)\n\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:27.249628Z","iopub.execute_input":"2025-01-10T08:42:27.249903Z","iopub.status.idle":"2025-01-10T08:42:27.256391Z","shell.execute_reply.started":"2025-01-10T08:42:27.249880Z","shell.execute_reply":"2025-01-10T08:42:27.255685Z"},"id":"wDhvR6Goa7JB"},"outputs":[],"execution_count":46},{"cell_type":"code","source":"custom_layer = CustomTransformerBlock(768,0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:27.562936Z","iopub.execute_input":"2025-01-10T08:42:27.563220Z","iopub.status.idle":"2025-01-10T08:42:27.680843Z","shell.execute_reply.started":"2025-01-10T08:42:27.563194Z","shell.execute_reply":"2025-01-10T08:42:27.680169Z"},"id":"v_D6juUna7JC"},"outputs":[],"execution_count":47},{"cell_type":"code","source":"custom_layer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:29.546940Z","iopub.execute_input":"2025-01-10T08:42:29.547237Z","iopub.status.idle":"2025-01-10T08:42:29.552427Z","shell.execute_reply.started":"2025-01-10T08:42:29.547215Z","shell.execute_reply":"2025-01-10T08:42:29.551456Z"},"id":"nwq9pZy6a7JC","outputId":"6fbb8e91-749a-4ef8-a24e-c2313f8dea45"},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"CustomTransformerBlock(\n  (input_self_attention): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (context_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (persona_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (persona_proj): Linear(in_features=768, out_features=768, bias=True)\n  (context_proj): Linear(in_features=768, out_features=768, bias=True)\n  (paa_layer): PAALayer(\n    (fc): Linear(in_features=1536, out_features=768, bias=True)\n    (sigmoid): Sigmoid()\n    (fc_out): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (mlp): Sequential(\n    (0): Linear(in_features=768, out_features=2048, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.1, inplace=False)\n    (3): Linear(in_features=2048, out_features=768, bias=True)\n    (4): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n)"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"class PAAModel(nn.Module):\n    def __init__(self, hidden_size=512, vocab_size = 32100 ,tau=0.5, max_length=512, num_transformer_blocks=4):\n        super(PAAModel , self).__init__()\n        self.hidden_size = hidden_size\n        self.tau = tau\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n        self.num_transformer_blocks = num_transformer_blocks\n        \n        self.t5_encoder = T5Model.from_pretrained(\"t5-base\").encoder\n        for param in self.t5_encoder.parameters():\n            param.requires_grad = False\n\n\n        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n        self.position_embedding = nn.Embedding(max_length, hidden_size)\n        self.dropout = nn.Dropout(p=0.1)        \n\n        self.transformer_blocks = nn.ModuleList([CustomTransformerBlock(hidden_size, tau) for _ in range(num_transformer_blocks)])\n        self.final_fc = nn.Linear(hidden_size, vocab_size)\n        self.layer_norm = nn.LayerNorm(hidden_size)\n\n    def forward(self,  metacognition_prompt_ids,\n                       problem_student_code_ids ,\n                       problem_expected_code_ids,\n                       metacognition_attention_mask,\n                       expected_attention_mask):\n\n        with torch.no_grad():\n\n            metacognition_prompt_encoded = self.t5_encoder(\n                                                        input_ids=metacognition_prompt_ids,\n                                                        attention_mask=metacognition_attention_mask).last_hidden_state\n    \n            problem_expected_code_encoded = self.t5_encoder(\n                                                        input_ids=problem_expected_code_ids,\n                                                        attention_mask=expected_attention_mask).last_hidden_state    \n\n\n        token_embeds = self.token_embedding(problem_student_code_ids)\n        position_ids = torch.arange(0, 1, device=problem_student_code_ids.device).unsqueeze(0)\n        position_ids = position_ids % self.max_length\n        position_embeds = self.position_embedding(position_ids)\n\n        inputs_embeds = token_embeds + position_embeds\n        inputs_embeds = self.dropout(inputs_embeds)\n        \n        student_initial_state = inputs_embeds\n        transformer_output = student_initial_state\n        \n        for transformer_block in self.transformer_blocks:\n            transformer_output = transformer_block(transformer_output, metacognition_prompt_encoded, problem_expected_code_encoded)\n\n        logits = self.final_fc(transformer_output)\n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:29.824501Z","iopub.execute_input":"2025-01-10T08:42:29.824782Z","iopub.status.idle":"2025-01-10T08:42:29.832217Z","shell.execute_reply.started":"2025-01-10T08:42:29.824761Z","shell.execute_reply":"2025-01-10T08:42:29.831265Z"},"id":"YSUMxJOYa7JC"},"outputs":[],"execution_count":49},{"cell_type":"code","source":"model = PAAModel()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:32.133562Z","iopub.execute_input":"2025-01-10T08:42:32.133843Z","iopub.status.idle":"2025-01-10T08:42:37.091607Z","shell.execute_reply.started":"2025-01-10T08:42:32.133822Z","shell.execute_reply":"2025-01-10T08:42:37.090665Z"},"id":"AROFcsBVa7JD"},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13855595319c4a60a5603beae91d6ec9"}},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:37.092739Z","iopub.execute_input":"2025-01-10T08:42:37.092972Z","iopub.status.idle":"2025-01-10T08:42:37.789388Z","shell.execute_reply.started":"2025-01-10T08:42:37.092951Z","shell.execute_reply":"2025-01-10T08:42:37.788633Z"},"id":"ukrbmnT1a7JD","outputId":"6e88007a-2a1a-4a99-b9e0-a11441cfa0f1"},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"PAAModel(\n  (t5_encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (token_embedding): Embedding(32100, 512)\n  (position_embedding): Embedding(512, 512)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (transformer_blocks): ModuleList(\n    (0-3): 4 x CustomTransformerBlock(\n      (input_self_attention): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n      )\n      (context_attn): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n      )\n      (persona_attn): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n      )\n      (persona_proj): Linear(in_features=768, out_features=512, bias=True)\n      (context_proj): Linear(in_features=768, out_features=512, bias=True)\n      (paa_layer): PAALayer(\n        (fc): Linear(in_features=1024, out_features=512, bias=True)\n        (sigmoid): Sigmoid()\n        (fc_out): Linear(in_features=512, out_features=512, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (mlp): Sequential(\n        (0): Linear(in_features=512, out_features=2048, bias=True)\n        (1): ReLU()\n        (2): Dropout(p=0.1, inplace=False)\n        (3): Linear(in_features=2048, out_features=512, bias=True)\n        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (final_fc): Linear(in_features=512, out_features=32100, bias=True)\n  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n)"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:38.329005Z","iopub.execute_input":"2025-01-10T08:42:38.329332Z","iopub.status.idle":"2025-01-10T08:42:38.333070Z","shell.execute_reply.started":"2025-01-10T08:42:38.329306Z","shell.execute_reply":"2025-01-10T08:42:38.332049Z"},"id":"w-YM7dxRa7JD"},"outputs":[],"execution_count":52},{"cell_type":"code","source":"import torch.optim as optim\noptimizer = optim.AdamW(model.parameters(), lr=5e-5)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:38.934044Z","iopub.execute_input":"2025-01-10T08:42:38.934396Z","iopub.status.idle":"2025-01-10T08:42:39.519943Z","shell.execute_reply.started":"2025-01-10T08:42:38.934367Z","shell.execute_reply":"2025-01-10T08:42:39.519233Z"},"id":"sCrMz7qra7JE"},"outputs":[],"execution_count":53},{"cell_type":"code","source":"LOSS = torch.nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:42.862634Z","iopub.execute_input":"2025-01-10T08:42:42.863150Z","iopub.status.idle":"2025-01-10T08:42:42.867001Z","shell.execute_reply.started":"2025-01-10T08:42:42.863125Z","shell.execute_reply":"2025-01-10T08:42:42.865993Z"},"id":"kNNiPR_ja7JE"},"outputs":[],"execution_count":54},{"cell_type":"code","source":"num_epochs = 200","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:43.112687Z","iopub.execute_input":"2025-01-10T08:42:43.112967Z","iopub.status.idle":"2025-01-10T08:42:43.116528Z","shell.execute_reply.started":"2025-01-10T08:42:43.112946Z","shell.execute_reply":"2025-01-10T08:42:43.115628Z"},"id":"gMr9Wd8_a7JE"},"outputs":[],"execution_count":55},{"cell_type":"code","source":"df_train = df[0:5000]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:43.382611Z","iopub.execute_input":"2025-01-10T08:42:43.383017Z","iopub.status.idle":"2025-01-10T08:42:43.386992Z","shell.execute_reply.started":"2025-01-10T08:42:43.382985Z","shell.execute_reply":"2025-01-10T08:42:43.386094Z"},"id":"U5UW8SVga7JE"},"outputs":[],"execution_count":56},{"cell_type":"code","source":"len(df_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:43.668141Z","iopub.execute_input":"2025-01-10T08:42:43.668501Z","iopub.status.idle":"2025-01-10T08:42:43.673696Z","shell.execute_reply.started":"2025-01-10T08:42:43.668470Z","shell.execute_reply":"2025-01-10T08:42:43.672740Z"},"id":"FADAr7rsa7JF","outputId":"2bc20724-bbfe-4263-c10d-0b4469249df3"},"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"5000"},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:43.968489Z","iopub.execute_input":"2025-01-10T08:42:43.968773Z","iopub.status.idle":"2025-01-10T08:42:43.983176Z","shell.execute_reply.started":"2025-01-10T08:42:43.968751Z","shell.execute_reply":"2025-01-10T08:42:43.982245Z"},"id":"V_g5jqJQa7JF","outputId":"d4a9adb3-2204-412b-ea78-d71c8bfcb8fe"},"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"Problem                         0\nStudent_code                    0\nExpected_code                   0\nQ01                             0\nQ02                             0\nQ03                             0\nQ04                             0\nQ05                             0\nQ06                             0\nQ07                             0\nQ08                             0\nQ09                             0\nQ10                             0\nQ11                             0\nQ12                             0\nQ13                             0\nQ14                             0\nQ15                             0\nQ16                             0\nmetacognitive_vector            0\nmetacognitive_feedback          0\ncombined_problem_student        0\ncombined_problem_expected       0\ncombined_metacogntion_prompt    0\ndtype: int64"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ntrain_dataset = CustomDataset(df_train, t5_tokenizer , gpt2_tokenizer)\ntrain_dataloader = DataLoader(train_dataset , batch_size = 4 ,shuffle = True )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:46.952228Z","iopub.execute_input":"2025-01-10T08:42:46.952593Z","iopub.status.idle":"2025-01-10T08:42:46.956875Z","shell.execute_reply.started":"2025-01-10T08:42:46.952567Z","shell.execute_reply":"2025-01-10T08:42:46.956024Z"},"id":"GqHiiAc1a7JF"},"outputs":[],"execution_count":59},{"cell_type":"code","source":"# from torch.utils.tensorboard import SummaryWriter\n# writer = SummaryWriter()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:47.303071Z","iopub.execute_input":"2025-01-10T08:42:47.303391Z","iopub.status.idle":"2025-01-10T08:42:47.306743Z","shell.execute_reply.started":"2025-01-10T08:42:47.303366Z","shell.execute_reply":"2025-01-10T08:42:47.305793Z"},"id":"pvWMI-1La7JG","outputId":"f1ad7ea8-6c46-426b-f009-98b9cf47574e"},"outputs":[],"execution_count":60},{"cell_type":"code","source":"import os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:47.542136Z","iopub.execute_input":"2025-01-10T08:42:47.542728Z","iopub.status.idle":"2025-01-10T08:42:47.546185Z","shell.execute_reply.started":"2025-01-10T08:42:47.542697Z","shell.execute_reply":"2025-01-10T08:42:47.545353Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"checkpoint_dir = \"./checkpoints\"\nos.makedirs(checkpoint_dir, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:47.829589Z","iopub.execute_input":"2025-01-10T08:42:47.829872Z","iopub.status.idle":"2025-01-10T08:42:47.833690Z","shell.execute_reply.started":"2025-01-10T08:42:47.829850Z","shell.execute_reply":"2025-01-10T08:42:47.832928Z"},"id":"_G2DGEL_a7JG"},"outputs":[],"execution_count":62},{"cell_type":"code","source":"# t5_encoder = T5Model.from_pretrained(checkpoint).encoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T07:53:09.111864Z","iopub.execute_input":"2025-01-10T07:53:09.112148Z","iopub.status.idle":"2025-01-10T07:53:13.560189Z","shell.execute_reply.started":"2025-01-10T07:53:09.112125Z","shell.execute_reply":"2025-01-10T07:53:13.559466Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36193b687f064d6e868bd8d8bb3ffb7b"}},"metadata":{}}],"execution_count":60},{"cell_type":"code","source":"# t5_encoder.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T07:46:30.860350Z","iopub.execute_input":"2025-01-10T07:46:30.860703Z","iopub.status.idle":"2025-01-10T07:46:31.018914Z","shell.execute_reply.started":"2025-01-10T07:46:30.860675Z","shell.execute_reply":"2025-01-10T07:46:31.018041Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"T5Stack(\n  (embed_tokens): Embedding(32128, 768)\n  (block): ModuleList(\n    (0): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n            (relative_attention_bias): Embedding(32, 12)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (1-11): 11 x T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (final_layer_norm): T5LayerNorm()\n  (dropout): Dropout(p=0.1, inplace=False)\n)"},"metadata":{}}],"execution_count":68},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    print(f\"Training started for epoch {epoch + 1}/{num_epochs}\")\n    model.train()\n    total_loss = 0\n\n    for idx, (metacognition_prompt_ids,\n              problem_student_code_ids,\n              problem_expected_code_ids,\n              student_code_ids,\n              target_ids) in enumerate(train_dataloader):\n        \n        \n        metacognition_prompt_ids = metacognition_prompt_ids.to(device)\n        problem_student_code_ids = problem_student_code_ids.to(device)\n        problem_expected_code_ids = problem_expected_code_ids.to(device)\n        student_code_ids = student_code_ids.to(device)\n        target_ids = target_ids.to(device)\n\n        #attention masking\n        student_attention_mask = (problem_student_code_ids != t5_pad_token_id).long().to(device)\n        expected_attention_mask = (problem_expected_code_ids != t5_pad_token_id).long().to(device)\n        metacognition_attention_mask = (metacognition_prompt_ids != t5_pad_token_id).long().to(device)\n\n        #encoding the decoder inputs for cross attention\n        # metacognition_prompt_encoded = t5_encoder(\n        #                                             input_ids=metacognition_prompt_ids,\n        #                                             attention_mask=metacognition_attention_mask).last_hidden_state\n\n        # problem_expected_code_encoded = t5_encoder(\n        #                                             input_ids=problem_expected_code_ids,\n        #                                             attention_mask=expected_attention_mask).last_hidden_state      \n        \n\n\n        optimizer.zero_grad()\n        logits = model(metacognition_prompt_ids,\n                       problem_student_code_ids ,\n                       problem_expected_code_ids,\n                      metacognition_attention_mask,\n                      expected_attention_mask)\n\n\n        logits = logits.view(-1, logits.size(-1))\n        \n        target_ids = target_ids.view(-1)\n        # print(logits.shape)\n        # print(target_ids.shape)\n\n\n        loss = LOSS(logits, target_ids)\n        total_loss += loss.item()\n\n\n        loss.backward()\n        # for name, param in model.named_parameters():\n        #     if 'context_encoder' in name:\n        #         assert param.grad is None, f\"Gradients found in frozen encoder {name}\"\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n\n        if idx % 10 == 0:\n            print(f\"Batch {idx + 1}/{len(train_dataloader)} | Loss: {loss.item():.4f}\" , end='\\r')\n\n\n\n\n\n\n    if epoch % 5 ==0 :\n            for name, param in model.named_parameters():\n                if param.requires_grad and param.grad is not None:\n                    print(f\"Layer: {name} | Grad Norm: {param.grad.norm().item()}\")\n                elif param.requires_grad:\n                    print(f\"Layer: {name} | Grad: None\")\n\n    if (epoch + 1) % 20 == 0:\n        checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch + 1}.pth\")\n        torch.save({\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': total_loss / max(len(train_dataloader), 1),\n        }, checkpoint_path)\n        print(f\"Checkpoint saved at {checkpoint_path}\")\n\n\n    avg_loss = total_loss / max(len(train_dataloader), 1)\n    #writer.add_scalar(\"Loss/train\", avg_loss, epoch + 1)\n    print(f\"Epoch [{epoch + 1}/{num_epochs}] completed | Average Loss: {avg_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:42:52.072512Z","iopub.execute_input":"2025-01-10T08:42:52.072807Z"},"id":"qBooPbJNa7JG","outputId":"c278d4f2-d0f6-408d-9237-2ae0a600ab95"},"outputs":[{"name":"stdout","text":"Training started for epoch 1/200\nLayer: token_embedding.weight | Grad Norm: 3.9925669107626627e-13\nLayer: position_embedding.weight | Grad Norm: 2.451501241590437e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 7.675216712188515e-11\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 3.4575411110571164e-12\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 9.322762700714193e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 6.135561243097998e-12\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 6.447607447945813e-10\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 4.4577869173778595e-10\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 8.173368093444822e-10\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 7.921868716564973e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 7.620996611557018e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 4.531779118632784e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 9.622820318355707e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 7.864074391683573e-10\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 1.1131973121081273e-09\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 3.2748001532745263e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 8.658035399733421e-10\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 3.1178970516521076e-10\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 1.3490381034486632e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 1.5892462693067877e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 1.2696619311469703e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 2.613546712382231e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 2.8369409044159966e-09\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 4.422197719122778e-09\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 1.0532056116119293e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 1.1320850923368653e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 2.0524963673107521e-10\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 2.132410220623271e-10\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 2.060235593237536e-10\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 2.1065506283779456e-10\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 6.4528333787450265e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 2.920244324133847e-10\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 8.100255577403459e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 5.197558494707266e-10\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 4.422870603093543e-08\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 3.0990836563660196e-08\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 5.79625520913396e-08\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 5.535439839832179e-08\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 5.5271460297490194e-08\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 3.124877423488215e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 7.476741359369043e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 5.488733023639725e-08\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 7.596324991254733e-08\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 2.2348340067424033e-08\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 6.12164967606077e-08\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 2.1857935905700288e-08\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 1.1909711439272996e-08\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 1.2657308534613776e-09\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 1.276185201959379e-07\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 1.9481207402805012e-07\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 2.6533567165643035e-07\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 3.4539749549367116e-07\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 1.0047784826383577e-06\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 9.045862725542975e-07\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 3.04897405101201e-08\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 3.115440705414585e-08\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 3.144697302559507e-08\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 3.1587170212787896e-08\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 1.016569513012655e-06\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 4.537228193157716e-08\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 1.718176122267323e-06\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 7.987459582636802e-08\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 8.815725777822081e-06\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 6.886350547574693e-06\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 1.2476341908040922e-05\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 1.1792338227678556e-05\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 1.0296680557075888e-05\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 6.72124815537245e-06\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 1.4210783774615265e-05\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 1.090110163204372e-05\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 1.5439667549799196e-05\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 4.981299298378872e-06\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 1.4502535123028792e-05\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 5.223592324909987e-06\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 3.4345478070463287e-06\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 1.9155048391894525e-07\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 3.803509389399551e-05\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 3.818125696852803e-05\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 8.777955372352153e-05\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 7.2567789175082e-05\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.00038803115603514016\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.00021356312208808959\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 1.83030770131154e-05\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 1.7174133972730488e-05\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 9.76058654487133e-05\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 6.363260035868734e-05\nLayer: transformer_blocks.3.input_self_attention.in_proj_weight | Grad Norm: 0.0009972654515877366\nLayer: transformer_blocks.3.input_self_attention.in_proj_bias | Grad Norm: 4.4087682908866554e-05\nLayer: transformer_blocks.3.input_self_attention.out_proj.weight | Grad Norm: 0.0017533344216644764\nLayer: transformer_blocks.3.input_self_attention.out_proj.bias | Grad Norm: 4.575944331008941e-05\nLayer: transformer_blocks.3.context_attn.in_proj_weight | Grad Norm: 0.019632553681731224\nLayer: transformer_blocks.3.context_attn.in_proj_bias | Grad Norm: 0.006117801181972027\nLayer: transformer_blocks.3.context_attn.out_proj.weight | Grad Norm: 0.025333089753985405\nLayer: transformer_blocks.3.context_attn.out_proj.bias | Grad Norm: 0.008006904274225235\nLayer: transformer_blocks.3.persona_attn.in_proj_weight | Grad Norm: 0.001890667830593884\nLayer: transformer_blocks.3.persona_attn.in_proj_bias | Grad Norm: 0.0017418137285858393\nLayer: transformer_blocks.3.persona_attn.out_proj.weight | Grad Norm: 0.0025373208336532116\nLayer: transformer_blocks.3.persona_attn.out_proj.bias | Grad Norm: 0.003058755537495017\nLayer: transformer_blocks.3.persona_proj.weight | Grad Norm: 0.0031701447442173958\nLayer: transformer_blocks.3.persona_proj.bias | Grad Norm: 0.0012364538852125406\nLayer: transformer_blocks.3.context_proj.weight | Grad Norm: 0.033401671797037125\nLayer: transformer_blocks.3.context_proj.bias | Grad Norm: 0.006007760297507048\nLayer: transformer_blocks.3.paa_layer.fc.weight | Grad Norm: 0.00459847180172801\nLayer: transformer_blocks.3.paa_layer.fc.bias | Grad Norm: 8.065978181548417e-05\nLayer: transformer_blocks.3.paa_layer.fc_out.weight | Grad Norm: 0.036069355905056\nLayer: transformer_blocks.3.paa_layer.fc_out.bias | Grad Norm: 0.013696100562810898\nLayer: transformer_blocks.3.mlp.0.weight | Grad Norm: 0.0609150156378746\nLayer: transformer_blocks.3.mlp.0.bias | Grad Norm: 0.025796953588724136\nLayer: transformer_blocks.3.mlp.3.weight | Grad Norm: 0.39509597420692444\nLayer: transformer_blocks.3.mlp.3.bias | Grad Norm: 0.15852777659893036\nLayer: transformer_blocks.3.mlp.4.weight | Grad Norm: 0.016827289015054703\nLayer: transformer_blocks.3.mlp.4.bias | Grad Norm: 0.01956862211227417\nLayer: transformer_blocks.3.layer_norm2.weight | Grad Norm: 0.019466660916805267\nLayer: transformer_blocks.3.layer_norm2.bias | Grad Norm: 0.021531669422984123\nLayer: final_fc.weight | Grad Norm: 0.8985642790794373\nLayer: final_fc.bias | Grad Norm: 0.04045197740197182\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [1/200] completed | Average Loss: 4.9447\nTraining started for epoch 2/200\nEpoch [2/200] completed | Average Loss: 4.7380\nTraining started for epoch 3/200\nEpoch [3/200] completed | Average Loss: 4.7235\nTraining started for epoch 4/200\nEpoch [4/200] completed | Average Loss: 4.7173\nTraining started for epoch 5/200\nEpoch [5/200] completed | Average Loss: 4.7138\nTraining started for epoch 6/200\nLayer: token_embedding.weight | Grad Norm: 5.5971462816113704e-14\nLayer: position_embedding.weight | Grad Norm: 3.721159936177082e-13\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 1.1750876313665337e-11\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 5.311989455074329e-13\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 1.3971478488028488e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 9.258601914655107e-13\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 9.358042118989829e-11\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 6.693844090133183e-11\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 1.2017213613102484e-10\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 1.1992259962845253e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 1.134688870640943e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 6.857617945721373e-11\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 1.4116925683982373e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 1.1820300294118624e-10\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 1.7335996527201303e-10\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 5.1080781965362476e-11\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 1.2760521805876834e-10\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 4.7924133411703096e-11\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 1.8207170146555818e-11\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 2.162306575112516e-12\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 1.8939758095193326e-10\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 4.017152166468918e-10\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 4.3223555290516913e-10\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 6.794195206438758e-10\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 1.647951108907364e-09\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 1.7810034558252141e-09\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 3.1396781008385943e-11\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 3.341302234893817e-11\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 3.16283770007697e-11\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 3.297094888776719e-11\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 1.0000820171995883e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 4.538159084632731e-11\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 1.2217692413329928e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 7.916478583780417e-11\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 6.553507514439616e-09\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 4.750817961252096e-09\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 8.440167675871635e-09\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 8.422726516243983e-09\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 8.478671098544055e-09\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 4.7832551253179645e-09\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 1.1593411564092548e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 8.488641789483609e-09\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 1.0908821401756086e-08\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 3.1864584304486243e-09\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 8.937243833884168e-09\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 3.321365404929111e-09\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 1.725635523364133e-09\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 1.8611338858942617e-10\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 1.9880129542571012e-08\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 3.091953360012667e-08\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 4.285277555027278e-08\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 5.636405475684114e-08\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 1.6746000142120465e-07\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 1.5163521993599716e-07\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 5.720326878844162e-09\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 5.468477226600044e-09\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 9.521208710339124e-09\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 7.025791504133849e-09\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 1.923982040352712e-07\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 8.605102408409948e-09\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 2.901972493418725e-07\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 1.335984212147423e-08\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 1.4016707154951291e-06\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 1.3823896551912185e-06\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 1.8887388932853355e-06\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 2.046904000962968e-06\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 1.184727921099693e-06\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 1.013238829727925e-06\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 1.6227957075898303e-06\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 1.6359385881514754e-06\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 2.166863168895361e-06\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 7.52482776533725e-07\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 3.2225339055003133e-06\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 1.1895604075107258e-06\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 5.884257916477509e-07\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 3.0522901539598024e-08\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 5.475753368955338e-06\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 6.159304120956222e-06\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 1.4019085938343778e-05\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 1.1045850442314986e-05\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 8.538927068002522e-05\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 3.579438998713158e-05\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 6.164466412883485e-06\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 6.120431862655096e-06\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 1.796272408682853e-05\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 1.0809108971443493e-05\nLayer: transformer_blocks.3.input_self_attention.in_proj_weight | Grad Norm: 0.0002494421787559986\nLayer: transformer_blocks.3.input_self_attention.in_proj_bias | Grad Norm: 1.1059399184887297e-05\nLayer: transformer_blocks.3.input_self_attention.out_proj.weight | Grad Norm: 0.0004940019571222365\nLayer: transformer_blocks.3.input_self_attention.out_proj.bias | Grad Norm: 1.6232708730967715e-05\nLayer: transformer_blocks.3.context_attn.in_proj_weight | Grad Norm: 0.008932500146329403\nLayer: transformer_blocks.3.context_attn.in_proj_bias | Grad Norm: 0.002920712810009718\nLayer: transformer_blocks.3.context_attn.out_proj.weight | Grad Norm: 0.013372305780649185\nLayer: transformer_blocks.3.context_attn.out_proj.bias | Grad Norm: 0.0032811802811920643\nLayer: transformer_blocks.3.persona_attn.in_proj_weight | Grad Norm: 0.00033959929714910686\nLayer: transformer_blocks.3.persona_attn.in_proj_bias | Grad Norm: 0.000657437602058053\nLayer: transformer_blocks.3.persona_attn.out_proj.weight | Grad Norm: 0.0005127196782268584\nLayer: transformer_blocks.3.persona_attn.out_proj.bias | Grad Norm: 0.0012268597492948174\nLayer: transformer_blocks.3.persona_proj.weight | Grad Norm: 0.0009181384230032563\nLayer: transformer_blocks.3.persona_proj.bias | Grad Norm: 0.00043389329221099615\nLayer: transformer_blocks.3.context_proj.weight | Grad Norm: 0.015413870103657246\nLayer: transformer_blocks.3.context_proj.bias | Grad Norm: 0.003110984107479453\nLayer: transformer_blocks.3.paa_layer.fc.weight | Grad Norm: 0.0013554333709180355\nLayer: transformer_blocks.3.paa_layer.fc.bias | Grad Norm: 3.0818515369901434e-05\nLayer: transformer_blocks.3.paa_layer.fc_out.weight | Grad Norm: 0.018794795498251915\nLayer: transformer_blocks.3.paa_layer.fc_out.bias | Grad Norm: 0.004745940212160349\nLayer: transformer_blocks.3.mlp.0.weight | Grad Norm: 0.030629361048340797\nLayer: transformer_blocks.3.mlp.0.bias | Grad Norm: 0.006944433320313692\nLayer: transformer_blocks.3.mlp.3.weight | Grad Norm: 0.1566050499677658\nLayer: transformer_blocks.3.mlp.3.bias | Grad Norm: 0.022102754563093185\nLayer: transformer_blocks.3.mlp.4.weight | Grad Norm: 0.009831633418798447\nLayer: transformer_blocks.3.mlp.4.bias | Grad Norm: 0.010692709125578403\nLayer: transformer_blocks.3.layer_norm2.weight | Grad Norm: 0.010914661921560764\nLayer: transformer_blocks.3.layer_norm2.bias | Grad Norm: 0.010601884685456753\nLayer: final_fc.weight | Grad Norm: 0.49257081747055054\nLayer: final_fc.bias | Grad Norm: 0.01986795850098133\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [6/200] completed | Average Loss: 4.7115\nTraining started for epoch 7/200\nEpoch [7/200] completed | Average Loss: 4.7103\nTraining started for epoch 8/200\nEpoch [8/200] completed | Average Loss: 4.7091\nTraining started for epoch 9/200\nEpoch [9/200] completed | Average Loss: 4.7082\nTraining started for epoch 10/200\nEpoch [10/200] completed | Average Loss: 4.7075\nTraining started for epoch 11/200\nLayer: token_embedding.weight | Grad Norm: 2.3199983397608794e-14\nLayer: position_embedding.weight | Grad Norm: 1.5912106159608752e-13\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 4.877026300170728e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 2.2187887107043974e-13\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 5.914470736084754e-12\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 3.9446920664827634e-13\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 3.7867764479670996e-11\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 2.644311505872743e-11\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 4.600776704055676e-11\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 4.5789327191014806e-11\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 4.3465613053239593e-11\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 2.6288603238722175e-11\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 5.4286124206992525e-11\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 4.574497725062798e-11\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 6.594744889065751e-11\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 1.9464839462468575e-11\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 5.1688105184855004e-11\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 1.8582458488625164e-11\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 7.711869337567734e-12\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 9.304783506192127e-13\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 7.437034321711167e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 1.598530335877868e-10\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 1.7527673756845275e-10\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 2.768194007352065e-10\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 6.580820333113024e-10\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 7.13743619762397e-10\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 1.2491110576440256e-11\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 1.3399724906049126e-11\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 1.2517688274815697e-11\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 1.3221105633898222e-11\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 4.1067046985254763e-10\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 1.868972511476219e-11\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 5.259755408992817e-10\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 3.416878238682308e-11\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 3.1468585515170844e-09\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 2.22460183607609e-09\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 3.84854415003133e-09\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 3.7242964268102696e-09\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 3.7205503122805794e-09\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 2.1395800686718758e-09\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 4.8998414214906916e-09\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 3.687281591169267e-09\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 5.0690185382507025e-09\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 1.5000738429193916e-09\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 4.266663911067781e-09\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 1.5388377239133888e-09\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 7.414966973762205e-10\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 8.028357839640066e-11\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 8.25708390550517e-09\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 1.3064956227992752e-08\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 1.6746234621223266e-08\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 2.2217628625753605e-08\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 6.369161553720915e-08\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 5.795470414682313e-08\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 2.09700656839118e-09\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 2.100234430812975e-09\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 2.110603913862974e-09\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 2.1095183377894955e-09\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 6.620271619794948e-08\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 2.9681832547368003e-09\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 1.1549857958925713e-07\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 4.9582635774925166e-09\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 7.230789833556628e-07\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 7.832791197870392e-07\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 9.006730010696629e-07\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 1.1270296909060562e-06\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 4.294138591376395e-07\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 4.253382996921573e-07\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 6.553960361088684e-07\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 7.573863172183337e-07\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 8.349987865585717e-07\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 2.9915668164903764e-07\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 1.9237122614867985e-06\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 6.709949502692325e-07\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 2.5504928657937853e-07\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 1.1538573652103423e-08\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 2.51718370236631e-06\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 2.9897021249780664e-06\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 7.635608199052513e-06\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 5.4311776693793945e-06\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 6.331162148853764e-05\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 1.98382003873121e-05\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 7.698026820435189e-06\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 6.251399099710397e-06\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 5.929986946284771e-05\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 2.5399782316526398e-05\nLayer: transformer_blocks.3.input_self_attention.in_proj_weight | Grad Norm: 0.0005806650733575225\nLayer: transformer_blocks.3.input_self_attention.in_proj_bias | Grad Norm: 2.589668656582944e-05\nLayer: transformer_blocks.3.input_self_attention.out_proj.weight | Grad Norm: 0.0005918372189626098\nLayer: transformer_blocks.3.input_self_attention.out_proj.bias | Grad Norm: 2.5200864911312237e-05\nLayer: transformer_blocks.3.context_attn.in_proj_weight | Grad Norm: 0.00738887581974268\nLayer: transformer_blocks.3.context_attn.in_proj_bias | Grad Norm: 0.0016562719829380512\nLayer: transformer_blocks.3.context_attn.out_proj.weight | Grad Norm: 0.011385940946638584\nLayer: transformer_blocks.3.context_attn.out_proj.bias | Grad Norm: 0.001736464793793857\nLayer: transformer_blocks.3.persona_attn.in_proj_weight | Grad Norm: 0.000580223451834172\nLayer: transformer_blocks.3.persona_attn.in_proj_bias | Grad Norm: 0.00036589885712601244\nLayer: transformer_blocks.3.persona_attn.out_proj.weight | Grad Norm: 0.0009231342119164765\nLayer: transformer_blocks.3.persona_attn.out_proj.bias | Grad Norm: 0.0006829914636909962\nLayer: transformer_blocks.3.persona_proj.weight | Grad Norm: 0.001212208648212254\nLayer: transformer_blocks.3.persona_proj.bias | Grad Norm: 0.0002704079379327595\nLayer: transformer_blocks.3.context_proj.weight | Grad Norm: 0.01067906990647316\nLayer: transformer_blocks.3.context_proj.bias | Grad Norm: 0.0017679769080132246\nLayer: transformer_blocks.3.paa_layer.fc.weight | Grad Norm: 0.00074516556924209\nLayer: transformer_blocks.3.paa_layer.fc.bias | Grad Norm: 2.3396662072627805e-05\nLayer: transformer_blocks.3.paa_layer.fc_out.weight | Grad Norm: 0.017194492742419243\nLayer: transformer_blocks.3.paa_layer.fc_out.bias | Grad Norm: 0.0025997336488217115\nLayer: transformer_blocks.3.mlp.0.weight | Grad Norm: 0.03683481365442276\nLayer: transformer_blocks.3.mlp.0.bias | Grad Norm: 0.004941487684845924\nLayer: transformer_blocks.3.mlp.3.weight | Grad Norm: 0.17474786937236786\nLayer: transformer_blocks.3.mlp.3.bias | Grad Norm: 0.012053979560732841\nLayer: transformer_blocks.3.mlp.4.weight | Grad Norm: 0.011858304031193256\nLayer: transformer_blocks.3.mlp.4.bias | Grad Norm: 0.012318608351051807\nLayer: transformer_blocks.3.layer_norm2.weight | Grad Norm: 0.013749991543591022\nLayer: transformer_blocks.3.layer_norm2.bias | Grad Norm: 0.011242656037211418\nLayer: final_fc.weight | Grad Norm: 0.6808642745018005\nLayer: final_fc.bias | Grad Norm: 0.023332156240940094\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [11/200] completed | Average Loss: 4.7069\nTraining started for epoch 12/200\nEpoch [12/200] completed | Average Loss: 4.7063\nTraining started for epoch 13/200\nEpoch [13/200] completed | Average Loss: 4.7059\nTraining started for epoch 14/200\nEpoch [14/200] completed | Average Loss: 4.7053\nTraining started for epoch 15/200\nEpoch [15/200] completed | Average Loss: 4.7049\nTraining started for epoch 16/200\nLayer: token_embedding.weight | Grad Norm: 3.469912820074604e-14\nLayer: position_embedding.weight | Grad Norm: 2.2822959884830074e-13\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 7.240867627711367e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 3.309506802721268e-13\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 9.035985501504307e-12\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 6.096442795033852e-13\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 6.516027994951656e-11\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 4.456114990891713e-11\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 7.8333874109493e-11\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 7.675281937791212e-11\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 6.956506204414126e-11\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 4.334523018290071e-11\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 8.811935209296351e-11\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 7.711450922265328e-11\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 9.45211409142388e-11\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 2.8748458474892047e-11\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 8.615822638669002e-11\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 3.0225551228557634e-11\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 1.2660336320968746e-11\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 1.5419947220138286e-12\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 1.201549970630822e-10\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 2.6404164965576626e-10\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 2.669759135986993e-10\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 4.26986446200317e-10\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 9.90738380224343e-10\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 1.0823966167805565e-09\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 1.8399209239516878e-11\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 2.0262815730864858e-11\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 1.8625951475614855e-11\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 2.0092871810817314e-11\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 6.265536423022411e-10\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 2.8602651497289244e-11\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 7.967825288446306e-10\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 5.194923310969379e-11\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 4.366899730712248e-09\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 2.9846913829345567e-09\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 5.756750187657644e-09\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 5.374996003837396e-09\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 5.648747247732899e-09\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 3.282804916793225e-09\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 7.286127701178202e-09\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 5.4608064736783035e-09\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 7.534382007179374e-09\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 2.2956221368275465e-09\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 5.926059198912981e-09\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 2.0768549102712086e-09\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 1.1269051247708717e-09\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 1.2233310198173086e-10\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 1.272269134489079e-08\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 1.8763806508559355e-08\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 2.81145862146559e-08\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 3.5266602083083853e-08\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 1.0628031787973669e-07\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 9.337374962115064e-08\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 4.072906900631779e-09\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 3.938886994347968e-09\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 4.107898021743495e-09\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 3.945280546702179e-09\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 1.271335463570722e-07\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 5.7147273579971625e-09\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 1.5910929107576521e-07\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 9.622518781782219e-09\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 7.394623935397249e-07\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 5.682521759808878e-07\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 1.3053302154730773e-06\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 8.746743560550385e-07\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 7.680840781176812e-07\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 4.463265383947146e-07\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 1.4436539004236693e-06\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 7.93836704815476e-07\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 9.42376459533989e-07\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 3.17629456958457e-07\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 1.3461609569276334e-06\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 4.715297450275102e-07\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 3.216435970898601e-07\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 2.4714163160410862e-08\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 4.9810719247034285e-06\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 2.709909949771827e-06\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 1.49407351273112e-05\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 4.786884801433189e-06\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 9.47461521718651e-05\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 1.2299086847633589e-05\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 1.010623873298755e-05\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 1.1836437806778122e-05\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 7.717990956734866e-05\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 2.8429822123143822e-05\nLayer: transformer_blocks.3.input_self_attention.in_proj_weight | Grad Norm: 0.0006997738964855671\nLayer: transformer_blocks.3.input_self_attention.in_proj_bias | Grad Norm: 3.137267776764929e-05\nLayer: transformer_blocks.3.input_self_attention.out_proj.weight | Grad Norm: 0.0005763127701357007\nLayer: transformer_blocks.3.input_self_attention.out_proj.bias | Grad Norm: 2.81869615719188e-05\nLayer: transformer_blocks.3.context_attn.in_proj_weight | Grad Norm: 0.007297204807400703\nLayer: transformer_blocks.3.context_attn.in_proj_bias | Grad Norm: 0.0016737914411351085\nLayer: transformer_blocks.3.context_attn.out_proj.weight | Grad Norm: 0.010512598790228367\nLayer: transformer_blocks.3.context_attn.out_proj.bias | Grad Norm: 0.0014708803500980139\nLayer: transformer_blocks.3.persona_attn.in_proj_weight | Grad Norm: 0.002224852330982685\nLayer: transformer_blocks.3.persona_attn.in_proj_bias | Grad Norm: 0.00048773238086141646\nLayer: transformer_blocks.3.persona_attn.out_proj.weight | Grad Norm: 0.0033121975138783455\nLayer: transformer_blocks.3.persona_attn.out_proj.bias | Grad Norm: 0.0006341827684082091\nLayer: transformer_blocks.3.persona_proj.weight | Grad Norm: 0.0038027644623070955\nLayer: transformer_blocks.3.persona_proj.bias | Grad Norm: 0.0005488930037245154\nLayer: transformer_blocks.3.context_proj.weight | Grad Norm: 0.010424649342894554\nLayer: transformer_blocks.3.context_proj.bias | Grad Norm: 0.0019112026784569025\nLayer: transformer_blocks.3.paa_layer.fc.weight | Grad Norm: 0.0009973107371479273\nLayer: transformer_blocks.3.paa_layer.fc.bias | Grad Norm: 3.265522900619544e-05\nLayer: transformer_blocks.3.paa_layer.fc_out.weight | Grad Norm: 0.014797160401940346\nLayer: transformer_blocks.3.paa_layer.fc_out.bias | Grad Norm: 0.0016385297058150172\nLayer: transformer_blocks.3.mlp.0.weight | Grad Norm: 0.02694050408899784\nLayer: transformer_blocks.3.mlp.0.bias | Grad Norm: 0.002281100954860449\nLayer: transformer_blocks.3.mlp.3.weight | Grad Norm: 0.13668864965438843\nLayer: transformer_blocks.3.mlp.3.bias | Grad Norm: 0.006453212350606918\nLayer: transformer_blocks.3.mlp.4.weight | Grad Norm: 0.011312458664178848\nLayer: transformer_blocks.3.mlp.4.bias | Grad Norm: 0.014318269677460194\nLayer: transformer_blocks.3.layer_norm2.weight | Grad Norm: 0.020947279408574104\nLayer: transformer_blocks.3.layer_norm2.bias | Grad Norm: 0.020151831209659576\nLayer: final_fc.weight | Grad Norm: 0.9882393479347229\nLayer: final_fc.bias | Grad Norm: 0.04702034592628479\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [16/200] completed | Average Loss: 4.7043\nTraining started for epoch 17/200\nEpoch [17/200] completed | Average Loss: 4.7038\nTraining started for epoch 18/200\nEpoch [18/200] completed | Average Loss: 4.7033\nTraining started for epoch 19/200\nEpoch [19/200] completed | Average Loss: 4.7027\nTraining started for epoch 20/200\nCheckpoint saved at ./checkpoints/model_epoch_20.pth\nEpoch [20/200] completed | Average Loss: 4.7021\nTraining started for epoch 21/200\nLayer: token_embedding.weight | Grad Norm: 1.5047692096276793e-13\nLayer: position_embedding.weight | Grad Norm: 5.171062146189231e-13\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 1.681414035781259e-11\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 7.330074047739998e-13\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 2.041838052802003e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 1.3109974260697155e-12\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 1.3497400419559824e-10\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 9.374696158248597e-11\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 1.7331394652764232e-10\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 1.7321259704328185e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 1.711659702863244e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 1.030618507202874e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 2.0716230120232382e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 1.7491837145389155e-10\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 2.446771674158299e-10\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 7.129959123108875e-11\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 1.7809206886987283e-10\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 6.250108069982829e-11\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 2.8585111708223643e-11\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 3.2884840683866656e-12\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 2.758248629497473e-10\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 5.946406700374496e-10\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 6.189662116184991e-10\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 9.846095050392023e-10\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 2.1884696277396642e-09\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 2.3898882872686045e-09\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 4.4820615968665933e-11\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 4.4648763852794815e-11\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 4.510038870142452e-11\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 4.418282406604135e-11\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 1.4316727803276308e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 6.554704001793255e-11\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 1.7860350975951178e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 1.1688051915204056e-10\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 1.0310888143294505e-08\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 7.123035938860767e-09\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 1.3130942555505953e-08\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 1.2139977023650772e-08\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 1.2140520588843629e-08\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 6.772563843071566e-09\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 1.703609697756292e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 1.2275450878007632e-08\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 1.5913970585756942e-08\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 4.63793226046505e-09\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 1.5031343281179943e-08\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 5.281485027097688e-09\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 2.647950747558525e-09\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 2.8966090637183584e-10\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 2.859600911619964e-08\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 4.0877456086718666e-08\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 6.210126457517617e-08\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 7.691669168252702e-08\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 2.331565696067628e-07\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 2.041579136857763e-07\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 8.711716681375492e-09\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 8.896352987619593e-09\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 1.0174337816692969e-08\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 9.34813293440584e-09\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 3.092046370056778e-07\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 1.3941568788311542e-08\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 3.838627264940442e-07\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 2.4261350262122505e-08\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 2.033163582382258e-06\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 2.0030195173603715e-06\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 2.290287511641509e-06\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 2.3175671231001616e-06\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 1.515420080977492e-06\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 9.457459100303822e-07\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 2.8590313831955427e-06\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 1.787929363672447e-06\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 2.0286850030970527e-06\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 6.760488417967281e-07\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 6.037580078555038e-06\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 2.1252349142741878e-06\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 6.754285664101189e-07\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 5.450847240240364e-08\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 7.723173439444508e-06\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 5.463682100526057e-06\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 2.313450022484176e-05\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 8.974768206826411e-06\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.0002732647699303925\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 3.7173565942794085e-05\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 5.560955105465837e-05\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 3.8718353607691824e-05\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.0005090230843052268\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.0001515378535259515\nLayer: transformer_blocks.3.input_self_attention.in_proj_weight | Grad Norm: 0.004715400747954845\nLayer: transformer_blocks.3.input_self_attention.in_proj_bias | Grad Norm: 0.00021279514476191252\nLayer: transformer_blocks.3.input_self_attention.out_proj.weight | Grad Norm: 0.002981861587613821\nLayer: transformer_blocks.3.input_self_attention.out_proj.bias | Grad Norm: 0.00018542443285696208\nLayer: transformer_blocks.3.context_attn.in_proj_weight | Grad Norm: 0.012859219685196877\nLayer: transformer_blocks.3.context_attn.in_proj_bias | Grad Norm: 0.0024214109871536493\nLayer: transformer_blocks.3.context_attn.out_proj.weight | Grad Norm: 0.019094403833150864\nLayer: transformer_blocks.3.context_attn.out_proj.bias | Grad Norm: 0.0020843264646828175\nLayer: transformer_blocks.3.persona_attn.in_proj_weight | Grad Norm: 0.005365058779716492\nLayer: transformer_blocks.3.persona_attn.in_proj_bias | Grad Norm: 0.000859987863805145\nLayer: transformer_blocks.3.persona_attn.out_proj.weight | Grad Norm: 0.0090469466522336\nLayer: transformer_blocks.3.persona_attn.out_proj.bias | Grad Norm: 0.001066394615918398\nLayer: transformer_blocks.3.persona_proj.weight | Grad Norm: 0.007733037229627371\nLayer: transformer_blocks.3.persona_proj.bias | Grad Norm: 0.0010102696251124144\nLayer: transformer_blocks.3.context_proj.weight | Grad Norm: 0.016449162736535072\nLayer: transformer_blocks.3.context_proj.bias | Grad Norm: 0.002576404018327594\nLayer: transformer_blocks.3.paa_layer.fc.weight | Grad Norm: 0.0031020769383758307\nLayer: transformer_blocks.3.paa_layer.fc.bias | Grad Norm: 0.00011186266056029126\nLayer: transformer_blocks.3.paa_layer.fc_out.weight | Grad Norm: 0.03227175772190094\nLayer: transformer_blocks.3.paa_layer.fc_out.bias | Grad Norm: 0.0024836547672748566\nLayer: transformer_blocks.3.mlp.0.weight | Grad Norm: 0.05914461240172386\nLayer: transformer_blocks.3.mlp.0.bias | Grad Norm: 0.003507063025608659\nLayer: transformer_blocks.3.mlp.3.weight | Grad Norm: 0.16033253073692322\nLayer: transformer_blocks.3.mlp.3.bias | Grad Norm: 0.005452668759971857\nLayer: transformer_blocks.3.mlp.4.weight | Grad Norm: 0.012263622134923935\nLayer: transformer_blocks.3.mlp.4.bias | Grad Norm: 0.014198187738656998\nLayer: transformer_blocks.3.layer_norm2.weight | Grad Norm: 0.014881135895848274\nLayer: transformer_blocks.3.layer_norm2.bias | Grad Norm: 0.013913022354245186\nLayer: final_fc.weight | Grad Norm: 0.6605457067489624\nLayer: final_fc.bias | Grad Norm: 0.01992328278720379\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [21/200] completed | Average Loss: 4.7015\nTraining started for epoch 22/200\nEpoch [22/200] completed | Average Loss: 4.7007\nTraining started for epoch 23/200\nEpoch [23/200] completed | Average Loss: 4.7001\nTraining started for epoch 24/200\nEpoch [24/200] completed | Average Loss: 4.6994\nTraining started for epoch 25/200\nEpoch [25/200] completed | Average Loss: 4.6986\nTraining started for epoch 26/200\nLayer: token_embedding.weight | Grad Norm: 9.855582739888916e-14\nLayer: position_embedding.weight | Grad Norm: 4.492919166050602e-13\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 1.4709825371661545e-11\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 6.603163111781885e-13\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 1.888630050961293e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 1.2433526430655206e-12\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 1.3149470401430108e-10\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 9.061441180735486e-11\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 1.6613750652982873e-10\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 1.6371570765727483e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 1.6369500199786557e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 9.914265242105813e-11\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 1.874603111629014e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 1.606562799461031e-10\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 2.3626012257693674e-10\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 6.850110062517345e-11\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 1.8539060564481957e-10\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 6.371375649294464e-11\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 2.5089368083097696e-11\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 2.9940709200121374e-12\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 2.7384411405151354e-10\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 5.939725378212302e-10\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 6.50122677914311e-10\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 1.0405432071536325e-09\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 2.4924831087957955e-09\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 2.733296478041325e-09\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 5.088980278844524e-11\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 5.097856511926402e-11\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 5.323180785055115e-11\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 5.1045528914883675e-11\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 1.5691767885073205e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 7.202689139562679e-11\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 1.8034083115736621e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 1.185057330044259e-10\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 1.1028017610215102e-08\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 7.419231895511302e-09\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 1.474497501874339e-08\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 1.3053118586014989e-08\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 1.376975955480475e-08\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 7.664258561135284e-09\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 1.8309231464286313e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 1.3039394453073783e-08\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 1.961644180425992e-08\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 5.691948246067113e-09\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 1.530363391566425e-08\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 5.2580957365933045e-09\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 2.6720112789035966e-09\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 2.940984678012626e-10\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 3.137953541454408e-08\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 4.4266300847084494e-08\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 6.266602525784037e-08\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 7.683395608637511e-08\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 2.3799834991677926e-07\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 2.0658049493249564e-07\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 9.962346503300523e-09\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 9.95400384340428e-09\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 2.3135882543101616e-08\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 1.444393138427813e-08\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 4.4013700062350836e-07\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 1.9887496094384005e-08\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 4.7416114057341474e-07\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 2.9376703736261334e-08\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 2.6847976641874993e-06\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 1.4837760318187065e-06\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 2.9520713269448606e-06\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 1.4614519159295014e-06\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 9.599167469787062e-07\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 5.956491122560692e-07\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 1.5299622191378148e-06\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 1.0526313189984648e-06\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 1.340213884759578e-06\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 4.4038660007572616e-07\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 5.562355454458157e-06\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 1.868645881586417e-06\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 7.813705451553687e-07\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 5.8748732101321366e-08\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 5.4300721785693895e-06\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 3.0378164410649333e-06\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 1.552511821500957e-05\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 4.835799700231291e-06\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.00016748277994338423\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 1.774955126165878e-05\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 3.397814725758508e-05\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 2.623406908242032e-05\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.00011001609527738765\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 3.950683822040446e-05\nLayer: transformer_blocks.3.input_self_attention.in_proj_weight | Grad Norm: 0.001314777065999806\nLayer: transformer_blocks.3.input_self_attention.in_proj_bias | Grad Norm: 5.965723539702594e-05\nLayer: transformer_blocks.3.input_self_attention.out_proj.weight | Grad Norm: 0.0013081301003694534\nLayer: transformer_blocks.3.input_self_attention.out_proj.bias | Grad Norm: 8.994616655400023e-05\nLayer: transformer_blocks.3.context_attn.in_proj_weight | Grad Norm: 0.010875962674617767\nLayer: transformer_blocks.3.context_attn.in_proj_bias | Grad Norm: 0.002564104739576578\nLayer: transformer_blocks.3.context_attn.out_proj.weight | Grad Norm: 0.020046014338731766\nLayer: transformer_blocks.3.context_attn.out_proj.bias | Grad Norm: 0.002131490269675851\nLayer: transformer_blocks.3.persona_attn.in_proj_weight | Grad Norm: 0.005766311660408974\nLayer: transformer_blocks.3.persona_attn.in_proj_bias | Grad Norm: 0.000554412545170635\nLayer: transformer_blocks.3.persona_attn.out_proj.weight | Grad Norm: 0.01423918828368187\nLayer: transformer_blocks.3.persona_attn.out_proj.bias | Grad Norm: 0.0008234740234911442\nLayer: transformer_blocks.3.persona_proj.weight | Grad Norm: 0.004629124887287617\nLayer: transformer_blocks.3.persona_proj.bias | Grad Norm: 0.0004063659580424428\nLayer: transformer_blocks.3.context_proj.weight | Grad Norm: 0.014462575316429138\nLayer: transformer_blocks.3.context_proj.bias | Grad Norm: 0.003142598317936063\nLayer: transformer_blocks.3.paa_layer.fc.weight | Grad Norm: 0.007709088735282421\nLayer: transformer_blocks.3.paa_layer.fc.bias | Grad Norm: 0.00018409792392048985\nLayer: transformer_blocks.3.paa_layer.fc_out.weight | Grad Norm: 0.03324735909700394\nLayer: transformer_blocks.3.paa_layer.fc_out.bias | Grad Norm: 0.0018776596989482641\nLayer: transformer_blocks.3.mlp.0.weight | Grad Norm: 0.04002201929688454\nLayer: transformer_blocks.3.mlp.0.bias | Grad Norm: 0.0019688680768013\nLayer: transformer_blocks.3.mlp.3.weight | Grad Norm: 0.12647245824337006\nLayer: transformer_blocks.3.mlp.3.bias | Grad Norm: 0.003238154109567404\nLayer: transformer_blocks.3.mlp.4.weight | Grad Norm: 0.010599404573440552\nLayer: transformer_blocks.3.mlp.4.bias | Grad Norm: 0.011676651425659657\nLayer: transformer_blocks.3.layer_norm2.weight | Grad Norm: 0.015766669064760208\nLayer: transformer_blocks.3.layer_norm2.bias | Grad Norm: 0.01343300100415945\nLayer: final_fc.weight | Grad Norm: 0.7188268899917603\nLayer: final_fc.bias | Grad Norm: 0.02732056751847267\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [26/200] completed | Average Loss: 4.6979\nTraining started for epoch 27/200\nEpoch [27/200] completed | Average Loss: 4.6971\nTraining started for epoch 28/200\nEpoch [28/200] completed | Average Loss: 4.6962\nTraining started for epoch 29/200\nEpoch [29/200] completed | Average Loss: 4.6952\nTraining started for epoch 30/200\nBatch 1211/1250 | Loss: 5.1729\r","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"torch.cuda.memory_summary(device='cuda', abbreviated=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T07:49:46.167024Z","iopub.execute_input":"2025-01-10T07:49:46.167397Z","iopub.status.idle":"2025-01-10T07:49:46.173875Z","shell.execute_reply.started":"2025-01-10T07:49:46.167365Z","shell.execute_reply":"2025-01-10T07:49:46.172871Z"}},"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 2            |        cudaMalloc retries: 2         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |   7929 MiB |   7972 MiB |  14364 MiB |   6434 MiB |\\n|       from large pool |   7927 MiB |   7970 MiB |  14332 MiB |   6405 MiB |\\n|       from small pool |      1 MiB |      3 MiB |     31 MiB |     29 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |   7929 MiB |   7972 MiB |  14364 MiB |   6434 MiB |\\n|       from large pool |   7927 MiB |   7970 MiB |  14332 MiB |   6405 MiB |\\n|       from small pool |      1 MiB |      3 MiB |     31 MiB |     29 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |   7926 MiB |   7968 MiB |  14360 MiB |   6433 MiB |\\n|       from large pool |   7924 MiB |   7966 MiB |  14328 MiB |   6404 MiB |\\n|       from small pool |      1 MiB |      3 MiB |     31 MiB |     29 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |   8202 MiB |   8252 MiB |   8302 MiB | 102400 KiB |\\n|       from large pool |   8200 MiB |   8248 MiB |   8296 MiB |  98304 KiB |\\n|       from small pool |      2 MiB |      4 MiB |      6 MiB |   4096 KiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory | 279286 KiB | 297210 KiB |   6963 MiB |   6690 MiB |\\n|       from large pool | 279080 KiB | 297000 KiB |   6931 MiB |   6659 MiB |\\n|       from small pool |    206 KiB |   2132 KiB |     31 MiB |     31 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |    1023    |    1025    |    1854    |     831    |\\n|       from large pool |     774    |     776    |    1320    |     546    |\\n|       from small pool |     249    |     250    |     534    |     285    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |    1023    |    1025    |    1854    |     831    |\\n|       from large pool |     774    |     776    |    1320    |     546    |\\n|       from small pool |     249    |     250    |     534    |     285    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     244    |     246    |     248    |       4    |\\n|       from large pool |     243    |     244    |     245    |       2    |\\n|       from small pool |       1    |       2    |       3    |       2    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |     135    |     136    |     666    |     531    |\\n|       from large pool |     129    |     130    |     509    |     380    |\\n|       from small pool |       6    |       7    |     157    |     151    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"},"metadata":{}}],"execution_count":74},{"cell_type":"code","source":"export CUDA_LAUNCH_BLOCKING=1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T14:16:16.118335Z","iopub.execute_input":"2025-01-08T14:16:16.118626Z","iopub.status.idle":"2025-01-08T14:16:16.123946Z","shell.execute_reply.started":"2025-01-08T14:16:16.118603Z","shell.execute_reply":"2025-01-08T14:16:16.122821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), 'paamodel.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:15:35.127979Z","iopub.execute_input":"2025-01-02T09:15:35.1283Z","iopub.status.idle":"2025-01-02T09:15:38.043298Z","shell.execute_reply.started":"2025-01-02T09:15:35.128272Z","shell.execute_reply":"2025-01-02T09:15:38.042319Z"},"id":"wkDGQ4eRa7JH"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = PAAModel()\n\n# Load the saved state dict\nmodel.load_state_dict(torch.load('paamodel.pth'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:15:38.044438Z","iopub.execute_input":"2025-01-02T09:15:38.044677Z","iopub.status.idle":"2025-01-02T09:15:43.200276Z","shell.execute_reply.started":"2025-01-02T09:15:38.044656Z","shell.execute_reply":"2025-01-02T09:15:43.199391Z"},"id":"7TcPgRsja7JH","outputId":"9367d669-eabf-48fb-e834-0d05129193bf"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_eval = df[9999:10000].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:13:31.90417Z","iopub.execute_input":"2025-01-02T09:13:31.904508Z","iopub.status.idle":"2025-01-02T09:13:31.909558Z","shell.execute_reply.started":"2025-01-02T09:13:31.904481Z","shell.execute_reply":"2025-01-02T09:13:31.908551Z"},"id":"K1MXJesua7JI"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:13:33.152773Z","iopub.execute_input":"2025-01-02T09:13:33.153078Z","iopub.status.idle":"2025-01-02T09:13:33.167906Z","shell.execute_reply.started":"2025-01-02T09:13:33.153052Z","shell.execute_reply":"2025-01-02T09:13:33.166951Z"},"id":"9D4sb3pxa7JI","outputId":"114e6bf8-46fd-487a-d0d7-713c7e4ec370"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_dataset = CustomDataset(df_eval, t5_tokenizer,gpt2_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:13:36.045686Z","iopub.execute_input":"2025-01-02T09:13:36.046004Z","iopub.status.idle":"2025-01-02T09:13:36.049807Z","shell.execute_reply.started":"2025-01-02T09:13:36.045976Z","shell.execute_reply":"2025-01-02T09:13:36.048902Z"},"id":"p0goTw6Da7JI"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef inference(model,gpt2_tokenizer, t5_tokenizer, eval_dataset, device):\n    model.eval()\n    model.to(device)\n\n    metacognitive_vector_ids, problem_student_code_ids, problem_expected_code_ids,student_code_ids, target_ids = eval_dataset[0]\n\n    metacognitive_tensor = metacognitive_vector_ids.unsqueeze(0).to(device)\n    problem_student_code_tensor = problem_student_code_ids.unsqueeze(0).to(device)\n    problem_expected_code_tensor = problem_expected_code_ids.unsqueeze(0).to(device)\n    target_tensor = target_ids.unsqueeze(0).to(device)\n\n    student_attention_mask = (problem_student_code_tensor != t5_tokenizer.pad_token_id).long().to(device)\n    expected_attention_mask = (problem_expected_code_tensor != t5_tokenizer.pad_token_id).long().to(device)\n\n\n    with torch.no_grad():\n\n        logits = model(\n            metacognitive_vector_ids=metacognitive_tensor,\n            problem_student_code_ids=problem_student_code_tensor,\n            problem_expected_code_ids=problem_expected_code_tensor,\n            expected_attention_mask=expected_attention_mask,\n            student_attention_mask=student_attention_mask\n        )\n\n        predictions = logits.argmax(dim=-1).squeeze().tolist()\n        filtered_tokens = [token for token in predictions if token != 0]\n        #decoded_text = t5_tokenizer.decode(filtered_tokens, skip_special_tokens=False)\n        decoded_text = t5_tokenizer.decode(predictions, skip_special_tokens=True)\n\n\n        return filtered_tokens, decoded_text\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:42:25.839368Z","iopub.execute_input":"2025-01-02T09:42:25.839702Z","iopub.status.idle":"2025-01-02T09:42:25.84611Z","shell.execute_reply.started":"2025-01-02T09:42:25.839676Z","shell.execute_reply":"2025-01-02T09:42:25.845101Z"},"id":"GGKQrOYQa7JJ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions, decoded_text = inference(model, gpt2_tokenizer, t5_tokenizer, eval_dataset, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:42:26.462145Z","iopub.execute_input":"2025-01-02T09:42:26.462503Z","iopub.status.idle":"2025-01-02T09:42:26.555247Z","shell.execute_reply.started":"2025-01-02T09:42:26.462473Z","shell.execute_reply":"2025-01-02T09:42:26.554554Z"},"id":"Nk1Ua-_va7JJ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Predicted Tokens:\", predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:42:26.833831Z","iopub.execute_input":"2025-01-02T09:42:26.834114Z","iopub.status.idle":"2025-01-02T09:42:26.838824Z","shell.execute_reply.started":"2025-01-02T09:42:26.834092Z","shell.execute_reply":"2025-01-02T09:42:26.837971Z"},"id":"0i90o8Nfa7JK","outputId":"f02ea63b-45b7-4fbf-84b4-46ed4860ea3d"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Decoded Text:\", decoded_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:42:27.089445Z","iopub.execute_input":"2025-01-02T09:42:27.089745Z","iopub.status.idle":"2025-01-02T09:42:27.094494Z","shell.execute_reply.started":"2025-01-02T09:42:27.089722Z","shell.execute_reply":"2025-01-02T09:42:27.093656Z"},"id":"Mv7lQhaFa7JK","outputId":"982d3c1f-680d-4721-f878-71bc628e6a92"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_checkpoint(checkpoint_path, model, optimizer=None):\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    loss = checkpoint['loss']\n    print(f\"Checkpoint loaded: Epoch {epoch}, Loss: {loss:.4f}\")\n    return model, optimizer, epoch, loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:14:56.480065Z","iopub.execute_input":"2025-01-02T09:14:56.480394Z","iopub.status.idle":"2025-01-02T09:14:56.484822Z","shell.execute_reply.started":"2025-01-02T09:14:56.480367Z","shell.execute_reply":"2025-01-02T09:14:56.483857Z"},"id":"XRItYXvQa7JN"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint_path = \"./checkpoints/model_epoch_20.pth\"\nmodel, optimizer, start_epoch, _ = load_checkpoint(checkpoint_path, model, optimizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:14:20.031344Z","iopub.execute_input":"2025-01-02T09:14:20.031652Z","iopub.status.idle":"2025-01-02T09:14:21.532472Z","shell.execute_reply.started":"2025-01-02T09:14:20.031628Z","shell.execute_reply":"2025-01-02T09:14:21.531698Z"},"id":"iJ7FRM_wa7JO","outputId":"9cca23dd-209b-4ba0-b713-602052e8deb7"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint_path = \"./checkpoints/model_epoch_10.pth\"\ncheckpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))  # Use GPU if available: 'cuda'\nmodel.load_state_dict(checkpoint['model_state_dict'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:13:55.653477Z","iopub.execute_input":"2025-01-02T09:13:55.653715Z","iopub.status.idle":"2025-01-02T09:13:55.676084Z","shell.execute_reply.started":"2025-01-02T09:13:55.653693Z","shell.execute_reply":"2025-01-02T09:13:55.674979Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"id":"tBk9w-l_a7JO","outputId":"046613ba-e750-46b5-8175-67d5f9e61095"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:14:26.196139Z","iopub.execute_input":"2025-01-02T09:14:26.19651Z","iopub.status.idle":"2025-01-02T09:14:26.201901Z","shell.execute_reply.started":"2025-01-02T09:14:26.196481Z","shell.execute_reply":"2025-01-02T09:14:26.200956Z"},"id":"b9Shapm3a7JP","outputId":"84970b93-34af-46a0-e4ab-d333fbfb7627"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_eval1 = df[449:450].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:15:21.71876Z","iopub.execute_input":"2025-01-02T09:15:21.719058Z","iopub.status.idle":"2025-01-02T09:15:21.723418Z","shell.execute_reply.started":"2025-01-02T09:15:21.719035Z","shell.execute_reply":"2025-01-02T09:15:21.722544Z"},"id":"QznZNXd2a7JP"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_dataset1 = CustomDataset(df_eval1, t5_tokenizer,gpt2_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:15:22.79416Z","iopub.execute_input":"2025-01-02T09:15:22.794471Z","iopub.status.idle":"2025-01-02T09:15:22.798035Z","shell.execute_reply.started":"2025-01-02T09:15:22.794449Z","shell.execute_reply":"2025-01-02T09:15:22.79709Z"},"id":"JuJQTbuma7JQ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions, decoded_text = inference(model, gpt2_tokenizer, t5_tokenizer, eval_dataset1, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:15:44.516595Z","iopub.execute_input":"2025-01-02T09:15:44.516895Z","iopub.status.idle":"2025-01-02T09:15:44.999499Z","shell.execute_reply.started":"2025-01-02T09:15:44.516872Z","shell.execute_reply":"2025-01-02T09:15:44.998792Z"},"id":"450TkAAMa7JQ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Decoded Text:\", decoded_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:15:47.030509Z","iopub.execute_input":"2025-01-02T09:15:47.030805Z","iopub.status.idle":"2025-01-02T09:15:47.035393Z","shell.execute_reply.started":"2025-01-02T09:15:47.030783Z","shell.execute_reply":"2025-01-02T09:15:47.034606Z"},"id":"wOV8HjGSa7JQ","outputId":"18b10659-bb6a-4ab0-e8b7-460d63296e53"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"id":"YVP6ZYJXa7JQ"},"outputs":[],"execution_count":null}]}