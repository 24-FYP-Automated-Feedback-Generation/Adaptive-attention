{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9859425,"sourceType":"datasetVersion","datasetId":6050791},{"sourceId":10048507,"sourceType":"datasetVersion","datasetId":6190931},{"sourceId":10242967,"sourceType":"datasetVersion","datasetId":6334477}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:38:11.453344Z","iopub.execute_input":"2024-12-26T17:38:11.453704Z","iopub.status.idle":"2024-12-26T17:38:11.786183Z","shell.execute_reply.started":"2024-12-26T17:38:11.453675Z","shell.execute_reply":"2024-12-26T17:38:11.785369Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/feedback-dataset/modified_dataset.csv\n/kaggle/input/modified-dataset/modified_dataset.csv\n/kaggle/input/metacognitive-dataset/metacognitive-dataset.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install transformers torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:38:13.903907Z","iopub.execute_input":"2024-12-26T17:38:13.904306Z","iopub.status.idle":"2024-12-26T17:38:17.724691Z","shell.execute_reply.started":"2024-12-26T17:38:13.904283Z","shell.execute_reply":"2024-12-26T17:38:17.723552Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import AutoModel, AutoTokenizer, GPT2Model,GPT2Tokenizer,GPT2LMHeadModel\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration , T5Tokenizer , T5Model\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:38:17.725952Z","iopub.execute_input":"2024-12-26T17:38:17.726200Z","iopub.status.idle":"2024-12-26T17:38:20.678588Z","shell.execute_reply.started":"2024-12-26T17:38:17.726178Z","shell.execute_reply":"2024-12-26T17:38:20.677924Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:38:22.138104Z","iopub.execute_input":"2024-12-26T17:38:22.138608Z","iopub.status.idle":"2024-12-26T17:38:22.160715Z","shell.execute_reply.started":"2024-12-26T17:38:22.138579Z","shell.execute_reply":"2024-12-26T17:38:22.159723Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:38:24.198287Z","iopub.execute_input":"2024-12-26T17:38:24.198652Z","iopub.status.idle":"2024-12-26T17:38:24.204479Z","shell.execute_reply.started":"2024-12-26T17:38:24.198623Z","shell.execute_reply":"2024-12-26T17:38:24.203567Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"checkpoint = \"t5-base\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:38:25.838356Z","iopub.execute_input":"2024-12-26T17:38:25.838680Z","iopub.status.idle":"2024-12-26T17:38:25.842606Z","shell.execute_reply.started":"2024-12-26T17:38:25.838656Z","shell.execute_reply":"2024-12-26T17:38:25.841523Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"t5_tokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:38:27.598340Z","iopub.execute_input":"2024-12-26T17:38:27.598703Z","iopub.status.idle":"2024-12-26T17:38:28.159699Z","shell.execute_reply.started":"2024-12-26T17:38:27.598673Z","shell.execute_reply":"2024-12-26T17:38:28.158674Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"#set the max length to model's default present max length\nt5_tokenizer.model_max_length = t5_tokenizer.model_max_length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:38:30.378264Z","iopub.execute_input":"2024-12-26T17:38:30.378578Z","iopub.status.idle":"2024-12-26T17:38:30.382268Z","shell.execute_reply.started":"2024-12-26T17:38:30.378554Z","shell.execute_reply":"2024-12-26T17:38:30.381286Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:38:32.198233Z","iopub.execute_input":"2024-12-26T17:38:32.198588Z","iopub.status.idle":"2024-12-26T17:38:32.499126Z","shell.execute_reply.started":"2024-12-26T17:38:32.198557Z","shell.execute_reply":"2024-12-26T17:38:32.498186Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:38:34.523082Z","iopub.execute_input":"2024-12-26T17:38:34.523373Z","iopub.status.idle":"2024-12-26T17:38:34.526940Z","shell.execute_reply.started":"2024-12-26T17:38:34.523351Z","shell.execute_reply":"2024-12-26T17:38:34.525968Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"file_path = \"/kaggle/input/metacognitive-dataset/metacognitive-dataset.csv\"\ndf = pd.read_csv(file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:38:37.663232Z","iopub.execute_input":"2024-12-26T17:38:37.663573Z","iopub.status.idle":"2024-12-26T17:38:37.714599Z","shell.execute_reply.started":"2024-12-26T17:38:37.663543Z","shell.execute_reply":"2024-12-26T17:38:37.713721Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"file_path = \"/kaggle/input/modified-dataset/modified_dataset.csv\"\ndf1 = pd.read_csv(file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:38:44.489466Z","iopub.execute_input":"2024-12-26T17:38:44.489794Z","iopub.status.idle":"2024-12-26T17:38:44.544295Z","shell.execute_reply.started":"2024-12-26T17:38:44.489770Z","shell.execute_reply":"2024-12-26T17:38:44.543370Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"df1.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:38:46.398214Z","iopub.execute_input":"2024-12-26T17:38:46.398527Z","iopub.status.idle":"2024-12-26T17:38:46.404101Z","shell.execute_reply.started":"2024-12-26T17:38:46.398504Z","shell.execute_reply":"2024-12-26T17:38:46.403070Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Index(['description', 'student_code', 'feedback', 'metacognitive_feedback',\n       'metacognitive_profile'],\n      dtype='object')"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:38:48.718757Z","iopub.execute_input":"2024-12-26T17:38:48.719069Z","iopub.status.idle":"2024-12-26T17:38:48.738947Z","shell.execute_reply.started":"2024-12-26T17:38:48.719037Z","shell.execute_reply":"2024-12-26T17:38:48.738168Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                            question    difficulty  \\\n0  =====Problem Statement=====\\nGiven an integer,...  introductory   \n1  =====Problem Statement=====\\nYou are given a s...  introductory   \n2  =====Problem Statement=====\\nYou are given a s...  introductory   \n3  =====Problem Statement=====\\nYou are given a p...  introductory   \n4  =====Function Descriptions=====\\nsum\\n\\nThe su...  introductory   \n\n                                     prefer_solution  \\\n0  n = int(input().strip())\\nw = len(str(bin(n))[...   \n1  #!/usr/bin/env python3\\n\\ndef __starting_point...   \n2  A  = set(input().split())\\nn = int(input())\\nc...   \n3  for i in range(1,int(input())): #More than 2 l...   \n4  import numpy\\nn,m=list(map(int,input().split()...   \n\n                                       flaw_solution  random_col_1  \\\n0  w = len(str(bin(n))[2:])\\nn = int(input().stri...             2   \n1  #!/usr/bin/env python3\\n\\ndef __starting_point...             1   \n2  s  = set(input().split())\\nm = int(input())\\nc...             1   \n3  for idx in range(1, int(input())):\\n    print(...             2   \n4  Variable Renaming\\n\\n```\\nimport numpy as np\\n...             2   \n\n   random_col_2  random_col_3  random_col_4  random_col_5  random_col_6  ...  \\\n0             2             1             2             2             2  ...   \n1             3             1             2             2             2  ...   \n2             1             2             3             3             3  ...   \n3             1             1             3             1             3  ...   \n4             2             3             2             2             1  ...   \n\n   random_col_9  random_col_10  random_col_11  random_col_12  random_col_13  \\\n0             1              1              1              1              3   \n1             2              3              1              3              3   \n2             1              3              1              1              1   \n3             1              3              1              1              3   \n4             1              2              3              3              3   \n\n   random_col_14  random_col_15  random_col_16  \\\n0              1              2              2   \n1              1              2              1   \n2              2              1              1   \n3              1              3              2   \n4              1              3              2   \n\n                               metacognitive_vector  \\\n0  [2, 2, 1, 2, 2, 2, 3, 2, 1, 1, 1, 1, 3, 1, 2, 2]   \n1  [1, 3, 1, 2, 2, 2, 3, 2, 2, 3, 1, 3, 3, 1, 2, 1]   \n2  [1, 1, 2, 3, 3, 3, 2, 2, 1, 3, 1, 1, 1, 2, 1, 1]   \n3  [2, 1, 1, 3, 1, 3, 1, 2, 1, 3, 1, 1, 3, 1, 3, 2]   \n4  [2, 2, 3, 2, 2, 1, 1, 3, 1, 2, 3, 3, 3, 1, 3, 2]   \n\n                              metacognitive_feedback  \n0  Your solution is very close to being correct, ...  \n1  It's clear that you have a good understanding ...  \n2  Your approach to the problem shows a good unde...  \n3  Your approach to the problem shows a good unde...  \n4  It appears you are on the right track with you...  \n\n[5 rows x 22 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>difficulty</th>\n      <th>prefer_solution</th>\n      <th>flaw_solution</th>\n      <th>random_col_1</th>\n      <th>random_col_2</th>\n      <th>random_col_3</th>\n      <th>random_col_4</th>\n      <th>random_col_5</th>\n      <th>random_col_6</th>\n      <th>...</th>\n      <th>random_col_9</th>\n      <th>random_col_10</th>\n      <th>random_col_11</th>\n      <th>random_col_12</th>\n      <th>random_col_13</th>\n      <th>random_col_14</th>\n      <th>random_col_15</th>\n      <th>random_col_16</th>\n      <th>metacognitive_vector</th>\n      <th>metacognitive_feedback</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>=====Problem Statement=====\\nGiven an integer,...</td>\n      <td>introductory</td>\n      <td>n = int(input().strip())\\nw = len(str(bin(n))[...</td>\n      <td>w = len(str(bin(n))[2:])\\nn = int(input().stri...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>[2, 2, 1, 2, 2, 2, 3, 2, 1, 1, 1, 1, 3, 1, 2, 2]</td>\n      <td>Your solution is very close to being correct, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>=====Problem Statement=====\\nYou are given a s...</td>\n      <td>introductory</td>\n      <td>#!/usr/bin/env python3\\n\\ndef __starting_point...</td>\n      <td>#!/usr/bin/env python3\\n\\ndef __starting_point...</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>[1, 3, 1, 2, 2, 2, 3, 2, 2, 3, 1, 3, 3, 1, 2, 1]</td>\n      <td>It's clear that you have a good understanding ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>=====Problem Statement=====\\nYou are given a s...</td>\n      <td>introductory</td>\n      <td>A  = set(input().split())\\nn = int(input())\\nc...</td>\n      <td>s  = set(input().split())\\nm = int(input())\\nc...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>...</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[1, 1, 2, 3, 3, 3, 2, 2, 1, 3, 1, 1, 1, 2, 1, 1]</td>\n      <td>Your approach to the problem shows a good unde...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>=====Problem Statement=====\\nYou are given a p...</td>\n      <td>introductory</td>\n      <td>for i in range(1,int(input())): #More than 2 l...</td>\n      <td>for idx in range(1, int(input())):\\n    print(...</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>...</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>[2, 1, 1, 3, 1, 3, 1, 2, 1, 3, 1, 1, 3, 1, 3, 2]</td>\n      <td>Your approach to the problem shows a good unde...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>=====Function Descriptions=====\\nsum\\n\\nThe su...</td>\n      <td>introductory</td>\n      <td>import numpy\\nn,m=list(map(int,input().split()...</td>\n      <td>Variable Renaming\\n\\n```\\nimport numpy as np\\n...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>[2, 2, 3, 2, 2, 1, 1, 3, 1, 2, 3, 3, 3, 1, 3, 2]</td>\n      <td>It appears you are on the right track with you...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"df['metacognitive_vector'][0].type","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:38:53.008221Z","iopub.execute_input":"2024-12-26T17:38:53.008582Z","iopub.status.idle":"2024-12-26T17:38:53.106354Z","shell.execute_reply.started":"2024-12-26T17:38:53.008552Z","shell.execute_reply":"2024-12-26T17:38:53.105184Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-e5ba6c82ae05>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metacognitive_vector'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'type'"],"ename":"AttributeError","evalue":"'str' object has no attribute 'type'","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:38:54.808367Z","iopub.execute_input":"2024-12-26T17:38:54.808715Z","iopub.status.idle":"2024-12-26T17:38:54.814267Z","shell.execute_reply.started":"2024-12-26T17:38:54.808688Z","shell.execute_reply":"2024-12-26T17:38:54.813448Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Index(['question', 'difficulty', 'prefer_solution', 'flaw_solution',\n       'random_col_1', 'random_col_2', 'random_col_3', 'random_col_4',\n       'random_col_5', 'random_col_6', 'random_col_7', 'random_col_8',\n       'random_col_9', 'random_col_10', 'random_col_11', 'random_col_12',\n       'random_col_13', 'random_col_14', 'random_col_15', 'random_col_16',\n       'metacognitive_vector', 'metacognitive_feedback'],\n      dtype='object')"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"df = df.loc[:, ~df.columns.str.startswith('random_col_')]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:39:34.493764Z","iopub.execute_input":"2024-12-26T17:39:34.494054Z","iopub.status.idle":"2024-12-26T17:39:34.499093Z","shell.execute_reply.started":"2024-12-26T17:39:34.494032Z","shell.execute_reply":"2024-12-26T17:39:34.498185Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"df['combined_question_student'] = df['question'] + \" \" + df['flaw_solution']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:39:35.798186Z","iopub.execute_input":"2024-12-26T17:39:35.798537Z","iopub.status.idle":"2024-12-26T17:39:35.804384Z","shell.execute_reply.started":"2024-12-26T17:39:35.798509Z","shell.execute_reply":"2024-12-26T17:39:35.803582Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"df['combined_question_expected'] = df['question'] + \" \" + df['prefer_solution']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:39:37.718050Z","iopub.execute_input":"2024-12-26T17:39:37.718355Z","iopub.status.idle":"2024-12-26T17:39:37.723814Z","shell.execute_reply.started":"2024-12-26T17:39:37.718331Z","shell.execute_reply":"2024-12-26T17:39:37.722866Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:39:39.968396Z","iopub.execute_input":"2024-12-26T17:39:39.968733Z","iopub.status.idle":"2024-12-26T17:39:39.974041Z","shell.execute_reply.started":"2024-12-26T17:39:39.968709Z","shell.execute_reply":"2024-12-26T17:39:39.973137Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Index(['question', 'difficulty', 'prefer_solution', 'flaw_solution',\n       'metacognitive_vector', 'metacognitive_feedback',\n       'combined_question_student', 'combined_question_expected'],\n      dtype='object')"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"df['metacognitive_feedback'][100]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:39:41.128643Z","iopub.execute_input":"2024-12-26T17:39:41.128956Z","iopub.status.idle":"2024-12-26T17:39:41.134473Z","shell.execute_reply.started":"2024-12-26T17:39:41.128929Z","shell.execute_reply":"2024-12-26T17:39:41.133782Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"\"It seems you're on the right track with your approach to solving the problem, but there are a few areas where you can improve both your coding skills and your metacognitive strategies. Firstly, ensure you read the problem statement thoroughly and identify all key requirements, inputs, outputs, and constraints. This will help you avoid minor mistakes, such as using incorrect variable names (e.g., using 'm' instead of 'n, l'). Breaking down the problem into smaller, manageable sub-goals can also help. For instance, start by understanding the input format, then focus on sorting the strings, and finally concatenate them. Before coding, try to sketch out your algorithm or plan your solution. This will help you systematically execute the designed algorithm and verify that you're on the correct path to the solution. Pay close attention during the implementation process to avoid negligent mistakes, such as missing colons or incorrect variable assignments. Monitor your problem-solving steps to verify intermediate results, ensuring each part of your code works correctly before moving on to the next. Lastly, always refer back to the problem statement to check if your implemented solution meets all the given requirements. Reflecting on similar problems you've solved earlier can also enhance your accuracy and efficiency. Keep up the good work on frequently revising your algorithm and being vigilant during implementation—these habits will significantly improve your problem-solving skills.\""},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:39:43.698529Z","iopub.execute_input":"2024-12-26T17:39:43.698866Z","iopub.status.idle":"2024-12-26T17:39:43.709396Z","shell.execute_reply.started":"2024-12-26T17:39:43.698838Z","shell.execute_reply":"2024-12-26T17:39:43.708230Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"                                            question    difficulty  \\\n0  =====Problem Statement=====\\nGiven an integer,...  introductory   \n1  =====Problem Statement=====\\nYou are given a s...  introductory   \n2  =====Problem Statement=====\\nYou are given a s...  introductory   \n3  =====Problem Statement=====\\nYou are given a p...  introductory   \n4  =====Function Descriptions=====\\nsum\\n\\nThe su...  introductory   \n\n                                     prefer_solution  \\\n0  n = int(input().strip())\\nw = len(str(bin(n))[...   \n1  #!/usr/bin/env python3\\n\\ndef __starting_point...   \n2  A  = set(input().split())\\nn = int(input())\\nc...   \n3  for i in range(1,int(input())): #More than 2 l...   \n4  import numpy\\nn,m=list(map(int,input().split()...   \n\n                                       flaw_solution  \\\n0  w = len(str(bin(n))[2:])\\nn = int(input().stri...   \n1  #!/usr/bin/env python3\\n\\ndef __starting_point...   \n2  s  = set(input().split())\\nm = int(input())\\nc...   \n3  for idx in range(1, int(input())):\\n    print(...   \n4  Variable Renaming\\n\\n```\\nimport numpy as np\\n...   \n\n                               metacognitive_vector  \\\n0  [2, 2, 1, 2, 2, 2, 3, 2, 1, 1, 1, 1, 3, 1, 2, 2]   \n1  [1, 3, 1, 2, 2, 2, 3, 2, 2, 3, 1, 3, 3, 1, 2, 1]   \n2  [1, 1, 2, 3, 3, 3, 2, 2, 1, 3, 1, 1, 1, 2, 1, 1]   \n3  [2, 1, 1, 3, 1, 3, 1, 2, 1, 3, 1, 1, 3, 1, 3, 2]   \n4  [2, 2, 3, 2, 2, 1, 1, 3, 1, 2, 3, 3, 3, 1, 3, 2]   \n\n                              metacognitive_feedback  \\\n0  Your solution is very close to being correct, ...   \n1  It's clear that you have a good understanding ...   \n2  Your approach to the problem shows a good unde...   \n3  Your approach to the problem shows a good unde...   \n4  It appears you are on the right track with you...   \n\n                           combined_question_student  \\\n0  =====Problem Statement=====\\nGiven an integer,...   \n1  =====Problem Statement=====\\nYou are given a s...   \n2  =====Problem Statement=====\\nYou are given a s...   \n3  =====Problem Statement=====\\nYou are given a p...   \n4  =====Function Descriptions=====\\nsum\\n\\nThe su...   \n\n                          combined_question_expected  \n0  =====Problem Statement=====\\nGiven an integer,...  \n1  =====Problem Statement=====\\nYou are given a s...  \n2  =====Problem Statement=====\\nYou are given a s...  \n3  =====Problem Statement=====\\nYou are given a p...  \n4  =====Function Descriptions=====\\nsum\\n\\nThe su...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>difficulty</th>\n      <th>prefer_solution</th>\n      <th>flaw_solution</th>\n      <th>metacognitive_vector</th>\n      <th>metacognitive_feedback</th>\n      <th>combined_question_student</th>\n      <th>combined_question_expected</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>=====Problem Statement=====\\nGiven an integer,...</td>\n      <td>introductory</td>\n      <td>n = int(input().strip())\\nw = len(str(bin(n))[...</td>\n      <td>w = len(str(bin(n))[2:])\\nn = int(input().stri...</td>\n      <td>[2, 2, 1, 2, 2, 2, 3, 2, 1, 1, 1, 1, 3, 1, 2, 2]</td>\n      <td>Your solution is very close to being correct, ...</td>\n      <td>=====Problem Statement=====\\nGiven an integer,...</td>\n      <td>=====Problem Statement=====\\nGiven an integer,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>=====Problem Statement=====\\nYou are given a s...</td>\n      <td>introductory</td>\n      <td>#!/usr/bin/env python3\\n\\ndef __starting_point...</td>\n      <td>#!/usr/bin/env python3\\n\\ndef __starting_point...</td>\n      <td>[1, 3, 1, 2, 2, 2, 3, 2, 2, 3, 1, 3, 3, 1, 2, 1]</td>\n      <td>It's clear that you have a good understanding ...</td>\n      <td>=====Problem Statement=====\\nYou are given a s...</td>\n      <td>=====Problem Statement=====\\nYou are given a s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>=====Problem Statement=====\\nYou are given a s...</td>\n      <td>introductory</td>\n      <td>A  = set(input().split())\\nn = int(input())\\nc...</td>\n      <td>s  = set(input().split())\\nm = int(input())\\nc...</td>\n      <td>[1, 1, 2, 3, 3, 3, 2, 2, 1, 3, 1, 1, 1, 2, 1, 1]</td>\n      <td>Your approach to the problem shows a good unde...</td>\n      <td>=====Problem Statement=====\\nYou are given a s...</td>\n      <td>=====Problem Statement=====\\nYou are given a s...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>=====Problem Statement=====\\nYou are given a p...</td>\n      <td>introductory</td>\n      <td>for i in range(1,int(input())): #More than 2 l...</td>\n      <td>for idx in range(1, int(input())):\\n    print(...</td>\n      <td>[2, 1, 1, 3, 1, 3, 1, 2, 1, 3, 1, 1, 3, 1, 3, 2]</td>\n      <td>Your approach to the problem shows a good unde...</td>\n      <td>=====Problem Statement=====\\nYou are given a p...</td>\n      <td>=====Problem Statement=====\\nYou are given a p...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>=====Function Descriptions=====\\nsum\\n\\nThe su...</td>\n      <td>introductory</td>\n      <td>import numpy\\nn,m=list(map(int,input().split()...</td>\n      <td>Variable Renaming\\n\\n```\\nimport numpy as np\\n...</td>\n      <td>[2, 2, 3, 2, 2, 1, 1, 3, 1, 2, 3, 3, 3, 1, 3, 2]</td>\n      <td>It appears you are on the right track with you...</td>\n      <td>=====Function Descriptions=====\\nsum\\n\\nThe su...</td>\n      <td>=====Function Descriptions=====\\nsum\\n\\nThe su...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport ast\nclass CustomDataset(Dataset):\n    def __init__(self, dataset, t5_tokenizer,gpt2_tokenizer, max_length=512):\n        self.t5_tokenizer = t5_tokenizer\n        self.gpt2_tokenizer = gpt2_tokenizer\n        self.data = dataset\n        self.max_length = max_length\n\n        # self.data = data.reset_index(drop=True)\n\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        metacognition_vector = self.data['metacognitive_vector'][idx]\n        problem_student_code = self.data['combined_question_student'][idx]\n        problem_expected_code = self.data['combined_question_expected'][idx]\n        student_code = self.data['flaw_solution'][idx]\n        target = self.data['metacognitive_feedback'][idx]\n\n        # Tokenize inputs and truncate/pad\n        metacognition_vector_ids = torch.tensor(\n            ast.literal_eval(metacognition_vector), dtype=torch.float\n        )\n        problem_student_code_ids = torch.tensor(\n            self.gpt2_tokenizer.encode(problem_student_code, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n        problem_expected_code_ids = torch.tensor(\n            self.t5_tokenizer.encode(problem_expected_code, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n        # feedback_encoded_ids = torch.tensor(\n        #     self.gpt2_tokenizer.encode(feedback, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        # )\n        \n        student_code_ids = torch.tensor(\n            self.t5_tokenizer.encode(student_code, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n        target_ids = torch.tensor(\n            self.gpt2_tokenizer.encode(target, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n\n        return metacognition_vector_ids, problem_student_code_ids, problem_expected_code_ids, student_code_ids, target_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:39:46.088437Z","iopub.execute_input":"2024-12-26T17:39:46.088724Z","iopub.status.idle":"2024-12-26T17:39:46.095428Z","shell.execute_reply.started":"2024-12-26T17:39:46.088703Z","shell.execute_reply":"2024-12-26T17:39:46.094479Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"dataset = CustomDataset(df, t5_tokenizer, gpt2_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:39:50.638788Z","iopub.execute_input":"2024-12-26T17:39:50.639270Z","iopub.status.idle":"2024-12-26T17:39:50.644469Z","shell.execute_reply.started":"2024-12-26T17:39:50.639222Z","shell.execute_reply":"2024-12-26T17:39:50.643516Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"dataset1 = CustomDataset(df1, t5_tokenizer, gpt2_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:39:51.977887Z","iopub.execute_input":"2024-12-26T17:39:51.978169Z","iopub.status.idle":"2024-12-26T17:39:51.982205Z","shell.execute_reply.started":"2024-12-26T17:39:51.978147Z","shell.execute_reply":"2024-12-26T17:39:51.981214Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"len(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:39:53.768328Z","iopub.execute_input":"2024-12-26T17:39:53.768648Z","iopub.status.idle":"2024-12-26T17:39:53.773532Z","shell.execute_reply.started":"2024-12-26T17:39:53.768625Z","shell.execute_reply":"2024-12-26T17:39:53.772659Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"500"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"metacognition_vector_ids, problem_student_code_ids, problem_expected_code_ids, student_code_ids, target_ids = dataset[100]\n#print(f\"Sample {10}:\")\nprint(f\"Metacognition vector IDs: {metacognition_vector_ids}\")\n#print(f\"Problem IDs: {description_ids}\")\nprint(f\"Expected feedback IDs: {problem_student_code_ids.shape}\")\nprint(f\"Expected encoded feedback IDs: {problem_expected_code_ids.shape}\")\n#print(f\"Student Answer IDs: {student_code_ids}\")\nprint(f\"Target IDs: {target_ids.shape}\")\nprint(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:39:55.378511Z","iopub.execute_input":"2024-12-26T17:39:55.378808Z","iopub.status.idle":"2024-12-26T17:39:55.402110Z","shell.execute_reply.started":"2024-12-26T17:39:55.378788Z","shell.execute_reply":"2024-12-26T17:39:55.401319Z"}},"outputs":[{"name":"stdout","text":"Metacognition vector IDs: tensor([1., 2., 1., 2., 2., 2., 3., 1., 2., 2., 2., 3., 1., 2., 3., 2.])\nExpected feedback IDs: torch.Size([512])\nExpected encoded feedback IDs: torch.Size([512])\nTarget IDs: torch.Size([512])\n\n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# model = T5ForConditionalGeneration.from_pretrained(checkpoint)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# save_directory = \"./t5_checkpoint\"\n# tokenizer.save_pretrained(save_directory)\n# model.save_pretrained(save_directory)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gpt2_tokenizer.pad_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:39:59.008127Z","iopub.execute_input":"2024-12-26T17:39:59.008493Z","iopub.status.idle":"2024-12-26T17:39:59.013775Z","shell.execute_reply.started":"2024-12-26T17:39:59.008461Z","shell.execute_reply":"2024-12-26T17:39:59.012741Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"'<|endoftext|>'"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"gpt2_pad_token_id = gpt2_tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:40:00.298351Z","iopub.execute_input":"2024-12-26T17:40:00.298665Z","iopub.status.idle":"2024-12-26T17:40:00.302609Z","shell.execute_reply.started":"2024-12-26T17:40:00.298643Z","shell.execute_reply":"2024-12-26T17:40:00.301608Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"gpt2_pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:40:02.888167Z","iopub.execute_input":"2024-12-26T17:40:02.888516Z","iopub.status.idle":"2024-12-26T17:40:02.893373Z","shell.execute_reply.started":"2024-12-26T17:40:02.888485Z","shell.execute_reply":"2024-12-26T17:40:02.892549Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"50256"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"t5_tokenizer.pad_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:40:04.758646Z","iopub.execute_input":"2024-12-26T17:40:04.758929Z","iopub.status.idle":"2024-12-26T17:40:04.763689Z","shell.execute_reply.started":"2024-12-26T17:40:04.758907Z","shell.execute_reply":"2024-12-26T17:40:04.762923Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"'<pad>'"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"t5_pad_token_id = t5_tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:40:06.578646Z","iopub.execute_input":"2024-12-26T17:40:06.578956Z","iopub.status.idle":"2024-12-26T17:40:06.582677Z","shell.execute_reply.started":"2024-12-26T17:40:06.578930Z","shell.execute_reply":"2024-12-26T17:40:06.581625Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"t5_pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:40:08.358351Z","iopub.execute_input":"2024-12-26T17:40:08.358709Z","iopub.status.idle":"2024-12-26T17:40:08.363870Z","shell.execute_reply.started":"2024-12-26T17:40:08.358678Z","shell.execute_reply":"2024-12-26T17:40:08.362939Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":33},{"cell_type":"markdown","source":"* context  = problem + expected answer\n* current state = problem + student code\n* persona = metacognitive vector mapped into linear representaion\nor else for persona:\n* persona  = metacognitive vector + prompt of metacognition profile defined","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Context Enocder","metadata":{}},{"cell_type":"code","source":"class ContextEncoder(nn.Module): \n    def __init__(self, t5_model_name='t5-base', output_dim=768): \n        super(ContextEncoder, self).__init__()\n        \n        self.t5_encoder = T5Model.from_pretrained(t5_model_name).encoder\n        self.fc = nn.Linear(self.t5_encoder.config.d_model, output_dim)\n    \n    def forward(self, problem_student_code_ids, attention_masks=None):        \n\n        encoder_outputs = self.t5_encoder(\n            input_ids=problem_student_code_ids,\n            attention_mask=attention_masks\n        )\n        context_hidden_states = encoder_outputs.last_hidden_state   \n        \n        context_rep = context_hidden_states.mean(dim=1)\n        \n       \n        context_rep = self.fc(context_rep)\n        final_rep = context_rep.unsqueeze(1)\n        \n        return final_rep","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:40:10.943692Z","iopub.execute_input":"2024-12-26T17:40:10.944093Z","iopub.status.idle":"2024-12-26T17:40:10.950844Z","shell.execute_reply.started":"2024-12-26T17:40:10.944059Z","shell.execute_reply":"2024-12-26T17:40:10.949798Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"context_encoder = ContextEncoder().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:40:13.438513Z","iopub.execute_input":"2024-12-26T17:40:13.438835Z","iopub.status.idle":"2024-12-26T17:40:14.275225Z","shell.execute_reply.started":"2024-12-26T17:40:13.438814Z","shell.execute_reply":"2024-12-26T17:40:14.274560Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"context_encoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:40:15.933333Z","iopub.execute_input":"2024-12-26T17:40:15.933675Z","iopub.status.idle":"2024-12-26T17:40:15.939786Z","shell.execute_reply.started":"2024-12-26T17:40:15.933650Z","shell.execute_reply":"2024-12-26T17:40:15.939011Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"ContextEncoder(\n  (t5_encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (fc): Linear(in_features=768, out_features=768, bias=True)\n)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"class PersonaEncoder(nn.Module): \n    def __init__(self, t5_model_name='t5-base', output_dim=768): \n        super(PersonaEncoder, self).__init__()\n        \n        self.t5_encoder = T5Model.from_pretrained(t5_model_name).encoder\n        self.fc = nn.Linear(self.t5_encoder.config.d_model, output_dim)\n    \n    def forward(self, problem_student_code_ids, metacognitive_vector_ids, attention_masks=None):        \n        \n        combined_inputs = torch.cat([problem_student_code_ids, metacognitive_vector_ids], dim=1)\n        print(\"combine input\", combined_inputs.shape)\n        \n        # if attention_masks is not None:\n        #     combined_masks = torch.cat(attention_masks, dim=1)\n        # else:\n        #     combined_masks = None\n\n        encoder_outputs = self.t5_encoder(\n            input_ids=combined_inputs,\n            attention_mask=attention_masks\n        )\n        context_hidden_states = encoder_outputs.last_hidden_state   \n        \n        context_rep = context_hidden_states.mean(dim=1)\n        \n       \n        context_rep = self.fc(context_rep)\n        final_rep = context_rep.unsqueeze(1)\n        \n        return final_rep","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:40:19.608570Z","iopub.execute_input":"2024-12-26T17:40:19.608858Z","iopub.status.idle":"2024-12-26T17:40:19.614477Z","shell.execute_reply.started":"2024-12-26T17:40:19.608838Z","shell.execute_reply":"2024-12-26T17:40:19.613466Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"persona_enocder = PersonaEnocder()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MetacognitionLayer(nn.Module):\n    def __init__(self, metacognitive_dim=16, output_dim=768):\n        super(MetacognitionLayer, self).__init__()\n        #16 to 768 mapping\n        self.metacognitive_fc = nn.Linear(metacognitive_dim, output_dim) \n        self.final_fc = nn.Linear(output_dim, output_dim)\n\n    def forward(self, metacognitive_vector):\n        \n\n        metacognitive_rep = self.metacognitive_fc(metacognitive_vector)\n        final_rep = self.final_fc(metacognitive_rep)  \n        persona_rep = final_rep.unsqueeze(1)\n\n        return persona_rep","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:40:23.528586Z","iopub.execute_input":"2024-12-26T17:40:23.528881Z","iopub.status.idle":"2024-12-26T17:40:23.533980Z","shell.execute_reply.started":"2024-12-26T17:40:23.528859Z","shell.execute_reply":"2024-12-26T17:40:23.532936Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"metacognitive_emb = MetacognitionLayer().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:40:25.938441Z","iopub.execute_input":"2024-12-26T17:40:25.938766Z","iopub.status.idle":"2024-12-26T17:40:25.948140Z","shell.execute_reply.started":"2024-12-26T17:40:25.938739Z","shell.execute_reply":"2024-12-26T17:40:25.947384Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"metacognitive_emb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:40:28.168650Z","iopub.execute_input":"2024-12-26T17:40:28.168945Z","iopub.status.idle":"2024-12-26T17:40:28.174287Z","shell.execute_reply.started":"2024-12-26T17:40:28.168923Z","shell.execute_reply":"2024-12-26T17:40:28.173470Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"MetacognitionLayer(\n  (metacognitive_fc): Linear(in_features=16, out_features=768, bias=True)\n  (final_fc): Linear(in_features=768, out_features=768, bias=True)\n)"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"class PAALayer(nn.Module):\n    def __init__(self, hidden_dimension = 768 , tau=0.8,dropout_rate=0.1):\n        super(PAALayer, self).__init__()\n        self.hidden_dimenstion = hidden_dimension\n        self.tau = tau\n\n       \n        self.fc = nn.Linear(2 * hidden_dimension, hidden_dimension)  \n        self.sigmoid = nn.Sigmoid()\n        self.fc_out = nn.Linear(hidden_dimension, hidden_dimension)\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n\n    def forward(self, hR , oP, oC):\n       \n        Mp_input  = torch.cat([hR,oP], dim=-1)        \n        Mp = self.fc(Mp_input)      \n        Wp = self.sigmoid(Mp)      \n     \n        Mpersona = Wp\n        Mcontext = 1 - Wp      \n       \n        oP_weighted = Mpersona * oP       \n        oC_weighted = Mcontext * oC\n       \n        HPAA = oP_weighted + oC_weighted \n       \n        output = self.fc_out(HPAA)\n       \n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:40:30.488808Z","iopub.execute_input":"2024-12-26T17:40:30.489137Z","iopub.status.idle":"2024-12-26T17:40:30.495273Z","shell.execute_reply.started":"2024-12-26T17:40:30.489108Z","shell.execute_reply":"2024-12-26T17:40:30.494428Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"paa = PAALayer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:40:32.038189Z","iopub.execute_input":"2024-12-26T17:40:32.038538Z","iopub.status.idle":"2024-12-26T17:40:32.059101Z","shell.execute_reply.started":"2024-12-26T17:40:32.038508Z","shell.execute_reply":"2024-12-26T17:40:32.058435Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"paa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:40:36.538103Z","iopub.execute_input":"2024-12-26T17:40:36.538467Z","iopub.status.idle":"2024-12-26T17:40:36.543741Z","shell.execute_reply.started":"2024-12-26T17:40:36.538425Z","shell.execute_reply":"2024-12-26T17:40:36.542863Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"PAALayer(\n  (fc): Linear(in_features=1536, out_features=768, bias=True)\n  (sigmoid): Sigmoid()\n  (fc_out): Linear(in_features=768, out_features=768, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"class CustomTransformerBlock(nn.Module):\n    def __init__(self, hidden_size, tau , dropout_rate=0.1):\n        super(CustomTransformerBlock, self).__init__()\n\n        self.self_attention = nn.MultiheadAttention(hidden_size, num_heads=12, batch_first=True)\n\n\n        self.context_attn = nn.MultiheadAttention(hidden_size, num_heads=12, batch_first=True)\n        self.persona_attn = nn.MultiheadAttention(hidden_size, num_heads=12, batch_first=True)\n\n        self.paa_layer = PAALayer(hidden_dimension=hidden_size, tau=tau)\n        \n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_size, 2048),\n            nn.ReLU(),\n            nn.Dropout(p=0.1),\n            nn.Linear(2048, hidden_size),\n            nn.LayerNorm(hidden_size)\n        )\n        self.layer_norm2 = nn.LayerNorm(hidden_size)\n       \n    def forward(self, inputs_embeds, encoded_persona, encoded_context):\n\n        hR, _ = self.self_attention(inputs_embeds, inputs_embeds, inputs_embeds)\n\n        oP, _ = self.persona_attn(hR, encoded_persona, encoded_persona)\n        oC, _ = self.context_attn(hR, encoded_context, encoded_context)      \n        \n        HPAA = self.paa_layer(hR, oP, oC)\n        \n        mlp_output = self.mlp(HPAA)\n        output = self.layer_norm2(mlp_output + HPAA)\n      \n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:40:38.613610Z","iopub.execute_input":"2024-12-26T17:40:38.613923Z","iopub.status.idle":"2024-12-26T17:40:38.620238Z","shell.execute_reply.started":"2024-12-26T17:40:38.613901Z","shell.execute_reply":"2024-12-26T17:40:38.619320Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"custom_layer = CustomTransformerBlock(768,0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:40:40.808372Z","iopub.execute_input":"2024-12-26T17:40:40.808787Z","iopub.status.idle":"2024-12-26T17:40:40.920530Z","shell.execute_reply.started":"2024-12-26T17:40:40.808753Z","shell.execute_reply":"2024-12-26T17:40:40.919602Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"custom_layer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:40:43.647863Z","iopub.execute_input":"2024-12-26T17:40:43.648191Z","iopub.status.idle":"2024-12-26T17:40:43.653360Z","shell.execute_reply.started":"2024-12-26T17:40:43.648160Z","shell.execute_reply":"2024-12-26T17:40:43.652487Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"CustomTransformerBlock(\n  (self_attention): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (context_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (persona_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (paa_layer): PAALayer(\n    (fc): Linear(in_features=1536, out_features=768, bias=True)\n    (sigmoid): Sigmoid()\n    (fc_out): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (mlp): Sequential(\n    (0): Linear(in_features=768, out_features=2048, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.1, inplace=False)\n    (3): Linear(in_features=2048, out_features=768, bias=True)\n    (4): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n)"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"class PAAModel(nn.Module):\n    def __init__(self, hidden_size=768, vocab_size = 50257 ,tau=0.8, max_length=512, num_transformer_blocks=6):\n        super(PAAModel , self).__init__()\n        self.hidden_size = hidden_size\n        self.tau = tau\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n        self.num_transformer_blocks = num_transformer_blocks\n\n        self.context_encoder = ContextEncoder()\n        \n        self.metacognitive_emb = MetacognitionLayer()\n        \n        for param in self.context_encoder.parameters():\n            param.requires_grad = False\n\n        for param in self.metacognitive_emb.parameters():\n            param.requires_grad = False\n\n        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n        self.position_embedding = nn.Embedding(max_length, hidden_size)\n        self.dropout = nn.Dropout(p=0.1) \n\n        self.self_attention = nn.MultiheadAttention(hidden_size, num_heads=12, batch_first=True)        \n        self.transformer_blocks = nn.ModuleList([CustomTransformerBlock(hidden_size, tau) for _ in range(num_transformer_blocks)])\n        self.final_fc = nn.Linear(hidden_size, vocab_size)\n        self.layer_norm = nn.LayerNorm(hidden_size)\n\n    def forward(self,  metacognitive_vector_ids,\n                       problem_student_code_ids ,\n                       problem_expected_code_ids ,\n                       context_attention_mask,\n                       student_attention_mask):\n\n        metacognitive_vector_emb = self.metacognitive_emb(metacognitive_vector_ids)\n\n        context_encoding = self.context_encoder(problem_expected_code_ids,\n                                                attention_masks=context_attention_mask)\n        \n        seq_length = problem_student_code_ids.size(1)\n        token_embeds = self.token_embedding(problem_student_code_ids)\n        position_ids = torch.arange(0, seq_length, device=problem_student_code_ids.device).unsqueeze(0)\n        position_embeds = self.position_embedding(position_ids)\n        \n        # Combine embeddings\n        inputs_embeds = token_embeds + position_embeds\n        inputs_embeds = self.dropout(inputs_embeds)\n        \n     \n        # outputs = self.gpt2_model(inputs_embeds=inputs_embeds, attention_mask=student_attention_mask, output_hidden_states=True)\n        # hR = (outputs.hidden_states)[-1]\n        # # print(\"hR shape\",hR.shape)\n      \n        transformer_input = inputs_embeds\n        for transformer_block in self.transformer_blocks:\n            transformer_output = transformer_block(transformer_input, metacognitive_vector_emb, context_encoding)           \n       \n        logits = self.final_fc(transformer_output)    \n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:33:52.286816Z","iopub.status.idle":"2024-12-26T19:33:52.287123Z","shell.execute_reply":"2024-12-26T19:33:52.287008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = PAAModel()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:33:52.288060Z","iopub.status.idle":"2024-12-26T19:33:52.288434Z","shell.execute_reply":"2024-12-26T19:33:52.288302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:33:52.289339Z","iopub.status.idle":"2024-12-26T19:33:52.289605Z","shell.execute_reply":"2024-12-26T19:33:52.289502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:40:56.758152Z","iopub.execute_input":"2024-12-26T17:40:56.758522Z","iopub.status.idle":"2024-12-26T17:40:56.762328Z","shell.execute_reply.started":"2024-12-26T17:40:56.758493Z","shell.execute_reply":"2024-12-26T17:40:56.761371Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"import torch.optim as optim\noptimizer = optim.AdamW(model.parameters(), lr=5e-5)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:40:58.097935Z","iopub.execute_input":"2024-12-26T17:40:58.098217Z","iopub.status.idle":"2024-12-26T17:40:58.582772Z","shell.execute_reply.started":"2024-12-26T17:40:58.098195Z","shell.execute_reply":"2024-12-26T17:40:58.582006Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"LOSS = torch.nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:41:00.918153Z","iopub.execute_input":"2024-12-26T17:41:00.918639Z","iopub.status.idle":"2024-12-26T17:41:00.922529Z","shell.execute_reply.started":"2024-12-26T17:41:00.918615Z","shell.execute_reply":"2024-12-26T17:41:00.921561Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"num_epochs = 200","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:41:02.838082Z","iopub.execute_input":"2024-12-26T17:41:02.838391Z","iopub.status.idle":"2024-12-26T17:41:02.842218Z","shell.execute_reply.started":"2024-12-26T17:41:02.838366Z","shell.execute_reply":"2024-12-26T17:41:02.841190Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"df_train = df[0:400]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:41:04.808219Z","iopub.execute_input":"2024-12-26T17:41:04.808559Z","iopub.status.idle":"2024-12-26T17:41:04.812652Z","shell.execute_reply.started":"2024-12-26T17:41:04.808531Z","shell.execute_reply":"2024-12-26T17:41:04.811765Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:41:08.048567Z","iopub.execute_input":"2024-12-26T17:41:08.048866Z","iopub.status.idle":"2024-12-26T17:41:08.054031Z","shell.execute_reply.started":"2024-12-26T17:41:08.048844Z","shell.execute_reply":"2024-12-26T17:41:08.053131Z"}},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"500"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ntrain_dataset = CustomDataset(df_train, t5_tokenizer , gpt2_tokenizer)\ntrain_dataloader = DataLoader(train_dataset , batch_size = 1 ,shuffle = True )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:41:09.848168Z","iopub.execute_input":"2024-12-26T17:41:09.848516Z","iopub.status.idle":"2024-12-26T17:41:09.853224Z","shell.execute_reply.started":"2024-12-26T17:41:09.848486Z","shell.execute_reply":"2024-12-26T17:41:09.852005Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:41:11.938191Z","iopub.execute_input":"2024-12-26T17:41:11.938527Z","iopub.status.idle":"2024-12-26T17:41:14.447261Z","shell.execute_reply.started":"2024-12-26T17:41:11.938501Z","shell.execute_reply":"2024-12-26T17:41:14.446294Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"checkpoint_dir = \"./checkpoints\"\nos.makedirs(checkpoint_dir, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:41:16.918556Z","iopub.execute_input":"2024-12-26T17:41:16.919162Z","iopub.status.idle":"2024-12-26T17:41:16.923282Z","shell.execute_reply.started":"2024-12-26T17:41:16.919122Z","shell.execute_reply":"2024-12-26T17:41:16.922430Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    print(f\"Training started for epoch {epoch + 1}/{num_epochs}\")\n    model.train()\n    total_loss = 0\n\n    for idx, (metacognition_vector_ids,\n              problem_student_code_ids,\n              problem_expected_code_ids,\n              student_code_ids,\n              target_ids) in enumerate(train_dataloader):\n        \n        metacognition_vector_ids = metacognition_vector_ids.to(device)\n        problem_student_code_ids = problem_student_code_ids.to(device)\n        problem_expected_code_ids = problem_expected_code_ids.to(device)\n        student_code_ids = student_code_ids.to(device)\n        target_ids = target_ids.to(device)\n        \n        #attention masking\n        student_attention_mask = (problem_student_code_ids != gpt2_pad_token_id).long().to(device)\n        context_attention_mask = (problem_expected_code_ids != t5_pad_token_id).long().to(device)\n        \n\n        optimizer.zero_grad()\n        logits = model(metacognition_vector_ids,\n                       problem_student_code_ids ,\n                       problem_expected_code_ids ,\n                       context_attention_mask,\n                       student_attention_mask)\n\n     \n        logits = logits.view(-1, logits.size(-1))\n        target_ids = target_ids.view(-1)\n\n       \n        loss = LOSS(logits, target_ids)\n        total_loss += loss.item()\n\n      \n        loss.backward()       \n        for name, param in model.named_parameters():\n            if 'context_encoder' in name or 'persona_encoder' in name:\n                assert param.grad is None, f\"Gradients found in frozen encoder {name}\"\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)       \n        optimizer.step()    \n        \n        if idx % 10 == 0:\n            print(f\"Batch {idx + 1}/{len(train_dataloader)} | Loss: {loss.item():.4f}\" , end='\\r')\n\n\n\n\n    \n                    \n    if epoch % 20 ==0 :\n            for name, param in model.named_parameters():\n                if param.requires_grad and param.grad is not None:\n                    print(f\"Layer: {name} | Grad Norm: {param.grad.norm().item()}\")\n                elif param.requires_grad:\n                    print(f\"Layer: {name} | Grad: None\")       \n    \n    if (epoch + 1) % 20 == 0:\n        checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch + 1}.pth\")\n        torch.save({\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': total_loss / max(len(train_dataloader), 1),\n        }, checkpoint_path)\n        print(f\"Checkpoint saved at {checkpoint_path}\")\n\n   \n    avg_loss = total_loss / max(len(train_dataloader), 1)\n    #writer.add_scalar(\"Loss/train\", avg_loss, epoch + 1)\n    print(f\"Epoch [{epoch + 1}/{num_epochs}] completed | Average Loss: {avg_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:25:52.571243Z","iopub.execute_input":"2024-12-26T19:25:52.571620Z","iopub.status.idle":"2024-12-26T19:33:52.285968Z","shell.execute_reply.started":"2024-12-26T19:25:52.571590Z","shell.execute_reply":"2024-12-26T19:33:52.284533Z"}},"outputs":[{"name":"stdout","text":"Training started for epoch 1/200\nLayer: token_embedding.weight | Grad Norm: 8.595009426404232e-11\nLayer: position_embedding.weight | Grad Norm: 2.1067354458170762e-11\nLayer: self_attention.in_proj_weight | Grad: None\nLayer: self_attention.in_proj_bias | Grad: None\nLayer: self_attention.out_proj.weight | Grad: None\nLayer: self_attention.out_proj.bias | Grad: None\nLayer: transformer_blocks.0.self_attention.in_proj_weight | Grad Norm: 3.422449434964392e-09\nLayer: transformer_blocks.0.self_attention.in_proj_bias | Grad Norm: 6.052214285290347e-10\nLayer: transformer_blocks.0.self_attention.out_proj.weight | Grad Norm: 4.0589096528265145e-09\nLayer: transformer_blocks.0.self_attention.out_proj.bias | Grad Norm: 1.0387882776186075e-09\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 1.5659276542123735e-08\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 9.279760071478904e-09\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 1.8311679284011007e-08\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 1.568943908125675e-08\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 2.1303014818840893e-07\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 9.09157726880494e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 2.579157296622725e-07\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 1.5540326359086976e-08\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 2.376561170081004e-08\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 2.518607544743645e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 2.3985077746147e-07\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 5.252869428318263e-08\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 2.5292121108577703e-07\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 9.111259657856863e-08\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 4.2111145148737705e-07\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 2.2198548776941607e-07\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 6.066832369810982e-09\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 6.2313270099423335e-09\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 6.12500672403371e-09\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 6.22695406349294e-09\nLayer: transformer_blocks.1.self_attention.in_proj_weight | Grad Norm: 2.449794749281864e-07\nLayer: transformer_blocks.1.self_attention.in_proj_bias | Grad Norm: 9.235160192133662e-09\nLayer: transformer_blocks.1.self_attention.out_proj.weight | Grad Norm: 3.0650534199594404e-07\nLayer: transformer_blocks.1.self_attention.out_proj.bias | Grad Norm: 1.5747177783964617e-08\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 2.2577421532332664e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 1.3382306462972338e-07\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 2.624038870635559e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 2.269123058340483e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 3.1921836125547998e-06\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 1.3627045802877547e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 3.7547233660006896e-06\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 2.2934611365599267e-07\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 5.977790920042025e-07\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 3.99879773738121e-08\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 3.745607727978495e-06\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 7.693007546549779e-07\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 3.7687680105591426e-06\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 1.2984309023522655e-06\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 6.665698492724914e-06\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 3.239318857595208e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 9.501948738943611e-08\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 1.001466287675612e-07\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 9.703884984446631e-08\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 1.0047644138921896e-07\nLayer: transformer_blocks.2.self_attention.in_proj_weight | Grad Norm: 3.789685479205218e-06\nLayer: transformer_blocks.2.self_attention.in_proj_bias | Grad Norm: 1.4261621572586591e-07\nLayer: transformer_blocks.2.self_attention.out_proj.weight | Grad Norm: 4.430747594597051e-06\nLayer: transformer_blocks.2.self_attention.out_proj.bias | Grad Norm: 2.4290332589771424e-07\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 3.2968898722174345e-06\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 1.956309006345691e-06\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 4.009683834738098e-06\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 3.3469279969722265e-06\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 4.5626016799360514e-05\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 1.948418002939434e-06\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 5.726948438677937e-05\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 3.357903551659547e-06\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 8.852762221067678e-06\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 6.139007382444106e-07\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 5.955325104878284e-05\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 1.1960945812461432e-05\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 6.231165025383234e-05\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 2.0716786821139976e-05\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.0001098250868381001\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 5.001368117518723e-05\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 1.5680051319577615e-06\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 1.637061814108165e-06\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 1.5798797221577843e-06\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 1.634155751162325e-06\nLayer: transformer_blocks.3.self_attention.in_proj_weight | Grad Norm: 6.370375922415406e-05\nLayer: transformer_blocks.3.self_attention.in_proj_bias | Grad Norm: 2.4005580598895904e-06\nLayer: transformer_blocks.3.self_attention.out_proj.weight | Grad Norm: 7.610686589032412e-05\nLayer: transformer_blocks.3.self_attention.out_proj.bias | Grad Norm: 4.095348685950739e-06\nLayer: transformer_blocks.3.context_attn.in_proj_weight | Grad Norm: 6.053602191968821e-05\nLayer: transformer_blocks.3.context_attn.in_proj_bias | Grad Norm: 3.594193185563199e-05\nLayer: transformer_blocks.3.context_attn.out_proj.weight | Grad Norm: 7.261288556037471e-05\nLayer: transformer_blocks.3.context_attn.out_proj.bias | Grad Norm: 6.298347580013797e-05\nLayer: transformer_blocks.3.persona_attn.in_proj_weight | Grad Norm: 0.0008493325440213084\nLayer: transformer_blocks.3.persona_attn.in_proj_bias | Grad Norm: 3.628105696407147e-05\nLayer: transformer_blocks.3.persona_attn.out_proj.weight | Grad Norm: 0.0010311342775821686\nLayer: transformer_blocks.3.persona_attn.out_proj.bias | Grad Norm: 6.290222518146038e-05\nLayer: transformer_blocks.3.paa_layer.fc.weight | Grad Norm: 0.0001447027752874419\nLayer: transformer_blocks.3.paa_layer.fc.bias | Grad Norm: 1.0187053703702986e-05\nLayer: transformer_blocks.3.paa_layer.fc_out.weight | Grad Norm: 0.0010119026992470026\nLayer: transformer_blocks.3.paa_layer.fc_out.bias | Grad Norm: 0.00021221983479335904\nLayer: transformer_blocks.3.mlp.0.weight | Grad Norm: 0.001056151231750846\nLayer: transformer_blocks.3.mlp.0.bias | Grad Norm: 0.0003682074893731624\nLayer: transformer_blocks.3.mlp.3.weight | Grad Norm: 0.0018760163802653551\nLayer: transformer_blocks.3.mlp.3.bias | Grad Norm: 0.000916940625756979\nLayer: transformer_blocks.3.mlp.4.weight | Grad Norm: 2.570562355685979e-05\nLayer: transformer_blocks.3.mlp.4.bias | Grad Norm: 2.8225475034560077e-05\nLayer: transformer_blocks.3.layer_norm2.weight | Grad Norm: 2.570676770119462e-05\nLayer: transformer_blocks.3.layer_norm2.bias | Grad Norm: 2.8254602511879057e-05\nLayer: transformer_blocks.4.self_attention.in_proj_weight | Grad Norm: 0.0010536211775615811\nLayer: transformer_blocks.4.self_attention.in_proj_bias | Grad Norm: 3.9674316212767735e-05\nLayer: transformer_blocks.4.self_attention.out_proj.weight | Grad Norm: 0.001351268612779677\nLayer: transformer_blocks.4.self_attention.out_proj.bias | Grad Norm: 7.148459553718567e-05\nLayer: transformer_blocks.4.context_attn.in_proj_weight | Grad Norm: 0.001020701602101326\nLayer: transformer_blocks.4.context_attn.in_proj_bias | Grad Norm: 0.0006064933259040117\nLayer: transformer_blocks.4.context_attn.out_proj.weight | Grad Norm: 0.001242403406649828\nLayer: transformer_blocks.4.context_attn.out_proj.bias | Grad Norm: 0.0010326392948627472\nLayer: transformer_blocks.4.persona_attn.in_proj_weight | Grad Norm: 0.013679367490112782\nLayer: transformer_blocks.4.persona_attn.in_proj_bias | Grad Norm: 0.0005845252890139818\nLayer: transformer_blocks.4.persona_attn.out_proj.weight | Grad Norm: 0.017997892573475838\nLayer: transformer_blocks.4.persona_attn.out_proj.bias | Grad Norm: 0.001040173345245421\nLayer: transformer_blocks.4.paa_layer.fc.weight | Grad Norm: 0.0026174760423600674\nLayer: transformer_blocks.4.paa_layer.fc.bias | Grad Norm: 0.00017362013750243932\nLayer: transformer_blocks.4.paa_layer.fc_out.weight | Grad Norm: 0.01859421096742153\nLayer: transformer_blocks.4.paa_layer.fc_out.bias | Grad Norm: 0.003549481276422739\nLayer: transformer_blocks.4.mlp.0.weight | Grad Norm: 0.01921028457581997\nLayer: transformer_blocks.4.mlp.0.bias | Grad Norm: 0.0061270189471542835\nLayer: transformer_blocks.4.mlp.3.weight | Grad Norm: 0.032942432910203934\nLayer: transformer_blocks.4.mlp.3.bias | Grad Norm: 0.014971702359616756\nLayer: transformer_blocks.4.mlp.4.weight | Grad Norm: 0.0004958784556947649\nLayer: transformer_blocks.4.mlp.4.bias | Grad Norm: 0.0005022879922762513\nLayer: transformer_blocks.4.layer_norm2.weight | Grad Norm: 0.0005029828171245754\nLayer: transformer_blocks.4.layer_norm2.bias | Grad Norm: 0.0005050821928307414\nLayer: transformer_blocks.5.self_attention.in_proj_weight | Grad Norm: 0.01900646463036537\nLayer: transformer_blocks.5.self_attention.in_proj_bias | Grad Norm: 0.0007154207560233772\nLayer: transformer_blocks.5.self_attention.out_proj.weight | Grad Norm: 0.023801811039447784\nLayer: transformer_blocks.5.self_attention.out_proj.bias | Grad Norm: 0.001266258885152638\nLayer: transformer_blocks.5.context_attn.in_proj_weight | Grad Norm: 0.019502680748701096\nLayer: transformer_blocks.5.context_attn.in_proj_bias | Grad Norm: 0.011585993692278862\nLayer: transformer_blocks.5.context_attn.out_proj.weight | Grad Norm: 0.023516936227679253\nLayer: transformer_blocks.5.context_attn.out_proj.bias | Grad Norm: 0.019829336553812027\nLayer: transformer_blocks.5.persona_attn.in_proj_weight | Grad Norm: 0.2724425196647644\nLayer: transformer_blocks.5.persona_attn.in_proj_bias | Grad Norm: 0.0116445766761899\nLayer: transformer_blocks.5.persona_attn.out_proj.weight | Grad Norm: 0.3254952132701874\nLayer: transformer_blocks.5.persona_attn.out_proj.bias | Grad Norm: 0.02003277838230133\nLayer: transformer_blocks.5.paa_layer.fc.weight | Grad Norm: 0.0446215458214283\nLayer: transformer_blocks.5.paa_layer.fc.bias | Grad Norm: 0.003117932938039303\nLayer: transformer_blocks.5.paa_layer.fc_out.weight | Grad Norm: 0.3228258490562439\nLayer: transformer_blocks.5.paa_layer.fc_out.bias | Grad Norm: 0.0704815611243248\nLayer: transformer_blocks.5.mlp.0.weight | Grad Norm: 0.33558574318885803\nLayer: transformer_blocks.5.mlp.0.bias | Grad Norm: 0.12225139141082764\nLayer: transformer_blocks.5.mlp.3.weight | Grad Norm: 0.5725283622741699\nLayer: transformer_blocks.5.mlp.3.bias | Grad Norm: 0.2964736223220825\nLayer: transformer_blocks.5.mlp.4.weight | Grad Norm: 0.008385341614484787\nLayer: transformer_blocks.5.mlp.4.bias | Grad Norm: 0.008718179538846016\nLayer: transformer_blocks.5.layer_norm2.weight | Grad Norm: 0.008414181880652905\nLayer: transformer_blocks.5.layer_norm2.bias | Grad Norm: 0.008749986067414284\nLayer: final_fc.weight | Grad Norm: 0.39917105436325073\nLayer: final_fc.bias | Grad Norm: 0.015011725015938282\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [1/200] completed | Average Loss: 10.9660\nTraining started for epoch 2/200\nEpoch [2/200] completed | Average Loss: 10.9665\nTraining started for epoch 3/200\nEpoch [3/200] completed | Average Loss: 10.9670\nTraining started for epoch 4/200\nEpoch [4/200] completed | Average Loss: 10.9663\nTraining started for epoch 5/200\nEpoch [5/200] completed | Average Loss: 10.9671\nTraining started for epoch 6/200\nEpoch [6/200] completed | Average Loss: 10.9675\nTraining started for epoch 7/200\nEpoch [7/200] completed | Average Loss: 10.9665\nTraining started for epoch 8/200\nEpoch [8/200] completed | Average Loss: 10.9665\nTraining started for epoch 9/200\nEpoch [9/200] completed | Average Loss: 10.9665\nTraining started for epoch 10/200\nEpoch [10/200] completed | Average Loss: 10.9670\nTraining started for epoch 11/200\nEpoch [11/200] completed | Average Loss: 10.9666\nTraining started for epoch 12/200\nEpoch [12/200] completed | Average Loss: 10.9668\nTraining started for epoch 13/200\nBatch 311/400 | Loss: 10.9460\r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-75-e1b034e90e10>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         logits = model(metacognition_vector_ids,\n\u001b[0m\u001b[1;32m     25\u001b[0m                        \u001b[0mproblem_student_code_ids\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                        \u001b[0mproblem_expected_code_ids\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-72-db3cf00c307c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, metacognitive_vector_ids, problem_student_code_ids, problem_expected_code_ids, context_attention_mask, student_attention_mask)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtransformer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtransformer_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mtransformer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetacognitive_vector_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-44-5676f7b94e37>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs_embeds, encoded_persona, encoded_context)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_persona\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mhR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersona_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_persona\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_persona\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1273\u001b[0m                 is_causal=is_causal)\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1276\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5530\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5532\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_len\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5533\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5534\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":75},{"cell_type":"code","source":"torch.save(model.state_dict(), 'paamodel.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:21:56.731191Z","iopub.execute_input":"2024-12-26T19:21:56.731555Z","iopub.status.idle":"2024-12-26T19:22:00.360417Z","shell.execute_reply.started":"2024-12-26T19:21:56.731524Z","shell.execute_reply":"2024-12-26T19:22:00.359417Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"model = PAAModel()\n\n# Load the saved state dict\nmodel.load_state_dict(torch.load('paamodel.pth'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:22:02.181250Z","iopub.execute_input":"2024-12-26T19:22:02.181564Z","iopub.status.idle":"2024-12-26T19:22:05.290361Z","shell.execute_reply.started":"2024-12-26T19:22:02.181540Z","shell.execute_reply":"2024-12-26T19:22:05.289506Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-61-576f8403c810>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('paamodel.pth'))\n","output_type":"stream"},{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"df_eval = df[149:150].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:24:18.746754Z","iopub.execute_input":"2024-12-26T19:24:18.747092Z","iopub.status.idle":"2024-12-26T19:24:18.752978Z","shell.execute_reply.started":"2024-12-26T19:24:18.747055Z","shell.execute_reply":"2024-12-26T19:24:18.751902Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"df_eval","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_dataset = CustomDataset(df_eval, t5_tokenizer,gpt2_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:24:22.141603Z","iopub.execute_input":"2024-12-26T19:24:22.142013Z","iopub.status.idle":"2024-12-26T19:24:22.146512Z","shell.execute_reply.started":"2024-12-26T19:24:22.141973Z","shell.execute_reply":"2024-12-26T19:24:22.145317Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"\n\ndef inference(model,gpt2_tokenizer, t5_tokenizer, eval_dataset, device):\n    model.eval()\n    model.to(device) \n\n    metacognitive_vector_ids, problem_student_code_ids, problem_expected_code_ids,student_code_ids, target_ids = eval_dataset[0]\n\n    metacognitive_tensor = metacognitive_vector_ids.unsqueeze(0).to(device)  \n    problem_student_code_tensor = problem_student_code_ids.unsqueeze(0).to(device)\n    problem_expected_code_tensor = problem_expected_code_ids.unsqueeze(0).to(device)\n    target_tensor = target_ids.unsqueeze(0).to(device)\n\n    student_attention_mask = (problem_student_code_tensor != gpt2_tokenizer.pad_token_id).long().to(device)\n    context_attention_mask = (problem_expected_code_tensor != t5_tokenizer.pad_token_id).long().to(device)\n\n    \n    with torch.no_grad():         \n        \n        logits = model(\n            metacognitive_vector_ids=metacognitive_tensor,\n            problem_student_code_ids=problem_student_code_tensor,\n            problem_expected_code_ids=problem_expected_code_tensor,\n            context_attention_mask=context_attention_mask,\n            student_attention_mask=student_attention_mask\n        )\n        \n        predictions = logits.argmax(dim=-1).squeeze().tolist()  \n        decoded_text = gpt2_tokenizer.decode(predictions, skip_special_tokens=True)\n\n        \n        return predictions, decoded_text\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:22:34.780970Z","iopub.execute_input":"2024-12-26T19:22:34.781265Z","iopub.status.idle":"2024-12-26T19:22:34.787034Z","shell.execute_reply.started":"2024-12-26T19:22:34.781241Z","shell.execute_reply":"2024-12-26T19:22:34.785813Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"predictions, decoded_text = inference(model, gpt2_tokenizer, t5_tokenizer, eval_dataset, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:24:42.225808Z","iopub.execute_input":"2024-12-26T19:24:42.226095Z","iopub.status.idle":"2024-12-26T19:24:42.294780Z","shell.execute_reply.started":"2024-12-26T19:24:42.226072Z","shell.execute_reply":"2024-12-26T19:24:42.293661Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"print(\"Predicted Tokens:\", predictions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Decoded Text:\", decoded_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:24:45.811008Z","iopub.execute_input":"2024-12-26T19:24:45.811303Z","iopub.status.idle":"2024-12-26T19:24:45.816273Z","shell.execute_reply.started":"2024-12-26T19:24:45.811279Z","shell.execute_reply":"2024-12-26T19:24:45.815227Z"}},"outputs":[{"name":"stdout","text":"Decoded Text: Your approach to reading the problem entirely before starting is excellent. However, you may benefit from summarizing the problem in your own words and identifying the key points more frequently. This will help you better understand the requirements, especially when dealing with complex inputs like arrays of strings or arrays of arrays. For this problem, breaking down the task into smaller sub-goals would have been beneficial. For instance, first handling single strings, then arrays of two strings with the appropriate prefixes, and finally combining all elements into the final string. This breakdown could help you manage the different types of inputs more systematically. Before diving into coding, try to sketch out the algorithm or plan the solution. This planning step can prevent errors like the one in your code where the variable names are inconsistent (you used `item` instead of `a`). Being more vigilant during the implementation process and verifying intermediate results can also help catch such mistakes early. Pay attention to avoiding negligent mistakes, such as using the correct variable names consistently throughout your code. Additionally, always confirm that your final implementation meets all the problem requirements by referring back to the problem statement. This will ensure that your solution is robust and handles all specified cases correctly. Reflecting on similar problems solved earlier can also enhance your problem-solving skills and improve the efficiency of your code.\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"def load_checkpoint(checkpoint_path, model, optimizer=None):\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    loss = checkpoint['loss']\n    print(f\"Checkpoint loaded: Epoch {epoch}, Loss: {loss:.4f}\")\n    return model, optimizer, epoch, loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint_path = \"./checkpoints/model_epoch_89.pth\"\nmodel, optimizer, start_epoch, _ = load_checkpoint(checkpoint_path, model, optimizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint_path = \"./checkpoints/model_epoch_10.pth\"\ncheckpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))  # Use GPU if available: 'cuda'\nmodel.load_state_dict(checkpoint['model_state_dict'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_eval1 = df[449:450].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:22:21.771480Z","iopub.execute_input":"2024-12-26T19:22:21.771768Z","iopub.status.idle":"2024-12-26T19:22:21.776543Z","shell.execute_reply.started":"2024-12-26T19:22:21.771748Z","shell.execute_reply":"2024-12-26T19:22:21.775528Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"eval_dataset1 = CustomDataset(df_eval1, t5_tokenizer,gpt2_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:22:26.490860Z","iopub.execute_input":"2024-12-26T19:22:26.491157Z","iopub.status.idle":"2024-12-26T19:22:26.495243Z","shell.execute_reply.started":"2024-12-26T19:22:26.491134Z","shell.execute_reply":"2024-12-26T19:22:26.494174Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"predictions, decoded_text = inference(model, gpt2_tokenizer, t5_tokenizer, eval_dataset1, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:22:39.226221Z","iopub.execute_input":"2024-12-26T19:22:39.226559Z","iopub.status.idle":"2024-12-26T19:22:39.583175Z","shell.execute_reply.started":"2024-12-26T19:22:39.226534Z","shell.execute_reply":"2024-12-26T19:22:39.582494Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"print(\"Decoded Text:\", decoded_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:22:43.546251Z","iopub.execute_input":"2024-12-26T19:22:43.546672Z","iopub.status.idle":"2024-12-26T19:22:43.552108Z","shell.execute_reply.started":"2024-12-26T19:22:43.546636Z","shell.execute_reply":"2024-12-26T19:22:43.551007Z"}},"outputs":[{"name":"stdout","text":"Decoded Text:  diving on your problem problem diving yous track's crucial's on steps few few you diving you You and you and crucial implementation you, you you. you which second identify you identify and,,,,, and of you you the and, on, the,,, you you solution attention coding coding plan a in and or,, you a plan in logical algorithm you in a instance and, coding with diving in the.,, each in algorithm plan with a or ensure as, needed needed you. and algorithm with, a. algorithm the and mistakes manually through, plan first correct you coding through and during is a This ( and a sketch the the diving a have plan the you with you list Think you the you a a before the will as you you needed coding are each a the the the the plan the needed the the, and you\n and track.. and are you a implementation each., the the a This systematically\n your. the list sequence ensure and the coding solutionolving meets. your the your the that the the the as and the. to,. the each each the coding the the with your, Pay ensure program the the second, Be the the. mistakes the always plan the\n data. mistakes data data will- that thats that the This that. thes process process Keeps the meets the the the the data that that compatible implementation of in that is, given the This andFinally solution data the all code implementation and implementation earlier implementation all constraints data data is the problem problem that that mistakes that in your code in all confirm the and implementation all ensuresss of final yours Ensure Ensure onsss of efficiency of efficiency of code code code code code solved code of of problem your, all of in efficiency code all solveds your, your your problems of thes problems your, and..s of your. the code problems accuracy efficiency accurately solution the\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}