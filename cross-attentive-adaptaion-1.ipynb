{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9859425,"sourceType":"datasetVersion","datasetId":6050791},{"sourceId":10048507,"sourceType":"datasetVersion","datasetId":6190931},{"sourceId":10242967,"sourceType":"datasetVersion","datasetId":6334477}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:47:33.024351Z","iopub.execute_input":"2024-12-28T06:47:33.024647Z","iopub.status.idle":"2024-12-28T06:47:33.340808Z","shell.execute_reply.started":"2024-12-28T06:47:33.024616Z","shell.execute_reply":"2024-12-28T06:47:33.340043Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/metacognitive-dataset/metacognitive-dataset.csv\n/kaggle/input/modified-dataset/modified_dataset.csv\n/kaggle/input/metacognitive-feedback-for-algorithm-solving/final_dataset_with_annotated_metacognitive_feedback_gpt-4o-mini.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install transformers torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:47:33.341912Z","iopub.execute_input":"2024-12-28T06:47:33.342321Z","iopub.status.idle":"2024-12-28T06:47:37.964725Z","shell.execute_reply.started":"2024-12-28T06:47:33.342289Z","shell.execute_reply":"2024-12-28T06:47:37.963651Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import AutoModel, AutoTokenizer, GPT2Model,GPT2Tokenizer,GPT2LMHeadModel\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration , T5Tokenizer , T5Model\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:47:37.966943Z","iopub.execute_input":"2024-12-28T06:47:37.967193Z","iopub.status.idle":"2024-12-28T06:47:42.909489Z","shell.execute_reply.started":"2024-12-28T06:47:37.967172Z","shell.execute_reply":"2024-12-28T06:47:42.908606Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:47:42.910936Z","iopub.execute_input":"2024-12-28T06:47:42.911424Z","iopub.status.idle":"2024-12-28T06:47:42.963632Z","shell.execute_reply.started":"2024-12-28T06:47:42.911390Z","shell.execute_reply":"2024-12-28T06:47:42.962697Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:47:42.964475Z","iopub.execute_input":"2024-12-28T06:47:42.964757Z","iopub.status.idle":"2024-12-28T06:47:42.984686Z","shell.execute_reply.started":"2024-12-28T06:47:42.964696Z","shell.execute_reply":"2024-12-28T06:47:42.983792Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"checkpoint = \"t5-base\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:47:42.985517Z","iopub.execute_input":"2024-12-28T06:47:42.985738Z","iopub.status.idle":"2024-12-28T06:47:43.000401Z","shell.execute_reply.started":"2024-12-28T06:47:42.985718Z","shell.execute_reply":"2024-12-28T06:47:42.999610Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"t5_tokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:47:43.001103Z","iopub.execute_input":"2024-12-28T06:47:43.001337Z","iopub.status.idle":"2024-12-28T06:47:44.314822Z","shell.execute_reply.started":"2024-12-28T06:47:43.001308Z","shell.execute_reply":"2024-12-28T06:47:44.314013Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5998aafd0d454904b1f718fd462c64fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b5ff272bd7245bb8e38c4b484257968"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31b8f84b0e844899892c04c07e9ca556"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"t5_tokenizer.vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:02:48.270038Z","iopub.execute_input":"2024-12-28T07:02:48.270351Z","iopub.status.idle":"2024-12-28T07:02:48.275123Z","shell.execute_reply.started":"2024-12-28T07:02:48.270328Z","shell.execute_reply":"2024-12-28T07:02:48.274406Z"}},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"32100"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"#set the max length to model's default present max length\nt5_tokenizer.model_max_length = t5_tokenizer.model_max_length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:47:44.315532Z","iopub.execute_input":"2024-12-28T06:47:44.315868Z","iopub.status.idle":"2024-12-28T06:47:44.319380Z","shell.execute_reply.started":"2024-12-28T06:47:44.315843Z","shell.execute_reply":"2024-12-28T06:47:44.318469Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:47:44.321167Z","iopub.execute_input":"2024-12-28T06:47:44.321446Z","iopub.status.idle":"2024-12-28T06:47:46.063905Z","shell.execute_reply.started":"2024-12-28T06:47:44.321407Z","shell.execute_reply":"2024-12-28T06:47:46.062999Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2fae9ab899e4db7bf94dc7f9a6cb482"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be4a71e4586e4594bb1087feef137b27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b4001296d2843ef8d7383986e3524f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17460af873844b3db2e9aa34ff39cf45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c9cd1827ba74fb59ddb6385e5432ac3"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:47:46.064865Z","iopub.execute_input":"2024-12-28T06:47:46.065116Z","iopub.status.idle":"2024-12-28T06:47:46.068677Z","shell.execute_reply.started":"2024-12-28T06:47:46.065096Z","shell.execute_reply":"2024-12-28T06:47:46.067717Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"file_path = \"/kaggle/input/metacognitive-feedback-for-algorithm-solving/final_dataset_with_annotated_metacognitive_feedback_gpt-4o-mini.csv\"\ndf = pd.read_csv(file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:49:14.556046Z","iopub.execute_input":"2024-12-28T07:49:14.556356Z","iopub.status.idle":"2024-12-28T07:49:15.510041Z","shell.execute_reply.started":"2024-12-28T07:49:14.556320Z","shell.execute_reply":"2024-12-28T07:49:15.509019Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:49:17.224447Z","iopub.execute_input":"2024-12-28T07:49:17.224727Z","iopub.status.idle":"2024-12-28T07:49:17.230003Z","shell.execute_reply.started":"2024-12-28T07:49:17.224704Z","shell.execute_reply":"2024-12-28T07:49:17.229218Z"}},"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"Index(['Question 1', 'Response 1', 'Right answer 1', 'Q01', 'Q02', 'Q03',\n       'Q04', 'Q05', 'Q06', 'Q07', 'Q08', 'Q09', 'Q10', 'Q11', 'Q12', 'Q13',\n       'Q14', 'Q15', 'Q16', 'metacognitive_vector', 'metacognitive_feedback'],\n      dtype='object')"},"metadata":{}}],"execution_count":91},{"cell_type":"code","source":"df.rename(\n    columns={\n        'Question 1': 'Problem',\n        'Response 1': 'Student_code',\n        'Right answer 1': 'Expected_code'\n    },\n    inplace=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:49:19.375007Z","iopub.execute_input":"2024-12-28T07:49:19.375312Z","iopub.status.idle":"2024-12-28T07:49:19.379941Z","shell.execute_reply.started":"2024-12-28T07:49:19.375283Z","shell.execute_reply":"2024-12-28T07:49:19.378886Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:50:45.999593Z","iopub.execute_input":"2024-12-28T06:50:45.999921Z","iopub.status.idle":"2024-12-28T06:50:46.020114Z","shell.execute_reply.started":"2024-12-28T06:50:45.999893Z","shell.execute_reply":"2024-12-28T06:50:46.019045Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                                             Problem  \\\n0  Develop a Python program that takes the name o...   \n1  Develop a Python program that takes the name o...   \n2  Develop a Python program that takes the name o...   \n\n                                        Student_code  \\\n0  file_input = input()      file_open = open(fil...   \n1  file_input = input()      file_open = open(fil...   \n2  file_input = input()      file_open = open(fil...   \n\n                                       Expected_code        Q01  \\\n0  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n1  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n2  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n\n             Q02               Q03        Q04            Q05            Q06  \\\n0  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n1  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n2  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n\n                Q07  ...        Q09        Q10            Q11            Q12  \\\n0  1 : Almost Never  ...  3 : Often  3 : Often  2 : Sometimes  2 : Sometimes   \n1  1 : Almost Never  ...  3 : Often  3 : Often  2 : Sometimes  2 : Sometimes   \n2  1 : Almost Never  ...  3 : Often  3 : Often  2 : Sometimes  2 : Sometimes   \n\n             Q13        Q14        Q15               Q16  \\\n0  2 : Sometimes  3 : Often  3 : Often  1 : Almost Never   \n1  2 : Sometimes  3 : Often  3 : Often  1 : Almost Never   \n2  2 : Sometimes  3 : Often  3 : Often  1 : Almost Never   \n\n                                metacognitive_vector  \\\n0  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n1  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n2  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n\n                              metacognitive_feedback  \n0  Your initial code serves as a starting point, ...  \n1  Your code exhibits a solid attempt at reading ...  \n2  It looks like you're in a good place with some...  \n\n[3 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Problem</th>\n      <th>Student_code</th>\n      <th>Expected_code</th>\n      <th>Q01</th>\n      <th>Q02</th>\n      <th>Q03</th>\n      <th>Q04</th>\n      <th>Q05</th>\n      <th>Q06</th>\n      <th>Q07</th>\n      <th>...</th>\n      <th>Q09</th>\n      <th>Q10</th>\n      <th>Q11</th>\n      <th>Q12</th>\n      <th>Q13</th>\n      <th>Q14</th>\n      <th>Q15</th>\n      <th>Q16</th>\n      <th>metacognitive_vector</th>\n      <th>metacognitive_feedback</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>Your initial code serves as a starting point, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>Your code exhibits a solid attempt at reading ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>It looks like you're in a good place with some...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 21 columns</p>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"df['combined_problem_student'] = df['Problem'] + \" \" + df['Student_code']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:49:23.416910Z","iopub.execute_input":"2024-12-28T07:49:23.417199Z","iopub.status.idle":"2024-12-28T07:49:23.550502Z","shell.execute_reply.started":"2024-12-28T07:49:23.417175Z","shell.execute_reply":"2024-12-28T07:49:23.549827Z"}},"outputs":[],"execution_count":93},{"cell_type":"code","source":"df['combined_problem_expected'] = df['Problem'] + \" \" + df['Expected_code']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:49:24.361060Z","iopub.execute_input":"2024-12-28T07:49:24.361347Z","iopub.status.idle":"2024-12-28T07:49:24.437067Z","shell.execute_reply.started":"2024-12-28T07:49:24.361313Z","shell.execute_reply":"2024-12-28T07:49:24.436255Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:49:25.646284Z","iopub.execute_input":"2024-12-28T07:49:25.646570Z","iopub.status.idle":"2024-12-28T07:49:25.651530Z","shell.execute_reply.started":"2024-12-28T07:49:25.646545Z","shell.execute_reply":"2024-12-28T07:49:25.650869Z"}},"outputs":[{"execution_count":95,"output_type":"execute_result","data":{"text/plain":"Index(['Problem', 'Student_code', 'Expected_code', 'Q01', 'Q02', 'Q03', 'Q04',\n       'Q05', 'Q06', 'Q07', 'Q08', 'Q09', 'Q10', 'Q11', 'Q12', 'Q13', 'Q14',\n       'Q15', 'Q16', 'metacognitive_vector', 'metacognitive_feedback',\n       'combined_problem_student', 'combined_problem_expected'],\n      dtype='object')"},"metadata":{}}],"execution_count":95},{"cell_type":"code","source":"df.dropna(subset=['Problem', 'metacognitive_feedback', 'combined_problem_student'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:49:27.161020Z","iopub.execute_input":"2024-12-28T07:49:27.161301Z","iopub.status.idle":"2024-12-28T07:49:27.179528Z","shell.execute_reply.started":"2024-12-28T07:49:27.161277Z","shell.execute_reply":"2024-12-28T07:49:27.178789Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"df.reset_index(drop=True, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:49:28.232268Z","iopub.execute_input":"2024-12-28T07:49:28.232565Z","iopub.status.idle":"2024-12-28T07:49:28.236375Z","shell.execute_reply.started":"2024-12-28T07:49:28.232542Z","shell.execute_reply":"2024-12-28T07:49:28.235630Z"}},"outputs":[],"execution_count":97},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:49:29.455424Z","iopub.execute_input":"2024-12-28T07:49:29.455702Z","iopub.status.idle":"2024-12-28T07:49:29.483067Z","shell.execute_reply.started":"2024-12-28T07:49:29.455680Z","shell.execute_reply":"2024-12-28T07:49:29.482239Z"}},"outputs":[{"execution_count":98,"output_type":"execute_result","data":{"text/plain":"Problem                      0\nStudent_code                 0\nExpected_code                0\nQ01                          0\nQ02                          0\nQ03                          0\nQ04                          0\nQ05                          0\nQ06                          0\nQ07                          0\nQ08                          0\nQ09                          0\nQ10                          0\nQ11                          0\nQ12                          0\nQ13                          0\nQ14                          0\nQ15                          0\nQ16                          0\nmetacognitive_vector         0\nmetacognitive_feedback       0\ncombined_problem_student     0\ncombined_problem_expected    0\ndtype: int64"},"metadata":{}}],"execution_count":98},{"cell_type":"code","source":"df['metacognitive_feedback'][100]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:39:41.128643Z","iopub.execute_input":"2024-12-26T17:39:41.128956Z","iopub.status.idle":"2024-12-26T17:39:41.134473Z","shell.execute_reply.started":"2024-12-26T17:39:41.128929Z","shell.execute_reply":"2024-12-26T17:39:41.133782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:39:43.698529Z","iopub.execute_input":"2024-12-26T17:39:43.698866Z","iopub.status.idle":"2024-12-26T17:39:43.709396Z","shell.execute_reply.started":"2024-12-26T17:39:43.698838Z","shell.execute_reply":"2024-12-26T17:39:43.70823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport ast\nclass CustomDataset(Dataset):\n    def __init__(self, dataset, t5_tokenizer,gpt2_tokenizer, max_length=512):\n        self.t5_tokenizer = t5_tokenizer\n        self.gpt2_tokenizer = gpt2_tokenizer\n        self.data = dataset\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        metacognitive_vector = self.data['metacognitive_vector'][idx]\n        problem_student_code = self.data['combined_problem_student'][idx]\n        problem_expected_code = self.data['combined_problem_expected'][idx]\n        student_code = self.data['Student_code'][idx]\n        target = self.data['metacognitive_feedback'][idx]\n\n        metacognitive_vector_float = [\n        float(item.strip()) for item in ast.literal_eval(metacognitive_vector)]\n        metacognition_vector_ids = torch.tensor(metacognitive_vector_float, dtype=torch.float)\n        \n        problem_student_code_ids = torch.tensor(\n            self.t5_tokenizer.encode(problem_student_code, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n        problem_expected_code_ids = torch.tensor(\n            self.t5_tokenizer.encode(problem_expected_code, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n        \n        student_code_ids = torch.tensor(\n            self.t5_tokenizer.encode(student_code, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n        target_ids = torch.tensor(\n            self.t5_tokenizer.encode(target, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n\n        return metacognition_vector_ids, problem_student_code_ids, problem_expected_code_ids, student_code_ids, target_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:49:32.648292Z","iopub.execute_input":"2024-12-28T07:49:32.648582Z","iopub.status.idle":"2024-12-28T07:49:32.655232Z","shell.execute_reply.started":"2024-12-28T07:49:32.648559Z","shell.execute_reply":"2024-12-28T07:49:32.654527Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"dataset = CustomDataset(df, t5_tokenizer, gpt2_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:49:34.202427Z","iopub.execute_input":"2024-12-28T07:49:34.202712Z","iopub.status.idle":"2024-12-28T07:49:34.206252Z","shell.execute_reply.started":"2024-12-28T07:49:34.202690Z","shell.execute_reply":"2024-12-28T07:49:34.205503Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"len(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:39:32.380356Z","iopub.execute_input":"2024-12-28T07:39:32.380638Z","iopub.status.idle":"2024-12-28T07:39:32.386228Z","shell.execute_reply.started":"2024-12-28T07:39:32.380614Z","shell.execute_reply":"2024-12-28T07:39:32.385310Z"}},"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"16803"},"metadata":{}}],"execution_count":84},{"cell_type":"code","source":"metacognition_vector_ids, problem_student_code_ids, problem_expected_code_ids, student_code_ids, target_ids = dataset[100]\nprint(f\"Metacognition vector IDs: {metacognition_vector_ids}\")\nprint(f\"Expected feedback IDs: {problem_student_code_ids.shape}\")\nprint(f\"Expected encoded feedback IDs: {problem_expected_code_ids.shape}\")\nprint(f\"Student Answer IDs: {student_code_ids}\")\nprint(f\"Target IDs: {target_ids}\")\nprint(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:57:38.334880Z","iopub.execute_input":"2024-12-28T06:57:38.335154Z","iopub.status.idle":"2024-12-28T06:57:38.358558Z","shell.execute_reply.started":"2024-12-28T06:57:38.335133Z","shell.execute_reply":"2024-12-28T06:57:38.357689Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Metacognition vector IDs: tensor([2., 3., 1., 3., 3., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3., 2.])\nExpected feedback IDs: torch.Size([512])\nExpected encoded feedback IDs: torch.Size([512])\nStudent Answer IDs: tensor([ 3785,   834, 11966,  4350,  2423,    77,  2562,  9960,  1713,   188,\n         2176,    15,     6,  2122, 31911,  2294,  2394,     6,  2938, 27436,\n         1713,   279,    32,   115,     6,  4928, 20223, 13523,  4433,     6,\n        27640,     5,   357,  1713, 18610,   760,   630,     6,  2884, 31497,\n         2294,  3072,     6,  2938, 19419,  1713,   308,     9,  6961,     6,\n         1714,    87,  4305, 13523,  3301,     6,  2606, 16029,  1713,   427,\n          162,     6,  2517,    87,  5176, 13523,  2079,     6,  2517, 12100,\n           20,    89,   608,   834, 11966,   599,    77,  2562,   834, 11966,\n         4350,    61,    10,    28,   539,   599,    77,  2562,   834, 11966,\n         4350,   976,    52,  8512,    38,  1042,    10, 10223,  2423, 11966,\n            5,  5236,  6972,  9960,  3785,   834,  3350,  2423,  6306,   908,\n           21,   331,    16, 10223,    10,  3785,   834,  3350,     5,  3096,\n          989,   599,  6757,     5,     7, 14192,  9960,     5,     7,  5900,\n           17,   599,  1686,  8512,    61,  1205,  3785,   834,  3350,    20,\n           89, 11837,   834, 28951,   834,    88,  2632,   599,  1201,  4347,\n         1201,  7318,    10,  1903,  3785,   834,  3350,  2423,  5236,   834,\n        11966,   599,    77,  2562,   834, 11966,  4350,    61,   331,   834,\n         4370,  2423,     2,   215,   834,  3350,  2423,  6306,   908,    21,\n            3,    23,    16,   620,   599,    40,    35,   599,    77,  2562,\n          834,  3350,    61,    61,    10,   331,   834,  4370,  6306,    77,\n         2562,   834,  3350,  6306,    23,   908,  6306,   536,  4275,     7,\n         5900,    17,   599,   121,    87,  8512,  6306,   357,   908,   908,\n         2423,    77,  2562,   834,  3350,  6306,    23,   908,  6306,   357,\n          908,  2281,   599,  6757,   834,  4370,    61,    20,    89,  3519,\n          834,  1201,   599,  4370,    61,    10,  3519,   834,  1201,  2423,\n          567,   782,    21,   843,    16,     3,  4370,    10,     3,    99,\n         3519,   834,  1201,  2423,  2423,   567,   782,    10,  3519,   834,\n         1201,  2423,  4397,     3,    15,    40,    99,  3519,   834,  1201,\n         3155,  4397,    10,  3519,   834,  1201,  2423,  4397,  1205,  3519,\n          834,  1201,    20,    89,  9858,   834,  1201,   599,  4370,    61,\n           10,  9858,   834,  1201,  2423,   567,   782,    21,   843,    16,\n            3,  4370,    10,     3,    99,  9858,   834,  1201,  2423,  2423,\n          567,   782,    10,  9858,   834,  1201,  2423,  4397,     3,    15,\n           40,    99,  9858,   834,  1201,     1,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0])\nTarget IDs: tensor([  696,   750,  4432,  1267,     3,     9,   207,  1941,    16, 21055,\n         1725,    39,  1081,    28,  3621,     6,    68,   132,    33,   128,\n          844,   213,   856, 15489,   297,    19,  1316,    12,   942,     8,\n          682,  1502,     5,  1485,     6,  1099,   149,    25,    31,    60,\n            3, 18147,     8,  3079,   344,  3879,   203,    11,  3902,     7,\n            5,   818,    25,    33,  6549,  7241,  3056,     6,  3879,  5522,\n            7,     6,    11,  3902,     7,   139,     3,     9, 24297,     6,\n          317,    81,   149,    25,    54, 12955,  3902,     7,    57,  5112,\n         1446,    13,   215,     5,   100,    56, 24104,     8, 18643,    13,\n         1348,  3902,     7,     5,    94,    22,     7,  4462,    12,  6494,\n          190,    39,  3785,   570,   728,    12, 21494,    11,   433,     8,\n          331,   274,  1849,   136, 19868,    21,     3,     9, 23980,   318,\n         2748,     7, 15596, 13250,  1014,    48,  9769,    16,    39,     3,\n            2, 10379, 14270,   834, 28951,   834,    88,  2632,     2,  1681,\n            5,  1203,     6,   766,    24,    25,    33,     3, 21049,  3902,\n            7,    12,     8,  2024,   331,   686,   274,  5505,   136,     3,\n            9, 30922,    51,  7578,  2673,     5,  5433,     6,   240,     3,\n            9,  4645,   320,    44,   149,    25,    22,    60,     3, 11682,\n            8,   620,    13,  4160,   117,   269,   230,    34,  1330,   114,\n           25,   164,    36,     3,  7388,    30,   775,   203,  1446,    13,\n         4160,     5,  9151,  1577,     3,     9, 20036,  1809,    24,  1153,\n         9624, 11498,   776,     7,   284,  3902,   139,   165,     3,  9921,\n         5112, 11325,     5,  7053,     6,    38,    25,  1344,    39,  1081,\n            6,  1423,    12,  3393,    39,   478,    31,     7,  2537,    11,\n         6142, 14031,   318,  8048,    19,   902,   359,   116,    34,   639,\n           12,  1681,  3088,    11,   331,  1809, 18175,     7,     5, 26550,\n           53,     3,     9,   515,   274,  4432,    54,  4019,  3391,    39,\n          682,    18,  6065,    53,   433,    11,  1792,  6854,  6269,    51,\n           53,    45, 20298,     5,  9126,     6,  1961, 15290,    11,   916,\n         6273,    77,    53,    39,  1295,    57,  6450,   284,  3876,   581,\n            8,   682,    31,     7,  1502,     6,  2932,     3,  5833,   576,\n          760,  1433,    11,   743,   655,    16,    39,  1127,     5,     1,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0])\n\n\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"gpt2_tokenizer.pad_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:55:55.371661Z","iopub.execute_input":"2024-12-28T06:55:55.371993Z","iopub.status.idle":"2024-12-28T06:55:55.376747Z","shell.execute_reply.started":"2024-12-28T06:55:55.371969Z","shell.execute_reply":"2024-12-28T06:55:55.375854Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"'<|endoftext|>'"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"gpt2_pad_token_id = gpt2_tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:55:57.319396Z","iopub.execute_input":"2024-12-28T06:55:57.319726Z","iopub.status.idle":"2024-12-28T06:55:57.323125Z","shell.execute_reply.started":"2024-12-28T06:55:57.319695Z","shell.execute_reply":"2024-12-28T06:55:57.322367Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"gpt2_pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:55:58.315080Z","iopub.execute_input":"2024-12-28T06:55:58.315354Z","iopub.status.idle":"2024-12-28T06:55:58.320154Z","shell.execute_reply.started":"2024-12-28T06:55:58.315333Z","shell.execute_reply":"2024-12-28T06:55:58.319221Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"50256"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"t5_tokenizer.pad_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:55:59.308954Z","iopub.execute_input":"2024-12-28T06:55:59.309264Z","iopub.status.idle":"2024-12-28T06:55:59.314175Z","shell.execute_reply.started":"2024-12-28T06:55:59.309236Z","shell.execute_reply":"2024-12-28T06:55:59.313469Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"'<pad>'"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"t5_pad_token_id = t5_tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:56:00.411684Z","iopub.execute_input":"2024-12-28T06:56:00.412086Z","iopub.status.idle":"2024-12-28T06:56:00.415714Z","shell.execute_reply.started":"2024-12-28T06:56:00.412044Z","shell.execute_reply":"2024-12-28T06:56:00.414966Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"t5_pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:56:01.659098Z","iopub.execute_input":"2024-12-28T06:56:01.659413Z","iopub.status.idle":"2024-12-28T06:56:01.664191Z","shell.execute_reply.started":"2024-12-28T06:56:01.659383Z","shell.execute_reply":"2024-12-28T06:56:01.663362Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":39},{"cell_type":"markdown","source":"# Context Enocder","metadata":{}},{"cell_type":"code","source":"class ContextEncoder(nn.Module): \n    def __init__(self, t5_model_name='t5-base', output_dim=1024): \n        super(ContextEncoder, self).__init__()\n        \n        self.t5_encoder = T5Model.from_pretrained(t5_model_name).encoder\n        self.fc = nn.Linear(self.t5_encoder.config.d_model, output_dim)\n    \n    def forward(self, problem_student_code_ids, attention_masks=None):        \n\n        encoder_outputs = self.t5_encoder(\n            input_ids=problem_student_code_ids,\n            attention_mask=attention_masks\n        )\n        context_hidden_states = encoder_outputs.last_hidden_state   \n        \n        context_rep = context_hidden_states.mean(dim=1)\n        \n       \n        context_rep = self.fc(context_rep)\n        final_rep = context_rep.unsqueeze(1)\n        \n        return final_rep","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:58:54.216987Z","iopub.execute_input":"2024-12-28T06:58:54.217277Z","iopub.status.idle":"2024-12-28T06:58:54.222425Z","shell.execute_reply.started":"2024-12-28T06:58:54.217256Z","shell.execute_reply":"2024-12-28T06:58:54.221428Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"context_encoder = ContextEncoder().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:58:56.329411Z","iopub.execute_input":"2024-12-28T06:58:56.329693Z","iopub.status.idle":"2024-12-28T06:58:57.031285Z","shell.execute_reply.started":"2024-12-28T06:58:56.329671Z","shell.execute_reply":"2024-12-28T06:58:57.030347Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"context_encoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:58:57.032358Z","iopub.execute_input":"2024-12-28T06:58:57.032600Z","iopub.status.idle":"2024-12-28T06:58:57.038555Z","shell.execute_reply.started":"2024-12-28T06:58:57.032579Z","shell.execute_reply":"2024-12-28T06:58:57.037650Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"ContextEncoder(\n  (t5_encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (fc): Linear(in_features=768, out_features=768, bias=True)\n)"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"class MetacognitionLayer(nn.Module):\n    def __init__(self, metacognitive_dim=16, output_dim=1024):\n        super(MetacognitionLayer, self).__init__()\n        #16 to 768 mapping\n        self.metacognitive_fc = nn.Linear(metacognitive_dim, output_dim) \n        self.final_fc = nn.Linear(output_dim, output_dim)\n\n    def forward(self, metacognitive_vector):\n        \n\n        metacognitive_rep = self.metacognitive_fc(metacognitive_vector)\n        final_rep = self.final_fc(metacognitive_rep)  \n        persona_rep = final_rep.unsqueeze(1)\n\n        return persona_rep","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:59:18.854372Z","iopub.execute_input":"2024-12-28T06:59:18.854699Z","iopub.status.idle":"2024-12-28T06:59:18.859340Z","shell.execute_reply.started":"2024-12-28T06:59:18.854672Z","shell.execute_reply":"2024-12-28T06:59:18.858562Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"metacognitive_emb = MetacognitionLayer().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:59:21.254191Z","iopub.execute_input":"2024-12-28T06:59:21.254474Z","iopub.status.idle":"2024-12-28T06:59:21.263885Z","shell.execute_reply.started":"2024-12-28T06:59:21.254451Z","shell.execute_reply":"2024-12-28T06:59:21.263062Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"metacognitive_emb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:59:23.137586Z","iopub.execute_input":"2024-12-28T06:59:23.137936Z","iopub.status.idle":"2024-12-28T06:59:23.143356Z","shell.execute_reply.started":"2024-12-28T06:59:23.137906Z","shell.execute_reply":"2024-12-28T06:59:23.142578Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"MetacognitionLayer(\n  (metacognitive_fc): Linear(in_features=16, out_features=768, bias=True)\n  (final_fc): Linear(in_features=768, out_features=768, bias=True)\n)"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"class PAALayer(nn.Module):\n    def __init__(self, hidden_dimension = 768 , tau=0.8,dropout_rate=0.1):\n        super(PAALayer, self).__init__()\n        self.hidden_dimenstion = hidden_dimension\n        self.tau = tau\n\n       \n        self.fc = nn.Linear(2 * hidden_dimension, hidden_dimension)  \n        self.sigmoid = nn.Sigmoid()\n        self.fc_out = nn.Linear(hidden_dimension, hidden_dimension)\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n\n    def forward(self, hR , oP, oC):\n       \n        Mp_input  = torch.cat([hR,oP], dim=-1)        \n        Mp = self.fc(Mp_input)      \n        Wp = self.sigmoid(Mp)      \n     \n        Mpersona = Wp\n        Mcontext = 1 - Wp      \n       \n        oP_weighted = Mpersona * oP       \n        oC_weighted = Mcontext * oC\n       \n        HPAA = oP_weighted + oC_weighted \n       \n        output = self.fc_out(HPAA)\n       \n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:26:37.931288Z","iopub.execute_input":"2024-12-28T07:26:37.931610Z","iopub.status.idle":"2024-12-28T07:26:37.937510Z","shell.execute_reply.started":"2024-12-28T07:26:37.931583Z","shell.execute_reply":"2024-12-28T07:26:37.936617Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"paa = PAALayer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:59:37.340110Z","iopub.execute_input":"2024-12-28T06:59:37.340420Z","iopub.status.idle":"2024-12-28T06:59:37.364570Z","shell.execute_reply.started":"2024-12-28T06:59:37.340392Z","shell.execute_reply":"2024-12-28T06:59:37.363762Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"paa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:59:37.803271Z","iopub.execute_input":"2024-12-28T06:59:37.803560Z","iopub.status.idle":"2024-12-28T06:59:37.808554Z","shell.execute_reply.started":"2024-12-28T06:59:37.803535Z","shell.execute_reply":"2024-12-28T06:59:37.807843Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"PAALayer(\n  (fc): Linear(in_features=2048, out_features=1024, bias=True)\n  (sigmoid): Sigmoid()\n  (fc_out): Linear(in_features=1024, out_features=1024, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"class CustomTransformerBlock(nn.Module):\n    def __init__(self, hidden_size, tau , dropout_rate=0.1):\n        super(CustomTransformerBlock, self).__init__()\n\n        self.input_self_attention = nn.MultiheadAttention(hidden_size, num_heads=12, batch_first=True)\n\n\n        self.context_attn = nn.MultiheadAttention(hidden_size, num_heads=12, batch_first=True)\n        self.persona_attn = nn.MultiheadAttention(hidden_size, num_heads=12, batch_first=True)\n        #self.output_attn = nn.MultiheadAttention(hidden_size, num_head = 12, batch_first = True)\n\n        #self.output_self_attention = nn.MultiheadAttention(hidden_size, num_heads , batch_first = True)\n\n        self.paa_layer = PAALayer(hidden_dimension=hidden_size, tau=tau)\n        \n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_size, 2048),\n            nn.ReLU(),\n            nn.Dropout(p=0.1),\n            nn.Linear(2048, hidden_size),\n            nn.LayerNorm(hidden_size)\n        )\n        self.layer_norm2 = nn.LayerNorm(hidden_size)\n       \n    def forward(self, current_state, encoded_persona, encoded_context):\n\n        hR, _ = self.input_self_attention(current_state, current_state, current_state)\n        oP, _ = self.persona_attn(hR, encoded_persona, encoded_persona)\n        oC, _ = self.context_attn(oP, encoded_context, encoded_context)   \n\n        #PC, _ = self.output_self_attention(oC , oC , oC)\n        \n        HPAA = self.paa_layer(hR, oP, oC)\n        \n        mlp_output = self.mlp(HPAA)\n        output = self.layer_norm2(mlp_output + HPAA)\n      \n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:26:09.441532Z","iopub.execute_input":"2024-12-28T07:26:09.441901Z","iopub.status.idle":"2024-12-28T07:26:09.448268Z","shell.execute_reply.started":"2024-12-28T07:26:09.441872Z","shell.execute_reply":"2024-12-28T07:26:09.447439Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"custom_layer = CustomTransformerBlock(768,0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:26:10.738824Z","iopub.execute_input":"2024-12-28T07:26:10.739099Z","iopub.status.idle":"2024-12-28T07:26:10.815343Z","shell.execute_reply.started":"2024-12-28T07:26:10.739079Z","shell.execute_reply":"2024-12-28T07:26:10.814734Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"custom_layer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:26:12.429127Z","iopub.execute_input":"2024-12-28T07:26:12.429412Z","iopub.status.idle":"2024-12-28T07:26:12.434471Z","shell.execute_reply.started":"2024-12-28T07:26:12.429388Z","shell.execute_reply":"2024-12-28T07:26:12.433646Z"}},"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"CustomTransformerBlock(\n  (input_self_attention): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (context_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (persona_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (paa_layer): PAALayer(\n    (fc): Linear(in_features=1536, out_features=768, bias=True)\n    (sigmoid): Sigmoid()\n    (fc_out): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (mlp): Sequential(\n    (0): Linear(in_features=768, out_features=2048, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.1, inplace=False)\n    (3): Linear(in_features=2048, out_features=768, bias=True)\n    (4): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n)"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PAAModel(nn.Module):\n    def __init__(self, hidden_size=768, vocab_size = 32100 ,tau=0.8, max_length=512, num_transformer_blocks=4):\n        super(PAAModel , self).__init__()\n        self.hidden_size = hidden_size\n        self.tau = tau\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n        self.num_transformer_blocks = num_transformer_blocks\n\n        self.context_encoder = ContextEncoder()\n        \n        self.metacognitive_emb = MetacognitionLayer()\n        \n        for param in self.context_encoder.parameters():\n            param.requires_grad = False\n\n        for param in self.metacognitive_emb.parameters():\n            param.requires_grad = False\n\n        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n        self.position_embedding = nn.Embedding(max_length, hidden_size)\n        self.dropout = nn.Dropout(p=0.1) \n\n        # self.self_attention = nn.MultiheadAttention(hidden_size, num_heads=12, batch_first=True)        \n        self.transformer_blocks = nn.ModuleList([CustomTransformerBlock(hidden_size, tau) for _ in range(num_transformer_blocks)])\n        self.final_fc = nn.Linear(hidden_size, vocab_size)\n        self.layer_norm = nn.LayerNorm(hidden_size)\n\n    def forward(self,  metacognitive_vector_ids,\n                       problem_student_code_ids ,\n                       problem_expected_code_ids ,\n                       expected_attention_mask,\n                       student_attention_mask):\n\n        metacognitive_vector_emb = self.metacognitive_emb(metacognitive_vector_ids)\n\n        problem_student_encoded = self.context_encoder(problem_student_code_ids,\n                                                attention_masks=student_attention_mask)\n        problem_expected_encoded = self.context_encoder(problem_expected_code_ids,\n                                                       attention_masks = expected_attention_mask)\n        \n        seq_length = problem_student_encoded.size(1)\n        token_embeds = self.token_embedding(problem_student_code_ids)\n        position_ids = torch.arange(0, seq_length, device=problem_student_code_ids.device).unsqueeze(0)\n        position_embeds = self.position_embedding(position_ids)\n        \n        # Combine embeddings\n        inputs_embeds = token_embeds + position_embeds\n        inputs_embeds = self.dropout(inputs_embeds)\n      \n        current_state = inputs_embeds\n        for transformer_block in self.transformer_blocks:\n            transformer_output = transformer_block(current_state, metacognitive_vector_emb, problem_expected_encoded)           \n       \n        logits = self.final_fc(transformer_output)    \n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:26:57.692304Z","iopub.execute_input":"2024-12-28T07:26:57.692639Z","iopub.status.idle":"2024-12-28T07:26:57.699920Z","shell.execute_reply.started":"2024-12-28T07:26:57.692611Z","shell.execute_reply":"2024-12-28T07:26:57.699173Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"model = PAAModel()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:26:59.025121Z","iopub.execute_input":"2024-12-28T07:26:59.025433Z","iopub.status.idle":"2024-12-28T07:27:00.518070Z","shell.execute_reply.started":"2024-12-28T07:26:59.025405Z","shell.execute_reply":"2024-12-28T07:27:00.517159Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:27:02.469546Z","iopub.execute_input":"2024-12-28T07:27:02.469891Z","iopub.status.idle":"2024-12-28T07:27:02.749679Z","shell.execute_reply.started":"2024-12-28T07:27:02.469863Z","shell.execute_reply":"2024-12-28T07:27:02.748874Z"}},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"PAAModel(\n  (context_encoder): ContextEncoder(\n    (t5_encoder): T5Stack(\n      (embed_tokens): Embedding(32128, 768)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n                (relative_attention_bias): Embedding(32, 12)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=768, out_features=3072, bias=False)\n                (wo): Linear(in_features=3072, out_features=768, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-11): 11 x T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=768, out_features=3072, bias=False)\n                (wo): Linear(in_features=3072, out_features=768, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (fc): Linear(in_features=768, out_features=768, bias=True)\n  )\n  (metacognitive_emb): MetacognitionLayer(\n    (metacognitive_fc): Linear(in_features=16, out_features=768, bias=True)\n    (final_fc): Linear(in_features=768, out_features=768, bias=True)\n  )\n  (token_embedding): Embedding(32100, 768)\n  (position_embedding): Embedding(512, 768)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (transformer_blocks): ModuleList(\n    (0-3): 4 x CustomTransformerBlock(\n      (input_self_attention): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n      )\n      (context_attn): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n      )\n      (persona_attn): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n      )\n      (paa_layer): PAALayer(\n        (fc): Linear(in_features=1536, out_features=768, bias=True)\n        (sigmoid): Sigmoid()\n        (fc_out): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (mlp): Sequential(\n        (0): Linear(in_features=768, out_features=2048, bias=True)\n        (1): ReLU()\n        (2): Dropout(p=0.1, inplace=False)\n        (3): Linear(in_features=2048, out_features=768, bias=True)\n        (4): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (final_fc): Linear(in_features=768, out_features=32100, bias=True)\n  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n)"},"metadata":{}}],"execution_count":62},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:27:42.200447Z","iopub.execute_input":"2024-12-28T07:27:42.200746Z","iopub.status.idle":"2024-12-28T07:27:42.204445Z","shell.execute_reply.started":"2024-12-28T07:27:42.200723Z","shell.execute_reply":"2024-12-28T07:27:42.203615Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"import torch.optim as optim\noptimizer = optim.AdamW(model.parameters(), lr=5e-5)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:27:43.514115Z","iopub.execute_input":"2024-12-28T07:27:43.514403Z","iopub.status.idle":"2024-12-28T07:27:43.945524Z","shell.execute_reply.started":"2024-12-28T07:27:43.514379Z","shell.execute_reply":"2024-12-28T07:27:43.944889Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"LOSS = torch.nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:27:45.524935Z","iopub.execute_input":"2024-12-28T07:27:45.525399Z","iopub.status.idle":"2024-12-28T07:27:45.529203Z","shell.execute_reply.started":"2024-12-28T07:27:45.525374Z","shell.execute_reply":"2024-12-28T07:27:45.528376Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"num_epochs = 200","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:27:46.830490Z","iopub.execute_input":"2024-12-28T07:27:46.830826Z","iopub.status.idle":"2024-12-28T07:27:46.834182Z","shell.execute_reply.started":"2024-12-28T07:27:46.830752Z","shell.execute_reply":"2024-12-28T07:27:46.833437Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"df_train = df[0:5000]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:49:43.637889Z","iopub.execute_input":"2024-12-28T07:49:43.638172Z","iopub.status.idle":"2024-12-28T07:49:43.642084Z","shell.execute_reply.started":"2024-12-28T07:49:43.638150Z","shell.execute_reply":"2024-12-28T07:49:43.641207Z"}},"outputs":[],"execution_count":101},{"cell_type":"code","source":"len(df_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:49:44.625381Z","iopub.execute_input":"2024-12-28T07:49:44.625669Z","iopub.status.idle":"2024-12-28T07:49:44.630506Z","shell.execute_reply.started":"2024-12-28T07:49:44.625645Z","shell.execute_reply":"2024-12-28T07:49:44.629643Z"}},"outputs":[{"execution_count":102,"output_type":"execute_result","data":{"text/plain":"5000"},"metadata":{}}],"execution_count":102},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:49:46.011448Z","iopub.execute_input":"2024-12-28T07:49:46.011901Z","iopub.status.idle":"2024-12-28T07:49:46.028314Z","shell.execute_reply.started":"2024-12-28T07:49:46.011861Z","shell.execute_reply":"2024-12-28T07:49:46.027377Z"}},"outputs":[{"execution_count":103,"output_type":"execute_result","data":{"text/plain":"Problem                      0\nStudent_code                 0\nExpected_code                0\nQ01                          0\nQ02                          0\nQ03                          0\nQ04                          0\nQ05                          0\nQ06                          0\nQ07                          0\nQ08                          0\nQ09                          0\nQ10                          0\nQ11                          0\nQ12                          0\nQ13                          0\nQ14                          0\nQ15                          0\nQ16                          0\nmetacognitive_vector         0\nmetacognitive_feedback       0\ncombined_problem_student     0\ncombined_problem_expected    0\ndtype: int64"},"metadata":{}}],"execution_count":103},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ntrain_dataset = CustomDataset(df_train, t5_tokenizer , gpt2_tokenizer)\ntrain_dataloader = DataLoader(train_dataset , batch_size = 1 ,shuffle = True )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:49:47.895381Z","iopub.execute_input":"2024-12-28T07:49:47.895680Z","iopub.status.idle":"2024-12-28T07:49:47.900095Z","shell.execute_reply.started":"2024-12-28T07:49:47.895656Z","shell.execute_reply":"2024-12-28T07:49:47.899256Z"}},"outputs":[],"execution_count":104},{"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T17:41:11.938191Z","iopub.execute_input":"2024-12-26T17:41:11.938527Z","iopub.status.idle":"2024-12-26T17:41:14.447261Z","shell.execute_reply.started":"2024-12-26T17:41:11.938501Z","shell.execute_reply":"2024-12-26T17:41:14.446294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint_dir = \"./checkpoints\"\nos.makedirs(checkpoint_dir, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:27:59.125123Z","iopub.execute_input":"2024-12-28T07:27:59.125443Z","iopub.status.idle":"2024-12-28T07:27:59.129413Z","shell.execute_reply.started":"2024-12-28T07:27:59.125414Z","shell.execute_reply":"2024-12-28T07:27:59.128469Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    print(f\"Training started for epoch {epoch + 1}/{num_epochs}\")\n    model.train()\n    total_loss = 0\n\n    for idx, (metacognition_vector_ids,\n              problem_student_code_ids,\n              problem_expected_code_ids,\n              student_code_ids,\n              target_ids) in enumerate(train_dataloader):\n        \n        metacognition_vector_ids = metacognition_vector_ids.to(device)\n        problem_student_code_ids = problem_student_code_ids.to(device)\n        problem_expected_code_ids = problem_expected_code_ids.to(device)\n        student_code_ids = student_code_ids.to(device)\n        target_ids = target_ids.to(device)\n        \n        #attention masking\n        student_attention_mask = (problem_student_code_ids != t5_pad_token_id).long().to(device)\n        expected_attention_mask = (problem_expected_code_ids != t5_pad_token_id).long().to(device)\n        \n\n        optimizer.zero_grad()\n        logits = model(metacognition_vector_ids,\n                       problem_student_code_ids ,\n                       problem_expected_code_ids ,\n                       expected_attention_mask,\n                       student_attention_mask)\n\n     \n        logits = logits.view(-1, logits.size(-1))\n        target_ids = target_ids.view(-1)\n\n       \n        loss = LOSS(logits, target_ids)\n        total_loss += loss.item()\n\n      \n        loss.backward()       \n        for name, param in model.named_parameters():\n            if 'context_encoder' in name:\n                assert param.grad is None, f\"Gradients found in frozen encoder {name}\"\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)       \n        optimizer.step()    \n        \n        if idx % 10 == 0:\n            print(f\"Batch {idx + 1}/{len(train_dataloader)} | Loss: {loss.item():.4f}\" , end='\\r')\n\n\n\n\n    \n                    \n    if epoch % 10 ==0 :\n            for name, param in model.named_parameters():\n                if param.requires_grad and param.grad is not None:\n                    print(f\"Layer: {name} | Grad Norm: {param.grad.norm().item()}\")\n                elif param.requires_grad:\n                    print(f\"Layer: {name} | Grad: None\")       \n    \n    if (epoch + 1) % 20 == 0:\n        checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch + 1}.pth\")\n        torch.save({\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': total_loss / max(len(train_dataloader), 1),\n        }, checkpoint_path)\n        print(f\"Checkpoint saved at {checkpoint_path}\")\n\n   \n    avg_loss = total_loss / max(len(train_dataloader), 1)\n    #writer.add_scalar(\"Loss/train\", avg_loss, epoch + 1)\n    print(f\"Epoch [{epoch + 1}/{num_epochs}] completed | Average Loss: {avg_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T07:49:50.314082Z","iopub.execute_input":"2024-12-28T07:49:50.314383Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Training started for epoch 1/200\nLayer: token_embedding.weight | Grad Norm: 0.008909603580832481\nLayer: position_embedding.weight | Grad Norm: 0.012957422994077206\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad: None\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad: None\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad: None\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad: None\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad: None\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad: None\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad: None\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad: None\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad: None\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad: None\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad: None\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad: None\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad: None\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad: None\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad: None\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad: None\nLayer: transformer_blocks.0.mlp.0.weight | Grad: None\nLayer: transformer_blocks.0.mlp.0.bias | Grad: None\nLayer: transformer_blocks.0.mlp.3.weight | Grad: None\nLayer: transformer_blocks.0.mlp.3.bias | Grad: None\nLayer: transformer_blocks.0.mlp.4.weight | Grad: None\nLayer: transformer_blocks.0.mlp.4.bias | Grad: None\nLayer: transformer_blocks.0.layer_norm2.weight | Grad: None\nLayer: transformer_blocks.0.layer_norm2.bias | Grad: None\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad: None\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad: None\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad: None\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad: None\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad: None\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad: None\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad: None\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad: None\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad: None\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad: None\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad: None\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad: None\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad: None\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad: None\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad: None\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad: None\nLayer: transformer_blocks.1.mlp.0.weight | Grad: None\nLayer: transformer_blocks.1.mlp.0.bias | Grad: None\nLayer: transformer_blocks.1.mlp.3.weight | Grad: None\nLayer: transformer_blocks.1.mlp.3.bias | Grad: None\nLayer: transformer_blocks.1.mlp.4.weight | Grad: None\nLayer: transformer_blocks.1.mlp.4.bias | Grad: None\nLayer: transformer_blocks.1.layer_norm2.weight | Grad: None\nLayer: transformer_blocks.1.layer_norm2.bias | Grad: None\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad: None\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad: None\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad: None\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad: None\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad: None\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad: None\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad: None\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad: None\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad: None\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad: None\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad: None\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad: None\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad: None\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad: None\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad: None\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad: None\nLayer: transformer_blocks.2.mlp.0.weight | Grad: None\nLayer: transformer_blocks.2.mlp.0.bias | Grad: None\nLayer: transformer_blocks.2.mlp.3.weight | Grad: None\nLayer: transformer_blocks.2.mlp.3.bias | Grad: None\nLayer: transformer_blocks.2.mlp.4.weight | Grad: None\nLayer: transformer_blocks.2.mlp.4.bias | Grad: None\nLayer: transformer_blocks.2.layer_norm2.weight | Grad: None\nLayer: transformer_blocks.2.layer_norm2.bias | Grad: None\nLayer: transformer_blocks.3.input_self_attention.in_proj_weight | Grad Norm: 0.34164661169052124\nLayer: transformer_blocks.3.input_self_attention.in_proj_bias | Grad Norm: 0.009814354591071606\nLayer: transformer_blocks.3.input_self_attention.out_proj.weight | Grad Norm: 0.04593345522880554\nLayer: transformer_blocks.3.input_self_attention.out_proj.bias | Grad Norm: 0.0015569080132991076\nLayer: transformer_blocks.3.context_attn.in_proj_weight | Grad Norm: 0.086506687104702\nLayer: transformer_blocks.3.context_attn.in_proj_bias | Grad Norm: 0.04869728907942772\nLayer: transformer_blocks.3.context_attn.out_proj.weight | Grad Norm: 0.028950558975338936\nLayer: transformer_blocks.3.context_attn.out_proj.bias | Grad Norm: 0.014973996207118034\nLayer: transformer_blocks.3.persona_attn.in_proj_weight | Grad Norm: 0.060747239738702774\nLayer: transformer_blocks.3.persona_attn.in_proj_bias | Grad Norm: 0.002405022270977497\nLayer: transformer_blocks.3.persona_attn.out_proj.weight | Grad Norm: 0.08388424664735794\nLayer: transformer_blocks.3.persona_attn.out_proj.bias | Grad Norm: 0.0036938763223588467\nLayer: transformer_blocks.3.paa_layer.fc.weight | Grad Norm: 0.09587253630161285\nLayer: transformer_blocks.3.paa_layer.fc.bias | Grad Norm: 0.0014396533370018005\nLayer: transformer_blocks.3.paa_layer.fc_out.weight | Grad Norm: 0.19589567184448242\nLayer: transformer_blocks.3.paa_layer.fc_out.bias | Grad Norm: 0.013385673053562641\nLayer: transformer_blocks.3.mlp.0.weight | Grad Norm: 0.09473924338817596\nLayer: transformer_blocks.3.mlp.0.bias | Grad Norm: 0.005566721316426992\nLayer: transformer_blocks.3.mlp.3.weight | Grad Norm: 0.21640415489673615\nLayer: transformer_blocks.3.mlp.3.bias | Grad Norm: 0.01774616539478302\nLayer: transformer_blocks.3.mlp.4.weight | Grad Norm: 0.010182980448007584\nLayer: transformer_blocks.3.mlp.4.bias | Grad Norm: 0.012053946033120155\nLayer: transformer_blocks.3.layer_norm2.weight | Grad Norm: 0.018506402149796486\nLayer: transformer_blocks.3.layer_norm2.bias | Grad Norm: 0.01960311457514763\nLayer: final_fc.weight | Grad Norm: 0.8678880929946899\nLayer: final_fc.bias | Grad Norm: 0.03242300823330879\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [1/200] completed | Average Loss: 4.4754\nTraining started for epoch 2/200\nEpoch [2/200] completed | Average Loss: 4.4200\nTraining started for epoch 3/200\nEpoch [3/200] completed | Average Loss: 4.3965\nTraining started for epoch 4/200\nEpoch [4/200] completed | Average Loss: 4.3812\nTraining started for epoch 5/200\nEpoch [5/200] completed | Average Loss: 4.3686\nTraining started for epoch 6/200\nEpoch [6/200] completed | Average Loss: 4.3568\nTraining started for epoch 7/200\nEpoch [7/200] completed | Average Loss: 4.3457\nTraining started for epoch 8/200\nEpoch [8/200] completed | Average Loss: 4.3329\nTraining started for epoch 9/200\nEpoch [9/200] completed | Average Loss: 4.3215\nTraining started for epoch 10/200\nEpoch [10/200] completed | Average Loss: 4.3081\nTraining started for epoch 11/200\nLayer: token_embedding.weight | Grad Norm: 0.007531884592026472\nLayer: position_embedding.weight | Grad Norm: 0.006503263488411903\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad: None\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad: None\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad: None\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad: None\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad: None\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad: None\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad: None\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad: None\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad: None\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad: None\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad: None\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad: None\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad: None\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad: None\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad: None\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad: None\nLayer: transformer_blocks.0.mlp.0.weight | Grad: None\nLayer: transformer_blocks.0.mlp.0.bias | Grad: None\nLayer: transformer_blocks.0.mlp.3.weight | Grad: None\nLayer: transformer_blocks.0.mlp.3.bias | Grad: None\nLayer: transformer_blocks.0.mlp.4.weight | Grad: None\nLayer: transformer_blocks.0.mlp.4.bias | Grad: None\nLayer: transformer_blocks.0.layer_norm2.weight | Grad: None\nLayer: transformer_blocks.0.layer_norm2.bias | Grad: None\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad: None\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad: None\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad: None\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad: None\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad: None\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad: None\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad: None\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad: None\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad: None\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad: None\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad: None\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad: None\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad: None\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad: None\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad: None\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad: None\nLayer: transformer_blocks.1.mlp.0.weight | Grad: None\nLayer: transformer_blocks.1.mlp.0.bias | Grad: None\nLayer: transformer_blocks.1.mlp.3.weight | Grad: None\nLayer: transformer_blocks.1.mlp.3.bias | Grad: None\nLayer: transformer_blocks.1.mlp.4.weight | Grad: None\nLayer: transformer_blocks.1.mlp.4.bias | Grad: None\nLayer: transformer_blocks.1.layer_norm2.weight | Grad: None\nLayer: transformer_blocks.1.layer_norm2.bias | Grad: None\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad: None\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad: None\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad: None\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad: None\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad: None\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad: None\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad: None\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad: None\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad: None\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad: None\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad: None\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad: None\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad: None\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad: None\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad: None\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad: None\nLayer: transformer_blocks.2.mlp.0.weight | Grad: None\nLayer: transformer_blocks.2.mlp.0.bias | Grad: None\nLayer: transformer_blocks.2.mlp.3.weight | Grad: None\nLayer: transformer_blocks.2.mlp.3.bias | Grad: None\nLayer: transformer_blocks.2.mlp.4.weight | Grad: None\nLayer: transformer_blocks.2.mlp.4.bias | Grad: None\nLayer: transformer_blocks.2.layer_norm2.weight | Grad: None\nLayer: transformer_blocks.2.layer_norm2.bias | Grad: None\nLayer: transformer_blocks.3.input_self_attention.in_proj_weight | Grad Norm: 0.1760217845439911\nLayer: transformer_blocks.3.input_self_attention.in_proj_bias | Grad Norm: 0.004197391681373119\nLayer: transformer_blocks.3.input_self_attention.out_proj.weight | Grad Norm: 0.06001237407326698\nLayer: transformer_blocks.3.input_self_attention.out_proj.bias | Grad Norm: 0.0008353578159585595\nLayer: transformer_blocks.3.context_attn.in_proj_weight | Grad Norm: 0.0436566099524498\nLayer: transformer_blocks.3.context_attn.in_proj_bias | Grad Norm: 0.026466835290193558\nLayer: transformer_blocks.3.context_attn.out_proj.weight | Grad Norm: 0.028535349294543266\nLayer: transformer_blocks.3.context_attn.out_proj.bias | Grad Norm: 0.013774624094367027\nLayer: transformer_blocks.3.persona_attn.in_proj_weight | Grad Norm: 0.07021602243185043\nLayer: transformer_blocks.3.persona_attn.in_proj_bias | Grad Norm: 0.0028343938756734133\nLayer: transformer_blocks.3.persona_attn.out_proj.weight | Grad Norm: 0.10606879740953445\nLayer: transformer_blocks.3.persona_attn.out_proj.bias | Grad Norm: 0.0036447616294026375\nLayer: transformer_blocks.3.paa_layer.fc.weight | Grad Norm: 0.19994677603244781\nLayer: transformer_blocks.3.paa_layer.fc.bias | Grad Norm: 0.001227709697559476\nLayer: transformer_blocks.3.paa_layer.fc_out.weight | Grad Norm: 0.3398875296115875\nLayer: transformer_blocks.3.paa_layer.fc_out.bias | Grad Norm: 0.01357453502714634\nLayer: transformer_blocks.3.mlp.0.weight | Grad Norm: 0.2595716714859009\nLayer: transformer_blocks.3.mlp.0.bias | Grad Norm: 0.01118945237249136\nLayer: transformer_blocks.3.mlp.3.weight | Grad Norm: 0.27176913619041443\nLayer: transformer_blocks.3.mlp.3.bias | Grad Norm: 0.04902657866477966\nLayer: transformer_blocks.3.mlp.4.weight | Grad Norm: 0.010565642267465591\nLayer: transformer_blocks.3.mlp.4.bias | Grad Norm: 0.010240571573376656\nLayer: transformer_blocks.3.layer_norm2.weight | Grad Norm: 0.016491740942001343\nLayer: transformer_blocks.3.layer_norm2.bias | Grad Norm: 0.015516962856054306\nLayer: final_fc.weight | Grad Norm: 0.802780032157898\nLayer: final_fc.bias | Grad Norm: 0.02907857857644558\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [11/200] completed | Average Loss: 4.2945\nTraining started for epoch 12/200\nEpoch [12/200] completed | Average Loss: 4.2798\nTraining started for epoch 13/200\nBatch 4141/5000 | Loss: 3.7218\r","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), 'paamodel.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = PAAModel()\n\n# Load the saved state dict\nmodel.load_state_dict(torch.load('paamodel.pth'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_eval = df[9999:10000].reset_index(drop=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_eval","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_dataset = CustomDataset(df_eval, t5_tokenizer,gpt2_tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef inference(model,gpt2_tokenizer, t5_tokenizer, eval_dataset, device):\n    model.eval()\n    model.to(device) \n\n    metacognitive_vector_ids, problem_student_code_ids, problem_expected_code_ids,student_code_ids, target_ids = eval_dataset[0]\n\n    metacognitive_tensor = metacognitive_vector_ids.unsqueeze(0).to(device)  \n    problem_student_code_tensor = problem_student_code_ids.unsqueeze(0).to(device)\n    problem_expected_code_tensor = problem_expected_code_ids.unsqueeze(0).to(device)\n    target_tensor = target_ids.unsqueeze(0).to(device)\n\n    student_attention_mask = (problem_student_code_tensor != t5_tokenizer.pad_token_id).long().to(device)\n    context_attention_mask = (problem_expected_code_tensor != t5_tokenizer.pad_token_id).long().to(device)\n\n    \n    with torch.no_grad():         \n        \n        logits = model(\n            metacognitive_vector_ids=metacognitive_tensor,\n            problem_student_code_ids=problem_student_code_tensor,\n            problem_expected_code_ids=problem_expected_code_tensor,\n            context_attention_mask=context_attention_mask,\n            student_attention_mask=student_attention_mask\n        )\n        \n        predictions = logits.argmax(dim=-1).squeeze().tolist()  \n        decoded_text = t5_tokenizer.decode(predictions, skip_special_tokens=True)\n\n        \n        return predictions, decoded_text\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions, decoded_text = inference(model, gpt2_tokenizer, t5_tokenizer, eval_dataset, device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Predicted Tokens:\", predictions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Decoded Text:\", decoded_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_checkpoint(checkpoint_path, model, optimizer=None):\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    loss = checkpoint['loss']\n    print(f\"Checkpoint loaded: Epoch {epoch}, Loss: {loss:.4f}\")\n    return model, optimizer, epoch, loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint_path = \"./checkpoints/model_epoch_89.pth\"\nmodel, optimizer, start_epoch, _ = load_checkpoint(checkpoint_path, model, optimizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint_path = \"./checkpoints/model_epoch_10.pth\"\ncheckpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))  # Use GPU if available: 'cuda'\nmodel.load_state_dict(checkpoint['model_state_dict'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_eval1 = df[449:450].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:22:21.77148Z","iopub.execute_input":"2024-12-26T19:22:21.771768Z","iopub.status.idle":"2024-12-26T19:22:21.776543Z","shell.execute_reply.started":"2024-12-26T19:22:21.771748Z","shell.execute_reply":"2024-12-26T19:22:21.775528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_dataset1 = CustomDataset(df_eval1, t5_tokenizer,gpt2_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:22:26.49086Z","iopub.execute_input":"2024-12-26T19:22:26.491157Z","iopub.status.idle":"2024-12-26T19:22:26.495243Z","shell.execute_reply.started":"2024-12-26T19:22:26.491134Z","shell.execute_reply":"2024-12-26T19:22:26.494174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions, decoded_text = inference(model, gpt2_tokenizer, t5_tokenizer, eval_dataset1, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:22:39.226221Z","iopub.execute_input":"2024-12-26T19:22:39.226559Z","iopub.status.idle":"2024-12-26T19:22:39.583175Z","shell.execute_reply.started":"2024-12-26T19:22:39.226534Z","shell.execute_reply":"2024-12-26T19:22:39.582494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Decoded Text:\", decoded_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:22:43.546251Z","iopub.execute_input":"2024-12-26T19:22:43.546672Z","iopub.status.idle":"2024-12-26T19:22:43.552108Z","shell.execute_reply.started":"2024-12-26T19:22:43.546636Z","shell.execute_reply":"2024-12-26T19:22:43.551007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}