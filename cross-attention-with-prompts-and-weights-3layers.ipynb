{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10048507,"sourceType":"datasetVersion","datasetId":6190931},{"sourceId":10242967,"sourceType":"datasetVersion","datasetId":6334477},{"sourceId":10316683,"sourceType":"datasetVersion","datasetId":6386966}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/cross-attentive-adaptaion-1-42e20c40-7d10-4162-a18f-309d0acc6c84.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20250102/auto/storage/goog4_request&X-Goog-Date=20250102T105331Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=92b18c91ac0391ce0f352019d957ea52f7551e139baab2f4c2d081e7b00cb3f45fdac366f022f08f3e38baf8177fd795fc8c3fbd8cad5cf56a5768e1f52ecd6947542b4909e5a9704e2c78165792da412f67a2b5534420e56e9210a50dc822df79a0f3bb4d052e68727023669ed37f2efb85edb1d0d5fdf1d1e463a7004504deba440307442799c3c2b8c08fe8201d4f5c87f259697732dcad9ac39bece48fa8c1893336ef7d31b901d26c6edfc0d56ea8bdc41c37a1801047f9dba70ca1e3df6573910e4c014ed26b38930e479a44a1e54ca15f5838a9a6213b67a6a320eb0377e57092dc5909688f060cd8995b0bfa7bcccc11ea2ad546230a88177d6e0307","timestamp":1736008492951}],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"43da2f47170345cdac7f72b7aa84662d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_89cfdd8672f548ca8d8722b5c39c240e","IPY_MODEL_a4ecd892759c4a759d069e293e15b779","IPY_MODEL_9aa1e481e9ad45678462f48428086ace"],"layout":"IPY_MODEL_f3027b6fbe7048378dd09030b5bd55d5"}},"89cfdd8672f548ca8d8722b5c39c240e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e0f7c835fd643c98686a30e7a7062fe","placeholder":"​","style":"IPY_MODEL_ad0642d1ee6e4d5381fb0fa34a4cf092","value":"config.json: 100%"}},"a4ecd892759c4a759d069e293e15b779":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d36ea6e57e142c1b40a79c8f4b8f6de","max":1208,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26e3066c9eb940c297966090d0d235d8","value":1208}},"9aa1e481e9ad45678462f48428086ace":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_feee58e7de1340e3860c9a95ff4be60e","placeholder":"​","style":"IPY_MODEL_2976f27258574b5d867dfdcbe16f9769","value":" 1.21k/1.21k [00:00&lt;00:00, 53.0kB/s]"}},"f3027b6fbe7048378dd09030b5bd55d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e0f7c835fd643c98686a30e7a7062fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad0642d1ee6e4d5381fb0fa34a4cf092":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d36ea6e57e142c1b40a79c8f4b8f6de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26e3066c9eb940c297966090d0d235d8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"feee58e7de1340e3860c9a95ff4be60e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2976f27258574b5d867dfdcbe16f9769":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"749891e75dea4320a86201f0d4e19c16":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3958951756e54804a17eae58aa6ba0dc","IPY_MODEL_1255530944b14d6099c53a92b2bb2bff","IPY_MODEL_eeb45b3f646a49f0bdee3d80c96685f5"],"layout":"IPY_MODEL_e2f860e4136747159aa03f5d0a63d368"}},"3958951756e54804a17eae58aa6ba0dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e539e4ce810e416986a5fa7f70494395","placeholder":"​","style":"IPY_MODEL_1d3f7e377c3348948b55485857e699f1","value":"spiece.model: 100%"}},"1255530944b14d6099c53a92b2bb2bff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3add81bdfdc2431cb7e774e4fa914292","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f85aef5dcd64053acd7128663657eb0","value":791656}},"eeb45b3f646a49f0bdee3d80c96685f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9fc7544d1fc4848a6161195104e1254","placeholder":"​","style":"IPY_MODEL_ee7e1d8f427d4b698a0ad3fa19904269","value":" 792k/792k [00:00&lt;00:00, 3.72MB/s]"}},"e2f860e4136747159aa03f5d0a63d368":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e539e4ce810e416986a5fa7f70494395":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d3f7e377c3348948b55485857e699f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3add81bdfdc2431cb7e774e4fa914292":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f85aef5dcd64053acd7128663657eb0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a9fc7544d1fc4848a6161195104e1254":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee7e1d8f427d4b698a0ad3fa19904269":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e04aa00ec6bf4c4cac5f87cf82a27c54":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7ec5c7b696ff4d7098aa0681b6ca625c","IPY_MODEL_9d530a87310d4c47bfa18a06fca19e17","IPY_MODEL_35a0e3310300473b97150e41c0b23339"],"layout":"IPY_MODEL_2d05e85d3811459e92522fe7c44cc95a"}},"7ec5c7b696ff4d7098aa0681b6ca625c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_744b2bc8bdb346c79694d3164fb8ceec","placeholder":"​","style":"IPY_MODEL_d03e94b61e1840e3b7f0ec3c6ae37b73","value":"tokenizer.json: 100%"}},"9d530a87310d4c47bfa18a06fca19e17":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_da66806b1f8f4a8faacf076045688d5f","max":1389353,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f000bf67102b4b0bb9322d272114958b","value":1389353}},"35a0e3310300473b97150e41c0b23339":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1770b5d95b514882bdc265bada08474f","placeholder":"​","style":"IPY_MODEL_6f0779e020d44f9396ca5294b3d3975a","value":" 1.39M/1.39M [00:00&lt;00:00, 6.71MB/s]"}},"2d05e85d3811459e92522fe7c44cc95a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"744b2bc8bdb346c79694d3164fb8ceec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d03e94b61e1840e3b7f0ec3c6ae37b73":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da66806b1f8f4a8faacf076045688d5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f000bf67102b4b0bb9322d272114958b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1770b5d95b514882bdc265bada08474f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f0779e020d44f9396ca5294b3d3975a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b4b9829c79e4df2b7f6159d4a54634a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7574a2d7e4984e01ae8043545e7f3afb","IPY_MODEL_b28caa6bf2c745ec87cf3da06ecf4a7a","IPY_MODEL_cd57ad7ae6aa4ced911bf44e8b5437d0"],"layout":"IPY_MODEL_5c3722f465704e12bd93bb12220b42e3"}},"7574a2d7e4984e01ae8043545e7f3afb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c98216f497743dcb8abb7b69be97942","placeholder":"​","style":"IPY_MODEL_9c817b66deb1488b95479ae5667b71cf","value":"tokenizer_config.json: 100%"}},"b28caa6bf2c745ec87cf3da06ecf4a7a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7e2b480b24244919ec20c23720905a7","max":26,"min":0,"orientation":"horizontal","style":"IPY_MODEL_214c9d061b734b03bd0060ef66fb8c02","value":26}},"cd57ad7ae6aa4ced911bf44e8b5437d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_770dcb4cf22d4035a8b8970aed70412b","placeholder":"​","style":"IPY_MODEL_17209e71fc73454b99b23f617c5643d0","value":" 26.0/26.0 [00:00&lt;00:00, 465B/s]"}},"5c3722f465704e12bd93bb12220b42e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c98216f497743dcb8abb7b69be97942":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c817b66deb1488b95479ae5667b71cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7e2b480b24244919ec20c23720905a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"214c9d061b734b03bd0060ef66fb8c02":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"770dcb4cf22d4035a8b8970aed70412b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17209e71fc73454b99b23f617c5643d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3aa1fd17abe44555ad47e0aef4c0f938":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1bd6bdbc2e204509b9c209417733f4a3","IPY_MODEL_b8ac8ee34b73426190aebf20a0035eae","IPY_MODEL_4cf598feee2b4c71938150e26c9cb341"],"layout":"IPY_MODEL_0ceaa855fc724594bf8c64a8a1926e01"}},"1bd6bdbc2e204509b9c209417733f4a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d80c6058f22a47cba18ed8e9eb1918ae","placeholder":"​","style":"IPY_MODEL_13f8d88212184e9889255c7e6f3f6dfd","value":"vocab.json: 100%"}},"b8ac8ee34b73426190aebf20a0035eae":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_de559a29729142c2ae3a9641c96478c4","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6df5febabe3c4062ab1c055fbea5a62b","value":1042301}},"4cf598feee2b4c71938150e26c9cb341":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfa0c3241c3f4d9088aa3a8ea8b45f70","placeholder":"​","style":"IPY_MODEL_ebcecc565e484bf0933b3910d4566726","value":" 1.04M/1.04M [00:00&lt;00:00, 5.02MB/s]"}},"0ceaa855fc724594bf8c64a8a1926e01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d80c6058f22a47cba18ed8e9eb1918ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13f8d88212184e9889255c7e6f3f6dfd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de559a29729142c2ae3a9641c96478c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6df5febabe3c4062ab1c055fbea5a62b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dfa0c3241c3f4d9088aa3a8ea8b45f70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebcecc565e484bf0933b3910d4566726":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3809873b8cf045c2a00ba0d62261f6af":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3d86694625d04af094c9beef8a589b16","IPY_MODEL_d5f9e809db154bbfaf908195b325a34a","IPY_MODEL_fc953cc050dd4280846cd6e1219019e7"],"layout":"IPY_MODEL_9c27818ce29f4d4c97bc4b55efad78e5"}},"3d86694625d04af094c9beef8a589b16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_617a9be3796a4ec8a0377b82f497635e","placeholder":"​","style":"IPY_MODEL_8309e9644c074f83ab4f4fd9a99582c6","value":"merges.txt: 100%"}},"d5f9e809db154bbfaf908195b325a34a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d533ecc7ab649aaa98a0712db11376c","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_357500baa89547c39282e086ab7a7999","value":456318}},"fc953cc050dd4280846cd6e1219019e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_942420c6462c47ca8a0e09321e60a1b8","placeholder":"​","style":"IPY_MODEL_9f5ed38aa7a94a00b373febd383104ca","value":" 456k/456k [00:00&lt;00:00, 13.6MB/s]"}},"9c27818ce29f4d4c97bc4b55efad78e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"617a9be3796a4ec8a0377b82f497635e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8309e9644c074f83ab4f4fd9a99582c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d533ecc7ab649aaa98a0712db11376c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"357500baa89547c39282e086ab7a7999":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"942420c6462c47ca8a0e09321e60a1b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f5ed38aa7a94a00b373febd383104ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e801240ba7b42378f27c301c0fcac29":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7b0cabdc4da749228879868a6f7e34e3","IPY_MODEL_9fabc1f28c5248d8a9ced22a2f37890c","IPY_MODEL_95879c33ac2c4568a8b1fbe4f4004009"],"layout":"IPY_MODEL_421fb5dee92b40e0b94ac250bdc13adc"}},"7b0cabdc4da749228879868a6f7e34e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbf744dc7b674882b25a67ececf38fe3","placeholder":"​","style":"IPY_MODEL_ff2f0600478f4801a2243cb634722196","value":"tokenizer.json: 100%"}},"9fabc1f28c5248d8a9ced22a2f37890c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0bffe6826a9406dad9fbc75e1dc99e2","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e1c977ccc074a42b090867b860297b9","value":1355256}},"95879c33ac2c4568a8b1fbe4f4004009":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f73dfe68ceb4c2689ad142e0664340a","placeholder":"​","style":"IPY_MODEL_ebeab6709a6e40e58a661585be171203","value":" 1.36M/1.36M [00:00&lt;00:00, 6.66MB/s]"}},"421fb5dee92b40e0b94ac250bdc13adc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbf744dc7b674882b25a67ececf38fe3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff2f0600478f4801a2243cb634722196":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0bffe6826a9406dad9fbc75e1dc99e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e1c977ccc074a42b090867b860297b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3f73dfe68ceb4c2689ad142e0664340a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebeab6709a6e40e58a661585be171203":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44d5dfde2cea49ffb47e9fa82d182502":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_78c2f42c86e644469f1697ace4f1e0d1","IPY_MODEL_bde9128d864947c7918a2192aa8bbdcb","IPY_MODEL_cf1a5f16f250435082e836611374688a"],"layout":"IPY_MODEL_c9185e9b796c4e14be13dc4ebb223e1b"}},"78c2f42c86e644469f1697ace4f1e0d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e24b589fab34b389b4b7b1bab26f624","placeholder":"​","style":"IPY_MODEL_5a6158b3e2934c5d9e2b15c2f6951031","value":"config.json: 100%"}},"bde9128d864947c7918a2192aa8bbdcb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aaa6919585e543969a70349ec8f870da","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9140609d187f48869561947744cda081","value":665}},"cf1a5f16f250435082e836611374688a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bb1f013da664d34a26d042c062d8336","placeholder":"​","style":"IPY_MODEL_ee1033c0ac7245358b4aaeb3ae3e2502","value":" 665/665 [00:00&lt;00:00, 15.5kB/s]"}},"c9185e9b796c4e14be13dc4ebb223e1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e24b589fab34b389b4b7b1bab26f624":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a6158b3e2934c5d9e2b15c2f6951031":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aaa6919585e543969a70349ec8f870da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9140609d187f48869561947744cda081":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7bb1f013da664d34a26d042c062d8336":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee1033c0ac7245358b4aaeb3ae3e2502":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n# THEN FEEL FREE TO DELETE THIS CELL.\n# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# NOTEBOOK.\nimport kagglehub\nuom200407h_modified_dataset_path = kagglehub.dataset_download('uom200407h/modified-dataset')\nuom200644f_metacognitive_dataset_path = kagglehub.dataset_download('uom200644f/metacognitive-dataset')\nbashadithennakoon_metacognitive_feedback_for_algorithm_solving_path = kagglehub.dataset_download('bashadithennakoon/metacognitive-feedback-for-algorithm-solving')\n\nprint('Data source import complete.')\n","metadata":{"id":"WZPr7xkva7Id"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"XOEjCcHQa7Ii","execution":{"iopub.status.busy":"2025-01-10T14:48:57.306143Z","iopub.execute_input":"2025-01-10T14:48:57.306455Z","iopub.status.idle":"2025-01-10T14:48:57.627223Z","shell.execute_reply.started":"2025-01-10T14:48:57.306430Z","shell.execute_reply":"2025-01-10T14:48:57.626472Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/metacognitive-feedback-for-algorithm-solving/final_dataset_with_annotated_metacognitive_feedback_gpt-4o-mini.csv\n/kaggle/input/modified-dataset/modified_dataset.csv\n/kaggle/input/metacognitive-dataset/metacognitive-dataset.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install transformers torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:48:58.958851Z","iopub.execute_input":"2025-01-10T14:48:58.959524Z","iopub.status.idle":"2025-01-10T14:49:03.421447Z","shell.execute_reply.started":"2025-01-10T14:48:58.959478Z","shell.execute_reply":"2025-01-10T14:49:03.420236Z"},"id":"TYCjdbZUa7Ij","executionInfo":{"status":"ok","timestamp":1736007590391,"user_tz":-330,"elapsed":3960,"user":{"displayName":"","userId":""}},"outputId":"1854ce93-b297-4c35-e5fc-59e5bbd37f61"},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import AutoModel, AutoTokenizer, GPT2Model,GPT2Tokenizer,GPT2LMHeadModel\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration , T5Tokenizer , T5Model\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:03.422958Z","iopub.execute_input":"2025-01-10T14:49:03.423275Z","iopub.status.idle":"2025-01-10T14:49:08.367547Z","shell.execute_reply.started":"2025-01-10T14:49:03.423250Z","shell.execute_reply":"2025-01-10T14:49:08.366854Z"},"id":"IgJFcjXoa7Il"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:08.368791Z","iopub.execute_input":"2025-01-10T14:49:08.369234Z","iopub.status.idle":"2025-01-10T14:49:08.426993Z","shell.execute_reply.started":"2025-01-10T14:49:08.369210Z","shell.execute_reply":"2025-01-10T14:49:08.425961Z"},"id":"DpWbDwTja7Im"},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:08.428696Z","iopub.execute_input":"2025-01-10T14:49:08.429052Z","iopub.status.idle":"2025-01-10T14:49:08.449776Z","shell.execute_reply.started":"2025-01-10T14:49:08.429027Z","shell.execute_reply":"2025-01-10T14:49:08.448924Z"},"id":"kdvnV1Efa7Im","executionInfo":{"status":"ok","timestamp":1736007616811,"user_tz":-330,"elapsed":5,"user":{"displayName":"","userId":""}},"outputId":"802e1d3c-dc5d-41ac-d16a-f50ffb3dacf8"},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"checkpoint = \"t5-base\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:08.450558Z","iopub.execute_input":"2025-01-10T14:49:08.450871Z","iopub.status.idle":"2025-01-10T14:49:08.463495Z","shell.execute_reply.started":"2025-01-10T14:49:08.450849Z","shell.execute_reply":"2025-01-10T14:49:08.462709Z"},"id":"OvfQPyhKa7In"},"outputs":[],"execution_count":6},{"cell_type":"code","source":"t5_tokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:08.464236Z","iopub.execute_input":"2025-01-10T14:49:08.464438Z","iopub.status.idle":"2025-01-10T14:49:09.826931Z","shell.execute_reply.started":"2025-01-10T14:49:08.464420Z","shell.execute_reply":"2025-01-10T14:49:09.825987Z"},"id":"EPXot-9Sa7In","executionInfo":{"status":"ok","timestamp":1736007620230,"user_tz":-330,"elapsed":3422,"user":{"displayName":"","userId":""}},"outputId":"c202dd01-1b26-4400-cd53-9c6f676a8c2a"},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f8a47e5a4924c74ae35245d3f03eb2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54d54a1d861644358474354fb23be7c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a903b0cded6d40c08d33ee09a6ad2f73"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"t5_tokenizer.vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:09.827795Z","iopub.execute_input":"2025-01-10T14:49:09.828012Z","iopub.status.idle":"2025-01-10T14:49:09.832568Z","shell.execute_reply.started":"2025-01-10T14:49:09.827993Z","shell.execute_reply":"2025-01-10T14:49:09.831882Z"},"id":"qX1l99_3a7Io","executionInfo":{"status":"ok","timestamp":1736007620230,"user_tz":-330,"elapsed":3,"user":{"displayName":"","userId":""}},"outputId":"803d67af-d77e-4ecf-8be7-b3813bdd5551"},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"32100"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"#set the max length to model's default present max length\nt5_tokenizer.model_max_length = t5_tokenizer.model_max_length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:09.834232Z","iopub.execute_input":"2025-01-10T14:49:09.834437Z","iopub.status.idle":"2025-01-10T14:49:09.847858Z","shell.execute_reply.started":"2025-01-10T14:49:09.834419Z","shell.execute_reply":"2025-01-10T14:49:09.847080Z"},"id":"9XRWaOWWa7Ip"},"outputs":[],"execution_count":9},{"cell_type":"code","source":"gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:09.848802Z","iopub.execute_input":"2025-01-10T14:49:09.849098Z","iopub.status.idle":"2025-01-10T14:49:11.227889Z","shell.execute_reply.started":"2025-01-10T14:49:09.849068Z","shell.execute_reply":"2025-01-10T14:49:11.227166Z"},"id":"tLqjpzCxa7Ip","executionInfo":{"status":"ok","timestamp":1736007623206,"user_tz":-330,"elapsed":2269,"user":{"displayName":"","userId":""}},"outputId":"fd776e6d-245d-4ab9-cc78-99e969b0a830"},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78e0d650c1784245aca09c496d2a2ae7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f045aa1902224959b046fcb6a243d51b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb31ba7dae4d44b78d842129a2f6a9f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86ba5ea8dcb747f99653f4f971747230"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d385d660f3664b0ea14bf6eda0d03171"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:11.228977Z","iopub.execute_input":"2025-01-10T14:49:11.229194Z","iopub.status.idle":"2025-01-10T14:49:11.232906Z","shell.execute_reply.started":"2025-01-10T14:49:11.229171Z","shell.execute_reply":"2025-01-10T14:49:11.232071Z"},"id":"LVVTv7zfa7Iq"},"outputs":[],"execution_count":11},{"cell_type":"code","source":"file_path = \"/kaggle/input/metacognitive-feedback-for-algorithm-solving/final_dataset_with_annotated_metacognitive_feedback_gpt-4o-mini.csv\"\ndf = pd.read_csv(file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:11.234102Z","iopub.execute_input":"2025-01-10T14:49:11.234397Z","iopub.status.idle":"2025-01-10T14:49:13.299452Z","shell.execute_reply.started":"2025-01-10T14:49:11.234371Z","shell.execute_reply":"2025-01-10T14:49:13.298710Z"},"id":"XrY_OzWca7Iq","executionInfo":{"status":"error","timestamp":1736007624024,"user_tz":-330,"elapsed":822,"user":{"displayName":"","userId":""}},"outputId":"3eb44ee1-99fa-46b6-becc-3de1a7578cb1"},"outputs":[],"execution_count":12},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:13.300265Z","iopub.execute_input":"2025-01-10T14:49:13.300504Z","iopub.status.idle":"2025-01-10T14:49:13.306406Z","shell.execute_reply.started":"2025-01-10T14:49:13.300484Z","shell.execute_reply":"2025-01-10T14:49:13.305797Z"},"id":"nqzlVJm_a7Ir"},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Index(['Question 1', 'Response 1', 'Right answer 1', 'Q01', 'Q02', 'Q03',\n       'Q04', 'Q05', 'Q06', 'Q07', 'Q08', 'Q09', 'Q10', 'Q11', 'Q12', 'Q13',\n       'Q14', 'Q15', 'Q16', 'metacognitive_vector', 'metacognitive_feedback'],\n      dtype='object')"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"df.rename(\n    columns={\n        'Question 1': 'Problem',\n        'Response 1': 'Student_code',\n        'Right answer 1': 'Expected_code'\n    },\n    inplace=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:13.307564Z","iopub.execute_input":"2025-01-10T14:49:13.307808Z","iopub.status.idle":"2025-01-10T14:49:13.326416Z","shell.execute_reply.started":"2025-01-10T14:49:13.307788Z","shell.execute_reply":"2025-01-10T14:49:13.325677Z"},"id":"ugJYy8pVa7Ir"},"outputs":[],"execution_count":14},{"cell_type":"code","source":"df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:13.895570Z","iopub.execute_input":"2025-01-10T14:49:13.895904Z","iopub.status.idle":"2025-01-10T14:49:13.917662Z","shell.execute_reply.started":"2025-01-10T14:49:13.895877Z","shell.execute_reply":"2025-01-10T14:49:13.916945Z"},"id":"mcjarOz3a7Is"},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                             Problem  \\\n0  Develop a Python program that takes the name o...   \n1  Develop a Python program that takes the name o...   \n2  Develop a Python program that takes the name o...   \n\n                                        Student_code  \\\n0  file_input = input()      file_open = open(fil...   \n1  file_input = input()      file_open = open(fil...   \n2  file_input = input()      file_open = open(fil...   \n\n                                       Expected_code        Q01  \\\n0  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n1  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n2  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n\n             Q02               Q03        Q04            Q05            Q06  \\\n0  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n1  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n2  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n\n                Q07  ...        Q09        Q10            Q11            Q12  \\\n0  1 : Almost Never  ...  3 : Often  3 : Often  2 : Sometimes  2 : Sometimes   \n1  1 : Almost Never  ...  3 : Often  3 : Often  2 : Sometimes  2 : Sometimes   \n2  1 : Almost Never  ...  3 : Often  3 : Often  2 : Sometimes  2 : Sometimes   \n\n             Q13        Q14        Q15               Q16  \\\n0  2 : Sometimes  3 : Often  3 : Often  1 : Almost Never   \n1  2 : Sometimes  3 : Often  3 : Often  1 : Almost Never   \n2  2 : Sometimes  3 : Often  3 : Often  1 : Almost Never   \n\n                                metacognitive_vector  \\\n0  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n1  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n2  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n\n                              metacognitive_feedback  \n0  Your initial code serves as a starting point, ...  \n1  Your code exhibits a solid attempt at reading ...  \n2  It looks like you're in a good place with some...  \n\n[3 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Problem</th>\n      <th>Student_code</th>\n      <th>Expected_code</th>\n      <th>Q01</th>\n      <th>Q02</th>\n      <th>Q03</th>\n      <th>Q04</th>\n      <th>Q05</th>\n      <th>Q06</th>\n      <th>Q07</th>\n      <th>...</th>\n      <th>Q09</th>\n      <th>Q10</th>\n      <th>Q11</th>\n      <th>Q12</th>\n      <th>Q13</th>\n      <th>Q14</th>\n      <th>Q15</th>\n      <th>Q16</th>\n      <th>metacognitive_vector</th>\n      <th>metacognitive_feedback</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>Your initial code serves as a starting point, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>Your code exhibits a solid attempt at reading ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>It looks like you're in a good place with some...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 21 columns</p>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"student_prompt = \"This is the combination of the problem desccription and the student provided code.\"\ncontext_prompt = \"This is the combination of the problem description and the expected correct answer for that algorithm design question.\"\nmetacognitive_components = [\n                \"Read\", \"Identify\", \"Rephrase\", \"Examples\", \"Breakdown\", \"Estimate\", \"Plan\",\n                \"Revise\", \"Verify\", \"AvoidMistakes\", \"MonitorSteps\", \"MonitorProcess\",\n                \"ValidateConstraints\", \"Confirm\", \"CheckRequirements\", \"Reflect\"\n            ]\n\nmetacognition_prompt = (\n            f\"Metacognitive feedback helps students reflect on their algorithm-solving strategies. \"\n            f\"The passed metacognitive vector is a 16-dimensional vector describing the student's metacognition profile:  \"\n            f\"where each dimension represents one of 16 metacognitive components, rated as 1 (Almost never), \"\n            f\"2 (Sometimes), or 3 (Often). These components include: {', '.join(metacognitive_components)}.\"\n        )\ndecoder_prompt = \"This is the combination student's metacognition profile and the student's code for the algorithm problem with the problem description itself.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:14.192173Z","iopub.execute_input":"2025-01-10T14:49:14.192481Z","iopub.status.idle":"2025-01-10T14:49:14.197141Z","shell.execute_reply.started":"2025-01-10T14:49:14.192457Z","shell.execute_reply":"2025-01-10T14:49:14.196148Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"df['combined_problem_student'] = student_prompt + \"  Problem:  \" + df['Problem'] + \".  Student provided code:  \" + df['Student_code']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:14.517933Z","iopub.execute_input":"2025-01-10T14:49:14.518252Z","iopub.status.idle":"2025-01-10T14:49:14.650500Z","shell.execute_reply.started":"2025-01-10T14:49:14.518225Z","shell.execute_reply":"2025-01-10T14:49:14.649478Z"},"id":"Z7eLV0EQa7Is"},"outputs":[],"execution_count":17},{"cell_type":"code","source":"df['combined_problem_expected'] = context_prompt + \" Problem:  \" + df['Problem'] + \".  Expected correct ansswer for the problem:  \" + df['Expected_code']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:17.198600Z","iopub.execute_input":"2025-01-10T14:49:17.198952Z","iopub.status.idle":"2025-01-10T14:49:17.298147Z","shell.execute_reply.started":"2025-01-10T14:49:17.198919Z","shell.execute_reply":"2025-01-10T14:49:17.297023Z"},"id":"7GpCPwv8a7Is"},"outputs":[],"execution_count":18},{"cell_type":"code","source":"df['combined_metacogntion_prompt'] =  metacognition_prompt + \"Here is the student's metacognition vector: \" + df['metacognitive_vector']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:17.516636Z","iopub.execute_input":"2025-01-10T14:49:17.517051Z","iopub.status.idle":"2025-01-10T14:49:17.527114Z","shell.execute_reply.started":"2025-01-10T14:49:17.517017Z","shell.execute_reply":"2025-01-10T14:49:17.526160Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# df['combined_persona_student'] = ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:17.847090Z","iopub.execute_input":"2025-01-10T14:49:17.847403Z","iopub.status.idle":"2025-01-10T14:49:17.851218Z","shell.execute_reply.started":"2025-01-10T14:49:17.847376Z","shell.execute_reply":"2025-01-10T14:49:17.850133Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"df['combined_metacogntion_prompt'][20]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:18.069188Z","iopub.execute_input":"2025-01-10T14:49:18.069513Z","iopub.status.idle":"2025-01-10T14:49:18.074668Z","shell.execute_reply.started":"2025-01-10T14:49:18.069485Z","shell.execute_reply":"2025-01-10T14:49:18.073824Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"\"Metacognitive feedback helps students reflect on their algorithm-solving strategies. The passed metacognitive vector is a 16-dimensional vector describing the student's metacognition profile:  where each dimension represents one of 16 metacognitive components, rated as 1 (Almost never), 2 (Sometimes), or 3 (Often). These components include: Read, Identify, Rephrase, Examples, Breakdown, Estimate, Plan, Revise, Verify, AvoidMistakes, MonitorSteps, MonitorProcess, ValidateConstraints, Confirm, CheckRequirements, Reflect.Here is the student's metacognition vector: ['2 ', '3 ', '2 ', '3 ', '2 ', '2 ', '1 ', '2 ', '3 ', '3 ', '3 ', '3 ', '3 ', '3 ', '2 ', '2 ']\""},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:20.316132Z","iopub.execute_input":"2025-01-10T14:49:20.316495Z","iopub.status.idle":"2025-01-10T14:49:20.321951Z","shell.execute_reply.started":"2025-01-10T14:49:20.316463Z","shell.execute_reply":"2025-01-10T14:49:20.320998Z"},"id":"-EMODMJCa7It"},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"Index(['Problem', 'Student_code', 'Expected_code', 'Q01', 'Q02', 'Q03', 'Q04',\n       'Q05', 'Q06', 'Q07', 'Q08', 'Q09', 'Q10', 'Q11', 'Q12', 'Q13', 'Q14',\n       'Q15', 'Q16', 'metacognitive_vector', 'metacognitive_feedback',\n       'combined_problem_student', 'combined_problem_expected',\n       'combined_metacogntion_prompt'],\n      dtype='object')"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"df.dropna(subset=['Problem', 'metacognitive_feedback', 'combined_problem_student','Student_code'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:20.579188Z","iopub.execute_input":"2025-01-10T14:49:20.579482Z","iopub.status.idle":"2025-01-10T14:49:20.604594Z","shell.execute_reply.started":"2025-01-10T14:49:20.579459Z","shell.execute_reply":"2025-01-10T14:49:20.603586Z"},"id":"94NeCGFua7It"},"outputs":[],"execution_count":23},{"cell_type":"code","source":"df.reset_index(drop=True, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:20.850829Z","iopub.execute_input":"2025-01-10T14:49:20.851136Z","iopub.status.idle":"2025-01-10T14:49:20.855004Z","shell.execute_reply.started":"2025-01-10T14:49:20.851113Z","shell.execute_reply":"2025-01-10T14:49:20.854056Z"},"id":"9wKnq90Ua7Iu"},"outputs":[],"execution_count":24},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:21.156197Z","iopub.execute_input":"2025-01-10T14:49:21.156485Z","iopub.status.idle":"2025-01-10T14:49:21.185660Z","shell.execute_reply.started":"2025-01-10T14:49:21.156464Z","shell.execute_reply":"2025-01-10T14:49:21.184806Z"},"id":"ZzXH3PICa7Iu"},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"Problem                         0\nStudent_code                    0\nExpected_code                   0\nQ01                             0\nQ02                             0\nQ03                             0\nQ04                             0\nQ05                             0\nQ06                             0\nQ07                             0\nQ08                             0\nQ09                             0\nQ10                             0\nQ11                             0\nQ12                             0\nQ13                             0\nQ14                             0\nQ15                             0\nQ16                             0\nmetacognitive_vector            0\nmetacognitive_feedback          0\ncombined_problem_student        0\ncombined_problem_expected       0\ncombined_metacogntion_prompt    0\ndtype: int64"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"df['metacognitive_feedback'][100]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:23.196376Z","iopub.execute_input":"2025-01-10T14:49:23.196717Z","iopub.status.idle":"2025-01-10T14:49:23.202056Z","shell.execute_reply.started":"2025-01-10T14:49:23.196689Z","shell.execute_reply":"2025-01-10T14:49:23.201194Z"},"id":"WtKweTkea7Iu"},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"\"Your current implementation shows a good effort in structuring your code with functions, but there are some areas where further refinement is necessary to meet the problem requirements. First, consider how you're capturing the relationships between birth years and heights. While you are correctly gathering names, birthdates, and heights into a dictionary, think about how you can aggregate heights by decade instead of year. This will streamline the calculation of average heights. It’s crucial to loop through your input list once to comprehend and process the data before beginning any calculations for averaging—possibly consolidating this logic in your `calculate_average_height` function. Also, ensure that you are converting heights to the correct data type before performing any arithmetic operations. Additionally, take a closer look at how you’re determining the range of decades; right now it seems like you may be focusing on unique years instead of decades. Consider creating a systematic structure that easily categorizes each height into its corresponding decade bucket. Furthermore, as you develop your code, remember to monitor your program's flow and adjust accordingly—this is especially important when it comes to function calls and data structure manipulations. Establishing a plan before implementation can significantly enhance your problem-solving process and avoid errors stemming from assumptions. Overall, maintain momentum and continue refining your approach by checking each component against the problem's requirements, thus ensuring coherence and completeness in your solution.\""},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:23.440989Z","iopub.execute_input":"2025-01-10T14:49:23.441320Z","iopub.status.idle":"2025-01-10T14:49:23.462229Z","shell.execute_reply.started":"2025-01-10T14:49:23.441292Z","shell.execute_reply":"2025-01-10T14:49:23.461567Z"},"id":"t_EBOaYQa7Iv"},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"                                             Problem  \\\n0  Develop a Python program that takes the name o...   \n1  Develop a Python program that takes the name o...   \n2  Develop a Python program that takes the name o...   \n3  Develop a Python program that takes the name o...   \n4  Develop a Python program that takes the name o...   \n\n                                        Student_code  \\\n0  file_input = input()      file_open = open(fil...   \n1  file_input = input()      file_open = open(fil...   \n2  file_input = input()      file_open = open(fil...   \n3  file_input = input()      file_open = open(fil...   \n4  file_input = input()      file_open = open(fil...   \n\n                                       Expected_code        Q01  \\\n0  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n1  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n2  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n3  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n4  def substitute_vowels(chunk, vowel_substitutes...  3 : Often   \n\n             Q02               Q03        Q04            Q05            Q06  \\\n0  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n1  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n2  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n3  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n4  2 : Sometimes  1 : Almost Never  3 : Often  2 : Sometimes  2 : Sometimes   \n\n                Q07  ...            Q12            Q13        Q14        Q15  \\\n0  1 : Almost Never  ...  2 : Sometimes  2 : Sometimes  3 : Often  3 : Often   \n1  1 : Almost Never  ...  2 : Sometimes  2 : Sometimes  3 : Often  3 : Often   \n2  1 : Almost Never  ...  2 : Sometimes  2 : Sometimes  3 : Often  3 : Often   \n3  1 : Almost Never  ...  2 : Sometimes  2 : Sometimes  3 : Often  3 : Often   \n4  1 : Almost Never  ...  2 : Sometimes  2 : Sometimes  3 : Often  3 : Often   \n\n                Q16                               metacognitive_vector  \\\n0  1 : Almost Never  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n1  1 : Almost Never  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n2  1 : Almost Never  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n3  1 : Almost Never  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n4  1 : Almost Never  ['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...   \n\n                              metacognitive_feedback  \\\n0  Your initial code serves as a starting point, ...   \n1  Your code exhibits a solid attempt at reading ...   \n2  It looks like you're in a good place with some...   \n3  Your approach to reading the file and splittin...   \n4  Your initial approach to the problem is a good...   \n\n                            combined_problem_student  \\\n0  This is the combination of the problem desccri...   \n1  This is the combination of the problem desccri...   \n2  This is the combination of the problem desccri...   \n3  This is the combination of the problem desccri...   \n4  This is the combination of the problem desccri...   \n\n                           combined_problem_expected  \\\n0  This is the combination of the problem descrip...   \n1  This is the combination of the problem descrip...   \n2  This is the combination of the problem descrip...   \n3  This is the combination of the problem descrip...   \n4  This is the combination of the problem descrip...   \n\n                        combined_metacogntion_prompt  \n0  Metacognitive feedback helps students reflect ...  \n1  Metacognitive feedback helps students reflect ...  \n2  Metacognitive feedback helps students reflect ...  \n3  Metacognitive feedback helps students reflect ...  \n4  Metacognitive feedback helps students reflect ...  \n\n[5 rows x 24 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Problem</th>\n      <th>Student_code</th>\n      <th>Expected_code</th>\n      <th>Q01</th>\n      <th>Q02</th>\n      <th>Q03</th>\n      <th>Q04</th>\n      <th>Q05</th>\n      <th>Q06</th>\n      <th>Q07</th>\n      <th>...</th>\n      <th>Q12</th>\n      <th>Q13</th>\n      <th>Q14</th>\n      <th>Q15</th>\n      <th>Q16</th>\n      <th>metacognitive_vector</th>\n      <th>metacognitive_feedback</th>\n      <th>combined_problem_student</th>\n      <th>combined_problem_expected</th>\n      <th>combined_metacogntion_prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>Your initial code serves as a starting point, ...</td>\n      <td>This is the combination of the problem desccri...</td>\n      <td>This is the combination of the problem descrip...</td>\n      <td>Metacognitive feedback helps students reflect ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>Your code exhibits a solid attempt at reading ...</td>\n      <td>This is the combination of the problem desccri...</td>\n      <td>This is the combination of the problem descrip...</td>\n      <td>Metacognitive feedback helps students reflect ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>It looks like you're in a good place with some...</td>\n      <td>This is the combination of the problem desccri...</td>\n      <td>This is the combination of the problem descrip...</td>\n      <td>Metacognitive feedback helps students reflect ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>Your approach to reading the file and splittin...</td>\n      <td>This is the combination of the problem desccri...</td>\n      <td>This is the combination of the problem descrip...</td>\n      <td>Metacognitive feedback helps students reflect ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_input = input()      file_open = open(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>['3 ', '2 ', '1 ', '3 ', '2 ', '2 ', '1 ', '3 ...</td>\n      <td>Your initial approach to the problem is a good...</td>\n      <td>This is the combination of the problem desccri...</td>\n      <td>This is the combination of the problem descrip...</td>\n      <td>Metacognitive feedback helps students reflect ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 24 columns</p>\n</div>"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport ast\nclass CustomDataset(Dataset):\n    def __init__(self, dataset, t5_tokenizer,gpt2_tokenizer, max_length=512):\n        self.t5_tokenizer = t5_tokenizer\n        self.gpt2_tokenizer = gpt2_tokenizer\n        self.data = dataset\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        metacognitive_prompt = self.data['combined_metacogntion_prompt'][idx]\n        problem_student_code = self.data['combined_problem_student'][idx]\n        problem_expected_code = self.data['combined_problem_expected'][idx]\n        student_code = self.data['Student_code'][idx]\n        target = self.data['metacognitive_feedback'][idx]\n\n        # metacognitive_vector_float = [\n        # float(item.strip()) for item in ast.literal_eval(metacognitive_vector)]\n        # metacognition_vector_ids = torch.tensor(metacognitive_vector_float, dtype=torch.float)\n        metacognition_prompt_ids = torch.tensor(\n            self.t5_tokenizer.encode(metacognitive_prompt, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n\n        problem_student_code_ids = torch.tensor(\n            self.t5_tokenizer.encode(problem_student_code, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n        problem_expected_code_ids = torch.tensor(\n            self.t5_tokenizer.encode(problem_expected_code, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n\n        student_code_ids = torch.tensor(\n            self.t5_tokenizer.encode(student_code, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n        target_ids = torch.tensor(\n            self.t5_tokenizer.encode(target, max_length=self.max_length, truncation=True, padding=\"max_length\")\n        )\n\n        return metacognition_prompt_ids, problem_student_code_ids, problem_expected_code_ids, student_code_ids, target_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:25.421410Z","iopub.execute_input":"2025-01-10T14:49:25.421730Z","iopub.status.idle":"2025-01-10T14:49:25.428782Z","shell.execute_reply.started":"2025-01-10T14:49:25.421707Z","shell.execute_reply":"2025-01-10T14:49:25.427817Z"},"id":"nNEuM_O3a7Iv"},"outputs":[],"execution_count":28},{"cell_type":"code","source":"dataset = CustomDataset(df, t5_tokenizer, gpt2_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:27.280467Z","iopub.execute_input":"2025-01-10T14:49:27.280808Z","iopub.status.idle":"2025-01-10T14:49:27.284631Z","shell.execute_reply.started":"2025-01-10T14:49:27.280775Z","shell.execute_reply":"2025-01-10T14:49:27.283722Z"},"id":"Xjf6vXzJa7Iv"},"outputs":[],"execution_count":29},{"cell_type":"code","source":"len(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:27.583669Z","iopub.execute_input":"2025-01-10T14:49:27.584029Z","iopub.status.idle":"2025-01-10T14:49:27.588944Z","shell.execute_reply.started":"2025-01-10T14:49:27.584003Z","shell.execute_reply":"2025-01-10T14:49:27.588162Z"},"id":"qmSPRdvNa7Iw"},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"16803"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"metacognition_prompt_ids, problem_student_code_ids, problem_expected_code_ids, student_code_ids, target_ids = dataset[4000]\nprint(f\"Metacognition vector IDs: {metacognition_prompt_ids}\")\nprint(f\"Expected feedback IDs: {problem_student_code_ids}\")\nprint(f\"Expected encoded feedback IDs: {problem_expected_code_ids}\")\nprint(f\"Student Answer IDs: {student_code_ids.shape}\")\nprint(f\"Target IDs: {target_ids.shape}\")\nprint(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:27.864707Z","iopub.execute_input":"2025-01-10T14:49:27.865056Z","iopub.status.idle":"2025-01-10T14:49:27.918704Z","shell.execute_reply.started":"2025-01-10T14:49:27.865028Z","shell.execute_reply":"2025-01-10T14:49:27.917709Z"},"id":"n5JGeIpza7Ix"},"outputs":[{"name":"stdout","text":"Metacognition vector IDs: tensor([14204,    75, 12905,  3268,  3160,  1691,   481,  3548,    30,    70,\n        12628,    18,  6065,    53,  3266,     5,    37,  2804, 10531,    75,\n        12905,  3268, 12938,    19,     3,     9,   898,    18, 11619, 12938,\n            3, 16012,     8,  1236,    31,     7, 10531,    75, 12905,  1575,\n         3278,    10,   213,   284,  9340,  5475,    80,    13,   898, 10531,\n           75, 12905,  3268,  3379,     6,     3,  4094,    38,   209,    41,\n        13283,   470,   201,   204,    41, 19055,   715,     7,   201,    42,\n          220,    41, 10084,   137,   506,  3379,   560,    10,  3403,     6,\n            3, 23393,     6,   419, 27111,     6, 19119,     6, 11429,  3035,\n            6, 23621,    15,     6,  2926,     6,  6342,   159,    15,     6,\n          781,  4921,     6, 15856,   329,   159,  4914,     7,     6, 15192,\n        14337,   102,     7,     6, 15192,  3174,  2319,     7,     6, 23545,\n          342,  4302,     7,  9719,    17,     7,     6,  1193,  7001,     6,\n         1972,  1649,  1169,    60,  4128,     6, 23966,     5, 12636,    15,\n           19,     8,  1236,    31,     7, 10531,    75, 12905,  1575, 12938,\n           10,   784,    31,   519,     3,    31,     6,     3,    31,   519,\n            3,    31,     6,     3,    31,   357,     3,    31,     6,     3,\n           31,   519,     3,    31,     6,     3,    31,   519,     3,    31,\n            6,     3,    31,   519,     3,    31,     6,     3,    31,   536,\n            3,    31,     6,     3,    31,   536,     3,    31,     6,     3,\n           31,   519,     3,    31,     6,     3,    31,   357,     3,    31,\n            6,     3,    31,   519,     3,    31,     6,     3,    31,   519,\n            3,    31,     6,     3,    31,   519,     3,    31,     6,     3,\n           31,   519,     3,    31,     6,     3,    31,   519,     3,    31,\n            6,     3,    31,   519,     3,    31,   908,     1,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0])\nExpected feedback IDs: tensor([  100,    19,     8,  2711,    13,     8,   682,    93,    75, 29771,\n           23,   106,    11,     8,  1236,   937,  1081,     5,  5289,    10,\n        24305,     3,     9, 20737,   478,    24,  1217,  1236,  3056,    11,\n           70,  7586,    16,  1317,  7404,    38,  3785,    45,     3,     9,\n         1042,    11,  3911,     7,   284,  1236,    31,     7,  1348,  2604,\n           12,     8,  8990,    41, 10475,  5595,   137,  5433,     6,     8,\n          478,   398,  3911,     8,   564,   599,     7,    61,    13,     8,\n         1236,   599,     7,    61,    28,     8,  2030,    11,  7402,  1348,\n         7586,     5,   696,   478,   398,  6634,    11,   169,    44,   709,\n           80,  1681,     5,    27,  9082,  6675,  5652, 18169,    10,  1429,\n           71,  1499,  1042,   213,   284,   689,  2579,     3,     9,  1236,\n           31,     7,   564,  2348,    57,    70,  7586,    16,  1317,  7404,\n            6,    66, 12494,    57,  4856,     5,    37,   381,    13,  7404,\n           54,  5215,   344,   481,     6,    68,   284,  1236,    56,    43,\n           44,   709,   220,  7404,     5,    37,   689,  1910,    19,    10,\n         6341, 23954, 17763,   536, 17763,   357,     3,   233, 17763,   567,\n            5,  1698,  2604,    19,    46, 30278,    16,     8,   620,     3,\n         9498,  2915,     5,   290,    56,    36,    44,   709,    80,  1236,\n           11,   112,    87,   760,  7586,    16,     8,  1042,     5,     3,\n         9744,   345,  6675,  5652, 18169,    10,  1429,    37,   478,   225,\n          166,  3911,     8,   564,    11,  1348,  2604,     3, 12279,    12,\n          192,  7908,  1982,  1747,    13,   334,  1236,    30,    80,   689,\n            6,    16,     8,   455,    79,  4283,    16,     8,  3785,     5,\n         1429,    37,   478,   225,   416,  3911,     8,   564,   599,     7,\n           61,    13,     8,  1236,   599,     7,    61,    28,     8,  2030,\n           11,  7402,  1348,  7586,     6,   590,    28,    70,  1348,  7586,\n            5,   156,   132,    19,   163,    80,  1236,     6,    24,  1236,\n          225,    36,  2008,    21,   321,     8,  2030,    11,  7402,  1348,\n            5,   156,   132,    33,  1317,   481,    28,     8,   337,  2030,\n           42,  7402,  1348,     6,   258,   570,    66,   224,   481,    16,\n            8,   455,    79,  4283,    16,     8,  3785,     5,   262,     4,\n        15837,  3765,   209,    10,    27,  9082,  6675,    41, 17752, 17779,\n         4578,  3347,   377, 20129,    61,    10, 13390, 11989,  2777,     3,\n         3940,  5762,  2861,  6374,  2775, 11989, 12707,     3,  4060, 11989,\n         2777,     3,  4271,     3,  4508,     3,  9744,   345,  6675,    10,\n        13390,    10,     3,  4608,     5,  4201,  5762,    10,     3,  4013,\n            5,  1752, 12707,    10,     3,  3914,     5,  3328,  1592,   222,\n        23836,    10, 12707,     6,     3,  3914,     5,  3328,  5586,   222,\n        23836,    10,  5762,     6,     3,  4013,     5,  1752,   262,     4,\n        15837,  3765,   204,    10,    27,  9082,  6675,    41, 17752, 17779,\n         4578,  3347,   377, 20129,    61,    10, 13390, 11989,  2777, 11923,\n         5762,  6374,  2775, 11989, 12707, 11923,  2777, 11989, 18821, 11923,\n         2777, 11989, 11566,  1640,  7123,  2861,     3,  9744,   345,  6675,\n           10, 13390,    10,   668, 10667,  5762,    10,   505, 10667, 12707,\n           10,   668, 10667, 18821,    10,   668, 10667, 11566,    10,   431,\n        23577,  1592,   222, 23836,    10, 13390,     6, 12707,     6, 18821,\n            6,   668, 10667,  5586,   222, 23836,    10, 11566,     6,   431,\n        23577,     5,  6341,   937,  1081,    10,  3785,   834, 11966,  2423,\n           77,  2562,  9960,     3,   226,    32,  2423,  8751,   599,    77,\n         2562,   834, 11966,     6,    31,    52,    31,    61,  1713,  8751,\n            8,  1042,   608,   834, 11966,  2423,   226,    32,     5,  5236,\n         9960,     1])\nExpected encoded feedback IDs: tensor([  100,    19,     8,  2711,    13,     8,   682,  4210,    11,     8,\n         1644,  2024,  1525,    21,    24, 12628,   408,   822,     5,  5289,\n           10, 24305,     3,     9, 20737,   478,    24,  1217,  1236,  3056,\n           11,    70,  7586,    16,  1317,  7404,    38,  3785,    45,     3,\n            9,  1042,    11,  3911,     7,   284,  1236,    31,     7,  1348,\n         2604,    12,     8,  8990,    41, 10475,  5595,   137,  5433,     6,\n            8,   478,   398,  3911,     8,   564,   599,     7,    61,    13,\n            8,  1236,   599,     7,    61,    28,     8,  2030,    11,  7402,\n         1348,  7586,     5,   696,   478,   398,  6634,    11,   169,    44,\n          709,    80,  1681,     5,    27,  9082,  6675,  5652, 18169,    10,\n         1429,    71,  1499,  1042,   213,   284,   689,  2579,     3,     9,\n         1236,    31,     7,   564,  2348,    57,    70,  7586,    16,  1317,\n         7404,     6,    66, 12494,    57,  4856,     5,    37,   381,    13,\n         7404,    54,  5215,   344,   481,     6,    68,   284,  1236,    56,\n           43,    44,   709,   220,  7404,     5,    37,   689,  1910,    19,\n           10,  6341, 23954, 17763,   536, 17763,   357,     3,   233, 17763,\n          567,     5,  1698,  2604,    19,    46, 30278,    16,     8,   620,\n            3,  9498,  2915,     5,   290,    56,    36,    44,   709,    80,\n         1236,    11,   112,    87,   760,  7586,    16,     8,  1042,     5,\n            3,  9744,   345,  6675,  5652, 18169,    10,  1429,    37,   478,\n          225,   166,  3911,     8,   564,    11,  1348,  2604,     3, 12279,\n           12,   192,  7908,  1982,  1747,    13,   334,  1236,    30,    80,\n          689,     6,    16,     8,   455,    79,  4283,    16,     8,  3785,\n            5,  1429,    37,   478,   225,   416,  3911,     8,   564,   599,\n            7,    61,    13,     8,  1236,   599,     7,    61,    28,     8,\n         2030,    11,  7402,  1348,  7586,     6,   590,    28,    70,  1348,\n         7586,     5,   156,   132,    19,   163,    80,  1236,     6,    24,\n         1236,   225,    36,  2008,    21,   321,     8,  2030,    11,  7402,\n         1348,     5,   156,   132,    33,  1317,   481,    28,     8,   337,\n         2030,    42,  7402,  1348,     6,   258,   570,    66,   224,   481,\n           16,     8,   455,    79,  4283,    16,     8,  3785,     5,   262,\n            4, 15837,  3765,   209,    10,    27,  9082,  6675,    41, 17752,\n        17779,  4578,  3347,   377, 20129,    61,    10, 13390, 11989,  2777,\n            3,  3940,  5762,  2861,  6374,  2775, 11989, 12707,     3,  4060,\n        11989,  2777,     3,  4271,     3,  4508,     3,  9744,   345,  6675,\n           10, 13390,    10,     3,  4608,     5,  4201,  5762,    10,     3,\n         4013,     5,  1752, 12707,    10,     3,  3914,     5,  3328,  1592,\n          222, 23836,    10, 12707,     6,     3,  3914,     5,  3328,  5586,\n          222, 23836,    10,  5762,     6,     3,  4013,     5,  1752,   262,\n            4, 15837,  3765,   204,    10,    27,  9082,  6675,    41, 17752,\n        17779,  4578,  3347,   377, 20129,    61,    10, 13390, 11989,  2777,\n        11923,  5762,  6374,  2775, 11989, 12707, 11923,  2777, 11989, 18821,\n        11923,  2777, 11989, 11566,  1640,  7123,  2861,     3,  9744,   345,\n         6675,    10, 13390,    10,   668, 10667,  5762,    10,   505, 10667,\n        12707,    10,   668, 10667, 18821,    10,   668, 10667, 11566,    10,\n          431, 23577,  1592,   222, 23836,    10, 13390,     6, 12707,     6,\n        18821,     6,   668, 10667,  5586,   222, 23836,    10, 11566,     6,\n          431, 23577,     5, 19539,    15,    26,  2024,    46,     7,     7,\n         3321,    21,     8,   682,    10,    20,    89, 11837,   834, 28951,\n          599,     7,  9022,     7,    61,    10,  1205,  4505,   599,     7,\n         9022,     7,    61,     3,    87,    90,    29,   599,     7,  9022,\n            7,     1])\nStudent Answer IDs: torch.Size([512])\nTarget IDs: torch.Size([512])\n\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"gpt2_tokenizer.pad_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:31.371840Z","iopub.execute_input":"2025-01-10T14:49:31.372128Z","iopub.status.idle":"2025-01-10T14:49:31.376936Z","shell.execute_reply.started":"2025-01-10T14:49:31.372108Z","shell.execute_reply":"2025-01-10T14:49:31.376220Z"},"id":"6Wlser8Ba7Iy"},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"'<|endoftext|>'"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"gpt2_pad_token_id = gpt2_tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:31.635157Z","iopub.execute_input":"2025-01-10T14:49:31.635443Z","iopub.status.idle":"2025-01-10T14:49:31.639196Z","shell.execute_reply.started":"2025-01-10T14:49:31.635421Z","shell.execute_reply":"2025-01-10T14:49:31.638351Z"},"id":"cnDxXCGxa7Iy"},"outputs":[],"execution_count":33},{"cell_type":"code","source":"gpt2_pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:31.982301Z","iopub.execute_input":"2025-01-10T14:49:31.982605Z","iopub.status.idle":"2025-01-10T14:49:31.987338Z","shell.execute_reply.started":"2025-01-10T14:49:31.982583Z","shell.execute_reply":"2025-01-10T14:49:31.986575Z"},"id":"5LAI9-sZa7Iy"},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"50256"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"t5_tokenizer.pad_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:32.186373Z","iopub.execute_input":"2025-01-10T14:49:32.186689Z","iopub.status.idle":"2025-01-10T14:49:32.191888Z","shell.execute_reply.started":"2025-01-10T14:49:32.186666Z","shell.execute_reply":"2025-01-10T14:49:32.191017Z"},"id":"hEyynFuNa7Iz"},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"'<pad>'"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"t5_pad_token_id = t5_tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:34.916418Z","iopub.execute_input":"2025-01-10T14:49:34.916771Z","iopub.status.idle":"2025-01-10T14:49:34.920513Z","shell.execute_reply.started":"2025-01-10T14:49:34.916707Z","shell.execute_reply":"2025-01-10T14:49:34.919677Z"},"id":"sGPK7f0ba7Iz"},"outputs":[],"execution_count":36},{"cell_type":"code","source":"t5_pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:35.191516Z","iopub.execute_input":"2025-01-10T14:49:35.191865Z","iopub.status.idle":"2025-01-10T14:49:35.196580Z","shell.execute_reply.started":"2025-01-10T14:49:35.191836Z","shell.execute_reply":"2025-01-10T14:49:35.195823Z"},"id":"nWn7op96a7I0"},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"# Context Enocder","metadata":{"id":"HK7ybN62a7I0"}},{"cell_type":"code","source":"# class ContextEncoder(nn.Module):\n#     def __init__(self, t5_model_name='t5-base', output_dim=768 , max_length = 512):\n#         super(ContextEncoder, self).__init__()\n\n#         self.max_length = max_length\n\n#         self.t5_encoder = T5Model.from_pretrained(t5_model_name).encoder\n#         self.t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n#         self.fc = nn.Linear(self.t5_encoder.config.d_model, output_dim)\n\n#     def forward(self, problem_code_ids, attention_masks=None, prompt=\"\"):\n\n#         prompt_ids = torch.tensor(\n#             self.t5_tokenizer.encode(prompt, max_length=self.max_length, truncation=True, padding=\"max_length\")\n#         )\n\n#         problem_code = [self.t5_tokenizer.decode(ids, skip_special_tokens=True) for ids in problem_code_ids]\n\n#         combined_text = prompt + \" \" + \" \".join(problem_code)\n\n#         encoded = self.t5_tokenizer(\n#             combined_text,\n#             max_length=self.max_length, \n#             truncation=True,\n#             padding=\"max_length\",\n#             return_tensors=\"pt\"\n#         ).to(problem_code_ids.device)\n\n#         encoder_outputs = self.t5_encoder(\n#             input_ids=encoded[\"input_ids\"],\n#             attention_mask=encoded[\"attention_mask\"]\n#         )\n\n#         context_hidden_states = encoder_outputs.last_hidden_state\n\n#         context_rep = context_hidden_states.mean(dim=1)\n\n#         # decoded_combined = [\n#         # self.t5_tokenizer.decode(ids, skip_special_tokens=True)\n#         # for ids in encoded_outputs]\n\n#         context_rep = self.fc(context_rep)\n#         final_rep = context_rep.unsqueeze(1)\n\n#         return final_rep","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:37.406946Z","iopub.execute_input":"2025-01-10T14:49:37.407236Z","iopub.status.idle":"2025-01-10T14:49:37.411315Z","shell.execute_reply.started":"2025-01-10T14:49:37.407214Z","shell.execute_reply":"2025-01-10T14:49:37.410372Z"},"id":"epmjfCO_a7I2"},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# context_encoder = ContextEncoder().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:37.655125Z","iopub.execute_input":"2025-01-10T14:49:37.655434Z","iopub.status.idle":"2025-01-10T14:49:37.659264Z","shell.execute_reply.started":"2025-01-10T14:49:37.655411Z","shell.execute_reply":"2025-01-10T14:49:37.658233Z"},"id":"Fnjz8vrra7I2"},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# context_encoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:37.859986Z","iopub.execute_input":"2025-01-10T14:49:37.860272Z","iopub.status.idle":"2025-01-10T14:49:37.863882Z","shell.execute_reply.started":"2025-01-10T14:49:37.860250Z","shell.execute_reply":"2025-01-10T14:49:37.862931Z"},"id":"GCZmPBnSa7I2"},"outputs":[],"execution_count":40},{"cell_type":"code","source":"\n# class MetacognitionLayer(nn.Module):\n#     def __init__(self, metacognitive_dim=16, output_dim=768):\n#         super(MetacognitionLayer, self).__init__()\n#         #16 to 768 mapping\n#         self.metacognitive_fc = nn.Linear(metacognitive_dim, output_dim)\n#         self.final_fc = nn.Linear(output_dim, output_dim)\n\n#         self.tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n#         self.t5_pad_token_id = self.tokenizer.pad_token_id\n#         self.encoder = T5Model.from_pretrained(\"t5-base\").encoder\n\n\n\n#         for param in self.encoder.parameters():\n#             param.requires_grad = False\n\n#     def forward(self, metacognitive_vector):\n\n#         metacognitive_components = [\n#                 \"Read\", \"Identify\", \"Rephrase\", \"Examples\", \"Breakdown\", \"Estimate\", \"Plan\",\n#                 \"Revise\", \"Verify\", \"AvoidMistakes\", \"MonitorSteps\", \"MonitorProcess\",\n#                 \"ValidateConstraints\", \"Confirm\", \"CheckRequirements\", \"Reflect\"\n#             ]\n\n#         prompt_text = (\n#             f\"Metacognitive feedback helps students reflect on their algorithm-solving strategies. \"\n#             f\"The passed metacognitive vector is a 16-dimensional vector describing the student's metacognition profile, \"\n#             f\"where each dimension represents one of 16 metacognitive components, rated as 1 (Almost never), \"\n#             f\"2 (Sometimes), or 3 (Often). These components include: {', '.join(metacognitive_components)}.\"\n#         )\n\n\n#         input_prompt = self.tokenizer(\n#         prompt_text,\n#         return_tensors=\"pt\",\n#         padding=True,\n#         truncation=True,\n#         max_length=512-16\n#         )\n#         input_prompt = {key: value.to(metacognitive_vector.device) for key, value in input_prompt.items()}\n        \n#         outputs = self.encoder(input_ids=input_prompt[\"input_ids\"], attention_mask=input_prompt[\"attention_mask\"])\n#         prompt_embedding = outputs.last_hidden_state.mean(dim=1)\n#         prompt_embedding = prompt_embedding.to(metacognitive_vector.device)\n\n#         metacognitive_rep = self.metacognitive_fc(metacognitive_vector)\n#         final_rep = self.final_fc(metacognitive_rep)\n#         persona_rep = final_rep.unsqueeze(1)\n       \n#         final_persona = persona_rep + prompt_embedding\n\n#         return final_persona","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:38.120567Z","iopub.execute_input":"2025-01-10T14:49:38.120922Z","iopub.status.idle":"2025-01-10T14:49:38.124712Z","shell.execute_reply.started":"2025-01-10T14:49:38.120893Z","shell.execute_reply":"2025-01-10T14:49:38.123813Z"},"id":"ZX97CJgLa7I2"},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# metacognitive_emb = MetacognitionLayer().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:40.185167Z","iopub.execute_input":"2025-01-10T14:49:40.185484Z","iopub.status.idle":"2025-01-10T14:49:40.189165Z","shell.execute_reply.started":"2025-01-10T14:49:40.185459Z","shell.execute_reply":"2025-01-10T14:49:40.188228Z"},"id":"x5pQ-96_a7JA"},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# metacognitive_emb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:40.420964Z","iopub.execute_input":"2025-01-10T14:49:40.421251Z","iopub.status.idle":"2025-01-10T14:49:40.425087Z","shell.execute_reply.started":"2025-01-10T14:49:40.421231Z","shell.execute_reply":"2025-01-10T14:49:40.424034Z"},"id":"BQMfinGKa7JA"},"outputs":[],"execution_count":43},{"cell_type":"code","source":"\nclass PAALayer(nn.Module):\n    def __init__(self, hidden_dimension = 512 , tau=0.8,dropout_rate=0.1):\n        super(PAALayer, self).__init__()\n        self.hidden_dimenstion = hidden_dimension\n        self.tau = tau\n\n\n        self.fc = nn.Linear(2 * hidden_dimension, hidden_dimension)\n        self.sigmoid = nn.Sigmoid()\n        self.fc_out = nn.Linear(hidden_dimension, hidden_dimension)\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n\n    def forward(self, hR , oP, oC):\n\n        Mp_input  = torch.cat([hR,oP], dim=-1)\n        Mp = self.fc(Mp_input)\n        Wp = self.sigmoid(Mp)\n\n        Mpersona = Wp\n        Mcontext = 1 - Wp\n\n        oP_weighted = Mcontext * oP\n        oC_weighted = Mpersona * oC\n\n        HPAA = oP_weighted + oC_weighted\n\n        output = self.fc_out(HPAA)\n\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:40.625214Z","iopub.execute_input":"2025-01-10T14:49:40.625513Z","iopub.status.idle":"2025-01-10T14:49:40.631429Z","shell.execute_reply.started":"2025-01-10T14:49:40.625489Z","shell.execute_reply":"2025-01-10T14:49:40.630587Z"},"id":"2Z_xRbhNa7JA"},"outputs":[],"execution_count":44},{"cell_type":"code","source":"paa = PAALayer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:40.852856Z","iopub.execute_input":"2025-01-10T14:49:40.853175Z","iopub.status.idle":"2025-01-10T14:49:40.870292Z","shell.execute_reply.started":"2025-01-10T14:49:40.853153Z","shell.execute_reply":"2025-01-10T14:49:40.869288Z"},"id":"E6kRw4dea7JB"},"outputs":[],"execution_count":45},{"cell_type":"code","source":"paa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:43.361323Z","iopub.execute_input":"2025-01-10T14:49:43.361630Z","iopub.status.idle":"2025-01-10T14:49:43.366892Z","shell.execute_reply.started":"2025-01-10T14:49:43.361608Z","shell.execute_reply":"2025-01-10T14:49:43.365873Z"},"id":"T3pInXDya7JB","outputId":"66ad94da-6b56-4525-e2b2-459d4aa68c9d"},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"PAALayer(\n  (fc): Linear(in_features=1024, out_features=512, bias=True)\n  (sigmoid): Sigmoid()\n  (fc_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"class CustomTransformerBlock(nn.Module):\n    def __init__(self, hidden_size, tau , dropout_rate=0.1):\n        super(CustomTransformerBlock, self).__init__()\n\n        self.input_self_attention = nn.MultiheadAttention(hidden_size, num_heads=8, batch_first=True)\n        self.context_attn = nn.MultiheadAttention(hidden_size, num_heads=8, batch_first=True)\n        self.persona_attn = nn.MultiheadAttention(hidden_size, num_heads=8, batch_first=True)\n        self.persona_proj = nn.Linear(768, hidden_size)  \n        self.context_proj = nn.Linear(768, hidden_size)\n\n        self.paa_layer = PAALayer(hidden_dimension=hidden_size, tau=tau)\n        \n\n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_size, 2048),\n            nn.ReLU(),\n            nn.Dropout(p=0.1),\n            nn.Linear(2048, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.Linear(2048,hidden_size),\n        )\n        self.layer_norm2 = nn.LayerNorm(hidden_size)\n\n    def forward(self, student_initial_state, encoded_persona, encoded_context):        \n\n        hR, _ = self.input_self_attention(student_initial_state, student_initial_state, student_initial_state) #query\n        encoded_persona = self.persona_proj(encoded_persona)\n        encoded_context = self.context_proj(encoded_context)\n        # print(\"hr shape\",hR.shape)\n        # print(\"encoded persona shape\", encoded_persona.shape)\n        # print(\"encoded context shape\", encoded_context.shape)\n        oP, _ = self.persona_attn(hR, encoded_persona, encoded_persona )\n       \n        # encoded_context = encoded_context.repeat(hR.size(0), hR.size(1), 1)\n      \n        oC, _ = self.context_attn(hR, encoded_context, encoded_context )\n\n        HPAA = self.paa_layer(hR, oP, oC)\n        output = self.layer_norm2(HPAA)\n\n        mlp_output = self.mlp(output)\n        # output = self.layer_norm2(mlp_output)\n\n        return mlp_output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:43.633811Z","iopub.execute_input":"2025-01-10T14:49:43.634101Z","iopub.status.idle":"2025-01-10T14:49:43.640680Z","shell.execute_reply.started":"2025-01-10T14:49:43.634079Z","shell.execute_reply":"2025-01-10T14:49:43.639874Z"},"id":"wDhvR6Goa7JB"},"outputs":[],"execution_count":47},{"cell_type":"code","source":"custom_layer = CustomTransformerBlock(768,0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:45.525672Z","iopub.execute_input":"2025-01-10T14:49:45.525989Z","iopub.status.idle":"2025-01-10T14:49:45.629488Z","shell.execute_reply.started":"2025-01-10T14:49:45.525965Z","shell.execute_reply":"2025-01-10T14:49:45.628810Z"},"id":"v_D6juUna7JC"},"outputs":[],"execution_count":48},{"cell_type":"code","source":"custom_layer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:45.839189Z","iopub.execute_input":"2025-01-10T14:49:45.839520Z","iopub.status.idle":"2025-01-10T14:49:45.845141Z","shell.execute_reply.started":"2025-01-10T14:49:45.839495Z","shell.execute_reply":"2025-01-10T14:49:45.844345Z"},"id":"nwq9pZy6a7JC","outputId":"6fbb8e91-749a-4ef8-a24e-c2313f8dea45"},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"CustomTransformerBlock(\n  (input_self_attention): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (context_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (persona_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (persona_proj): Linear(in_features=768, out_features=768, bias=True)\n  (context_proj): Linear(in_features=768, out_features=768, bias=True)\n  (paa_layer): PAALayer(\n    (fc): Linear(in_features=1536, out_features=768, bias=True)\n    (sigmoid): Sigmoid()\n    (fc_out): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (mlp): Sequential(\n    (0): Linear(in_features=768, out_features=2048, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.1, inplace=False)\n    (3): Linear(in_features=2048, out_features=768, bias=True)\n    (4): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n)"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"class PAAModel(nn.Module):\n    def __init__(self, hidden_size=512, vocab_size = 32100 ,tau=0.5, max_length=512, num_transformer_blocks=3):\n        super(PAAModel , self).__init__()\n        self.hidden_size = hidden_size\n        self.tau = tau\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n        self.num_transformer_blocks = num_transformer_blocks\n        \n        self.t5_encoder = T5Model.from_pretrained(\"t5-base\").encoder\n        for param in self.t5_encoder.parameters():\n            param.requires_grad = False\n\n\n        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n        self.position_embedding = nn.Embedding(max_length, hidden_size)\n        self.dropout = nn.Dropout(p=0.1)        \n\n        self.transformer_blocks = nn.ModuleList([CustomTransformerBlock(hidden_size, tau) for _ in range(num_transformer_blocks)])\n        self.final_fc = nn.Linear(hidden_size, vocab_size)\n        self.layer_norm = nn.LayerNorm(hidden_size)\n\n    def forward(self,  metacognition_prompt_ids,\n                       problem_student_code_ids ,\n                       problem_expected_code_ids,\n                       metacognition_attention_mask,\n                       expected_attention_mask):\n\n        with torch.no_grad():\n\n            metacognition_prompt_encoded = self.t5_encoder(\n                                                        input_ids=metacognition_prompt_ids,\n                                                        attention_mask=metacognition_attention_mask).last_hidden_state\n    \n            problem_expected_code_encoded = self.t5_encoder(\n                                                        input_ids=problem_expected_code_ids,\n                                                        attention_mask=expected_attention_mask).last_hidden_state    \n\n\n        token_embeds = self.token_embedding(problem_student_code_ids)\n        position_ids = torch.arange(0, 1, device=problem_student_code_ids.device).unsqueeze(0)\n        position_ids = position_ids % self.max_length\n        position_embeds = self.position_embedding(position_ids)\n\n        inputs_embeds = token_embeds + position_embeds\n        inputs_embeds = self.dropout(inputs_embeds)\n        \n        student_initial_state = inputs_embeds\n        transformer_output = student_initial_state\n        \n        for transformer_block in self.transformer_blocks:\n            transformer_output = transformer_block(transformer_output, metacognition_prompt_encoded, problem_expected_code_encoded)\n\n        logits = self.final_fc(transformer_output)\n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:47.669097Z","iopub.execute_input":"2025-01-10T14:49:47.669417Z","iopub.status.idle":"2025-01-10T14:49:47.677375Z","shell.execute_reply.started":"2025-01-10T14:49:47.669389Z","shell.execute_reply":"2025-01-10T14:49:47.676356Z"},"id":"YSUMxJOYa7JC"},"outputs":[],"execution_count":50},{"cell_type":"code","source":"model = PAAModel()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:50.270145Z","iopub.execute_input":"2025-01-10T14:49:50.270438Z","iopub.status.idle":"2025-01-10T14:49:55.150278Z","shell.execute_reply.started":"2025-01-10T14:49:50.270416Z","shell.execute_reply":"2025-01-10T14:49:55.149539Z"},"id":"AROFcsBVa7JD"},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2014343d9414432931fa7e7ded6df7f"}},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:55.151492Z","iopub.execute_input":"2025-01-10T14:49:55.151825Z","iopub.status.idle":"2025-01-10T14:49:55.557529Z","shell.execute_reply.started":"2025-01-10T14:49:55.151791Z","shell.execute_reply":"2025-01-10T14:49:55.556803Z"},"id":"ukrbmnT1a7JD","outputId":"6e88007a-2a1a-4a99-b9e0-a11441cfa0f1"},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"PAAModel(\n  (t5_encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (token_embedding): Embedding(32100, 512)\n  (position_embedding): Embedding(512, 512)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (transformer_blocks): ModuleList(\n    (0-2): 3 x CustomTransformerBlock(\n      (input_self_attention): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n      )\n      (context_attn): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n      )\n      (persona_attn): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n      )\n      (persona_proj): Linear(in_features=768, out_features=512, bias=True)\n      (context_proj): Linear(in_features=768, out_features=512, bias=True)\n      (paa_layer): PAALayer(\n        (fc): Linear(in_features=1024, out_features=512, bias=True)\n        (sigmoid): Sigmoid()\n        (fc_out): Linear(in_features=512, out_features=512, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (mlp): Sequential(\n        (0): Linear(in_features=512, out_features=2048, bias=True)\n        (1): ReLU()\n        (2): Dropout(p=0.1, inplace=False)\n        (3): Linear(in_features=2048, out_features=512, bias=True)\n        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (final_fc): Linear(in_features=512, out_features=32100, bias=True)\n  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n)"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:55.559329Z","iopub.execute_input":"2025-01-10T14:49:55.559601Z","iopub.status.idle":"2025-01-10T14:49:55.562952Z","shell.execute_reply.started":"2025-01-10T14:49:55.559582Z","shell.execute_reply":"2025-01-10T14:49:55.562098Z"},"id":"w-YM7dxRa7JD"},"outputs":[],"execution_count":53},{"cell_type":"code","source":"import torch.optim as optim\noptimizer = optim.AdamW(model.parameters(), lr=5e-5)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:55.563959Z","iopub.execute_input":"2025-01-10T14:49:55.564228Z","iopub.status.idle":"2025-01-10T14:49:56.153044Z","shell.execute_reply.started":"2025-01-10T14:49:55.564207Z","shell.execute_reply":"2025-01-10T14:49:56.152099Z"},"id":"sCrMz7qra7JE"},"outputs":[],"execution_count":54},{"cell_type":"code","source":"LOSS = torch.nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:56.153895Z","iopub.execute_input":"2025-01-10T14:49:56.154281Z","iopub.status.idle":"2025-01-10T14:49:56.158445Z","shell.execute_reply.started":"2025-01-10T14:49:56.154257Z","shell.execute_reply":"2025-01-10T14:49:56.157537Z"},"id":"kNNiPR_ja7JE"},"outputs":[],"execution_count":55},{"cell_type":"code","source":"num_epochs = 200","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:56.159480Z","iopub.execute_input":"2025-01-10T14:49:56.159788Z","iopub.status.idle":"2025-01-10T14:49:56.172912Z","shell.execute_reply.started":"2025-01-10T14:49:56.159759Z","shell.execute_reply":"2025-01-10T14:49:56.171923Z"},"id":"gMr9Wd8_a7JE"},"outputs":[],"execution_count":56},{"cell_type":"code","source":"df_train = df[0:5000]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:56.173841Z","iopub.execute_input":"2025-01-10T14:49:56.174149Z","iopub.status.idle":"2025-01-10T14:49:56.186603Z","shell.execute_reply.started":"2025-01-10T14:49:56.174117Z","shell.execute_reply":"2025-01-10T14:49:56.185704Z"},"id":"U5UW8SVga7JE"},"outputs":[],"execution_count":57},{"cell_type":"code","source":"len(df_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:58.810759Z","iopub.execute_input":"2025-01-10T14:49:58.811071Z","iopub.status.idle":"2025-01-10T14:49:58.816224Z","shell.execute_reply.started":"2025-01-10T14:49:58.811046Z","shell.execute_reply":"2025-01-10T14:49:58.815192Z"},"id":"FADAr7rsa7JF","outputId":"2bc20724-bbfe-4263-c10d-0b4469249df3"},"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"5000"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:59.010330Z","iopub.execute_input":"2025-01-10T14:49:59.010630Z","iopub.status.idle":"2025-01-10T14:49:59.027008Z","shell.execute_reply.started":"2025-01-10T14:49:59.010600Z","shell.execute_reply":"2025-01-10T14:49:59.026045Z"},"id":"V_g5jqJQa7JF","outputId":"d4a9adb3-2204-412b-ea78-d71c8bfcb8fe"},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"Problem                         0\nStudent_code                    0\nExpected_code                   0\nQ01                             0\nQ02                             0\nQ03                             0\nQ04                             0\nQ05                             0\nQ06                             0\nQ07                             0\nQ08                             0\nQ09                             0\nQ10                             0\nQ11                             0\nQ12                             0\nQ13                             0\nQ14                             0\nQ15                             0\nQ16                             0\nmetacognitive_vector            0\nmetacognitive_feedback          0\ncombined_problem_student        0\ncombined_problem_expected       0\ncombined_metacogntion_prompt    0\ndtype: int64"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ntrain_dataset = CustomDataset(df_train, t5_tokenizer , gpt2_tokenizer)\ntrain_dataloader = DataLoader(train_dataset , batch_size = 4 ,shuffle = True )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:59.255018Z","iopub.execute_input":"2025-01-10T14:49:59.255352Z","iopub.status.idle":"2025-01-10T14:49:59.260117Z","shell.execute_reply.started":"2025-01-10T14:49:59.255322Z","shell.execute_reply":"2025-01-10T14:49:59.258951Z"},"id":"GqHiiAc1a7JF"},"outputs":[],"execution_count":60},{"cell_type":"code","source":"# from torch.utils.tensorboard import SummaryWriter\n# writer = SummaryWriter()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:49:59.795287Z","iopub.execute_input":"2025-01-10T14:49:59.795687Z","iopub.status.idle":"2025-01-10T14:49:59.799569Z","shell.execute_reply.started":"2025-01-10T14:49:59.795655Z","shell.execute_reply":"2025-01-10T14:49:59.798749Z"},"id":"pvWMI-1La7JG","outputId":"f1ad7ea8-6c46-426b-f009-98b9cf47574e"},"outputs":[],"execution_count":61},{"cell_type":"code","source":"import os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:50:01.850231Z","iopub.execute_input":"2025-01-10T14:50:01.850526Z","iopub.status.idle":"2025-01-10T14:50:01.854288Z","shell.execute_reply.started":"2025-01-10T14:50:01.850503Z","shell.execute_reply":"2025-01-10T14:50:01.853423Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"checkpoint_dir = \"./checkpoints\"\nos.makedirs(checkpoint_dir, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:50:02.107268Z","iopub.execute_input":"2025-01-10T14:50:02.107562Z","iopub.status.idle":"2025-01-10T14:50:02.111464Z","shell.execute_reply.started":"2025-01-10T14:50:02.107539Z","shell.execute_reply":"2025-01-10T14:50:02.110524Z"},"id":"_G2DGEL_a7JG"},"outputs":[],"execution_count":63},{"cell_type":"code","source":"# t5_encoder = T5Model.from_pretrained(checkpoint).encoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:50:02.329876Z","iopub.execute_input":"2025-01-10T14:50:02.330194Z","iopub.status.idle":"2025-01-10T14:50:02.333546Z","shell.execute_reply.started":"2025-01-10T14:50:02.330168Z","shell.execute_reply":"2025-01-10T14:50:02.332699Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"# t5_encoder.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:50:04.395160Z","iopub.execute_input":"2025-01-10T14:50:04.395495Z","iopub.status.idle":"2025-01-10T14:50:04.398971Z","shell.execute_reply.started":"2025-01-10T14:50:04.395468Z","shell.execute_reply":"2025-01-10T14:50:04.398057Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    print(f\"Training started for epoch {epoch + 1}/{num_epochs}\")\n    model.train()\n    total_loss = 0\n\n    for idx, (metacognition_prompt_ids,\n              problem_student_code_ids,\n              problem_expected_code_ids,\n              student_code_ids,\n              target_ids) in enumerate(train_dataloader):\n        \n        \n        metacognition_prompt_ids = metacognition_prompt_ids.to(device)\n        problem_student_code_ids = problem_student_code_ids.to(device)\n        problem_expected_code_ids = problem_expected_code_ids.to(device)\n        student_code_ids = student_code_ids.to(device)\n        target_ids = target_ids.to(device)\n\n        #attention masking\n        student_attention_mask = (problem_student_code_ids != t5_pad_token_id).long().to(device)\n        expected_attention_mask = (problem_expected_code_ids != t5_pad_token_id).long().to(device)\n        metacognition_attention_mask = (metacognition_prompt_ids != t5_pad_token_id).long().to(device)\n\n        #encoding the decoder inputs for cross attention\n        # metacognition_prompt_encoded = t5_encoder(\n        #                                             input_ids=metacognition_prompt_ids,\n        #                                             attention_mask=metacognition_attention_mask).last_hidden_state\n\n        # problem_expected_code_encoded = t5_encoder(\n        #                                             input_ids=problem_expected_code_ids,\n        #                                             attention_mask=expected_attention_mask).last_hidden_state      \n        \n\n\n        optimizer.zero_grad()\n        logits = model(metacognition_prompt_ids,\n                       problem_student_code_ids ,\n                       problem_expected_code_ids,\n                      metacognition_attention_mask,\n                      expected_attention_mask)\n\n\n        logits = logits.view(-1, logits.size(-1))\n        \n        target_ids = target_ids.view(-1)\n        # print(logits.shape)\n        # print(target_ids.shape)\n\n\n        loss = LOSS(logits, target_ids)\n        total_loss += loss.item()\n\n\n        loss.backward()\n        # for name, param in model.named_parameters():\n        #     if 'context_encoder' in name:\n        #         assert param.grad is None, f\"Gradients found in frozen encoder {name}\"\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n\n        if idx % 10 == 0:\n            print(f\"Batch {idx + 1}/{len(train_dataloader)} | Loss: {loss.item():.4f}\" , end='\\r')\n\n\n\n\n\n\n    if epoch % 5 ==0 :\n            for name, param in model.named_parameters():\n                if param.requires_grad and param.grad is not None:\n                    print(f\"Layer: {name} | Grad Norm: {param.grad.norm().item()}\")\n                elif param.requires_grad:\n                    print(f\"Layer: {name} | Grad: None\")\n\n    if (epoch + 1) % 20 == 0:\n        checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch + 1}.pth\")\n        torch.save({\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': total_loss / max(len(train_dataloader), 1),\n        }, checkpoint_path)\n        print(f\"Checkpoint saved at {checkpoint_path}\")\n\n\n    avg_loss = total_loss / max(len(train_dataloader), 1)\n    #writer.add_scalar(\"Loss/train\", avg_loss, epoch + 1)\n    print(f\"Epoch [{epoch + 1}/{num_epochs}] completed | Average Loss: {avg_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:50:04.665806Z","iopub.execute_input":"2025-01-10T14:50:04.666104Z","iopub.status.idle":"2025-01-11T02:20:43.558065Z","shell.execute_reply.started":"2025-01-10T14:50:04.666083Z","shell.execute_reply":"2025-01-11T02:20:43.556858Z"},"id":"qBooPbJNa7JG","outputId":"c278d4f2-d0f6-408d-9237-2ae0a600ab95","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Training started for epoch 1/200\nLayer: token_embedding.weight | Grad Norm: 5.6810833815035267e-11\nLayer: position_embedding.weight | Grad Norm: 3.873159570844109e-10\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 1.2882997779684047e-08\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 5.491487264919215e-10\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 1.5508581086010054e-08\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 9.559334435138567e-10\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 8.177220678362573e-08\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 5.651207146684101e-08\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 1.1486227435852925e-07\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 1.0231259039983343e-07\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 9.826586477856836e-08\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 5.661804536316595e-08\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 1.4273085469085345e-07\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 1.0216430013088029e-07\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 1.437719845398533e-07\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 4.2653283571780776e-08\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 1.1162295976419045e-07\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 4.079948467961003e-08\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 2.1243369729972983e-08\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 2.1995367749383377e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 2.5419114990654634e-07\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 3.6088769661546394e-07\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 5.178477522349567e-07\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 6.386641757671896e-07\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 1.8815125031323987e-06\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 1.6379862017856794e-06\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 5.275002479265822e-08\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 5.774738554009673e-08\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 7.358126197232195e-08\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 6.555774945127268e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 1.95355869436753e-06\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 8.714442145674184e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 3.4087083804479335e-06\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 1.4622031585531658e-07\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 9.909927030093968e-06\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 6.434538590838201e-06\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 1.6640098692732863e-05\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 1.1594220268307254e-05\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 1.3584975022240542e-05\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 6.1291807469388004e-06\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 2.436665636196267e-05\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 1.090704790840391e-05\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 1.587194492458366e-05\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 4.5461947593139485e-06\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 1.2791303561243694e-05\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 4.7156272557913326e-06\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 6.934086286491947e-06\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 3.364016549767257e-07\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 6.745479186065495e-05\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 3.6904471926391125e-05\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 0.00013722464791499078\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 6.551451224368066e-05\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.0006010723300278187\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 0.00019960070494562387\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 2.7518388378666714e-05\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 2.7301870431983843e-05\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 0.00016047288954723626\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 0.00010945478425128385\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.0017007702263072133\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 7.516328332712874e-05\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.0031938476022332907\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 8.052658085944131e-05\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.027392476797103882\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.009062845259904861\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.040364813059568405\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.012875941582024097\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.0025032234843820333\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.002519536530598998\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.003143256064504385\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.004451512359082699\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.004652403760701418\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.0018112263642251492\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.04458277300000191\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.008437907323241234\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.008865565061569214\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.00015069026267156005\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.060946181416511536\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.023698102682828903\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.09583679586648941\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.042302414774894714\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.38109445571899414\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.13398277759552002\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.01498206052929163\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.014968838542699814\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.017257366329431534\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.016393963247537613\nLayer: final_fc.weight | Grad Norm: 0.6853713393211365\nLayer: final_fc.bias | Grad Norm: 0.027782920747995377\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [1/200] completed | Average Loss: 4.9466\nTraining started for epoch 2/200\nEpoch [2/200] completed | Average Loss: 4.7381\nTraining started for epoch 3/200\nEpoch [3/200] completed | Average Loss: 4.7232\nTraining started for epoch 4/200\nEpoch [4/200] completed | Average Loss: 4.7175\nTraining started for epoch 5/200\nEpoch [5/200] completed | Average Loss: 4.7144\nTraining started for epoch 6/200\nLayer: token_embedding.weight | Grad Norm: 4.37277688017379e-12\nLayer: position_embedding.weight | Grad Norm: 2.8801232232478213e-11\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 9.48080836060683e-10\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 4.070242753950737e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 1.131948423882534e-09\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 7.06111488724126e-11\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 6.143068276287522e-09\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 4.0538270518197805e-09\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 8.351688229879528e-09\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 7.010991343037176e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 6.970797272742857e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 3.990659358521498e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 9.789863142373179e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 6.973736255133645e-09\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 9.215386675975878e-09\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 2.692428280326453e-09\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 7.966938220249631e-09\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 2.770321527734154e-09\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 1.6732262242413753e-09\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 1.7429518939238164e-10\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 1.8654302991194527e-08\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 2.5885677601422685e-08\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 3.646754720421086e-08\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 4.463864300419118e-08\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 1.3238776830348797e-07\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 1.1471241379013009e-07\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 4.32433377994812e-09\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 4.254961716299022e-09\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 5.84994275243389e-09\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 4.902758643510197e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 1.4111076040990156e-07\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 6.308516375952422e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 2.2797227927640051e-07\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 9.977853210330068e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 8.017304935492575e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 5.463004413286399e-07\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 1.2476455140131293e-06\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 9.107245659834007e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 1.0782125627883943e-06\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 5.245670990916551e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 1.8569933217804646e-06\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 8.617129765298159e-07\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 1.319250827691576e-06\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 3.906879157966614e-07\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 1.0787998689920641e-06\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 3.6945789361197967e-07\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 4.938017923450388e-07\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 2.4075315963045796e-08\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 6.054295681678923e-06\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 2.9669433843082516e-06\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 1.4919804016244598e-05\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 5.340269581211032e-06\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 8.701170736458153e-05\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 1.7038486475939862e-05\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 5.612188033410348e-06\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 6.376325018209172e-06\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 3.3743108360795304e-05\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 1.9089151464868337e-05\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.0003564724465832114\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 1.580513344379142e-05\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.0005964651936665177\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 1.8513292161514983e-05\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.010930345393717289\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.0032700092997401953\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.01760847307741642\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.003732707118615508\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.0003362993011251092\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.0006893575191497803\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.0004713070811703801\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.0013073323061689734\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.0009240590734407306\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.0004206053854431957\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.01773589663207531\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.0032688083592802286\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.0014084813883528113\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 2.9588674806291237e-05\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.025251803919672966\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.005852621514350176\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.047227974981069565\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.010258305817842484\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.26309916377067566\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.033170443028211594\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.015208655036985874\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.01664804480969906\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.018850307911634445\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.018550440669059753\nLayer: final_fc.weight | Grad Norm: 0.9607670903205872\nLayer: final_fc.bias | Grad Norm: 0.040039729326963425\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [6/200] completed | Average Loss: 4.7118\nTraining started for epoch 7/200\nEpoch [7/200] completed | Average Loss: 4.7105\nTraining started for epoch 8/200\nEpoch [8/200] completed | Average Loss: 4.7092\nTraining started for epoch 9/200\nEpoch [9/200] completed | Average Loss: 4.7084\nTraining started for epoch 10/200\nEpoch [10/200] completed | Average Loss: 4.7076\nTraining started for epoch 11/200\nLayer: token_embedding.weight | Grad Norm: 1.7218304690022612e-12\nLayer: position_embedding.weight | Grad Norm: 9.309699712523045e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 2.9572053139581556e-10\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 1.256456917675397e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 3.356180611202575e-10\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 2.090671386012488e-11\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 1.905283708580896e-09\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 1.2976262286912288e-09\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 2.6419846310687944e-09\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 2.2958890344426663e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 2.1827948337715952e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 1.2935092996713138e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 3.0667794970185014e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 2.294207934738779e-09\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 2.953653766013531e-09\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 8.937912632234202e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 2.4745374638257545e-09\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 8.742038204445635e-10\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 4.780697504536136e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 5.011661571852066e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 5.422546411892881e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 7.886257868960911e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 1.0822623686124189e-08\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 1.3565077061628017e-08\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 4.0842181192601856e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 3.592134945051839e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 1.4004163384484514e-09\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 1.3193831582825055e-09\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 1.6306760386441965e-09\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 1.3996475090038984e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 4.372180129053049e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 1.960424045321929e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 8.126194472879433e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 3.541511306437428e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 2.783760919555789e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 1.9535305284534843e-07\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 4.430050921655493e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 3.21019030025127e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 3.6210511211720586e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 1.8306135984857974e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 6.299995902736555e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 2.9498102094294154e-07\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 4.3935699522990035e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 1.3519559161068173e-07\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 3.8178325212356867e-07\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 1.3592081415936264e-07\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 1.7446373590246367e-07\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 8.383571170611503e-09\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 2.2330427782435436e-06\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 1.0305487876394182e-06\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 6.446015959227225e-06\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 1.8863659079215722e-06\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 5.220372986514121e-05\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 6.9589063969033305e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 5.11297776029096e-06\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 5.216347290115664e-06\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 1.4794487469771411e-05\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 7.953452950459905e-06\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.0002043749118456617\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 9.108458471018821e-06\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.00040499825263395905\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 1.5769972378620878e-05\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.0067658014595508575\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.001337550114840269\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.011323779821395874\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.0013989706058055162\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.0010314076207578182\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.0003224726242478937\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.0019496831810101867\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.0006042022141627967\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.001974058337509632\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.00022792664822191\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.009292983449995518\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.0014044580748304725\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.0008408173453062773\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 2.249088356620632e-05\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.014794394373893738\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.0021497588604688644\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.026085752993822098\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.003870435757562518\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.1380791813135147\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.010838217101991177\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.010866380296647549\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.011065940372645855\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.011384462006390095\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.010950766503810883\nLayer: final_fc.weight | Grad Norm: 0.5108087658882141\nLayer: final_fc.bias | Grad Norm: 0.021069467067718506\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [11/200] completed | Average Loss: 4.7071\nTraining started for epoch 12/200\nEpoch [12/200] completed | Average Loss: 4.7064\nTraining started for epoch 13/200\nEpoch [13/200] completed | Average Loss: 4.7060\nTraining started for epoch 14/200\nEpoch [14/200] completed | Average Loss: 4.7055\nTraining started for epoch 15/200\nEpoch [15/200] completed | Average Loss: 4.7051\nTraining started for epoch 16/200\nLayer: token_embedding.weight | Grad Norm: 1.559256630062622e-12\nLayer: position_embedding.weight | Grad Norm: 7.639422948402252e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 2.5340426978992525e-10\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 1.0753945477171012e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 2.9230257103662893e-10\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 1.8329100390235276e-11\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 1.7743126967673106e-09\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 1.192938303695712e-09\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 2.3619344258207775e-09\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 2.0829660218879553e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 2.0724872928923332e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 1.1925378462507297e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 2.8866053991549734e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 2.073779148403787e-09\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 2.819432465273053e-09\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 8.247109106740425e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 2.4381785479477003e-09\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 8.428744369126662e-10\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 4.349877957832149e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 4.623955038307592e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 5.402672087484461e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 7.771507881670914e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 1.1419211354279923e-08\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 1.42249616530421e-08\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 4.304970957491605e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 3.786291458141022e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 1.4798965386475516e-09\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 1.39457823067346e-09\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 1.5588348389883322e-09\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 1.423658857468979e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 4.433525901959001e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 1.9936956530131056e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 7.774864485554644e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 3.3771259122516994e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 2.625475019613077e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 1.6456128548725246e-07\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 4.713742498552165e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 2.9022410785728425e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 2.950983741811797e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 1.3811619226089533e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 6.071074949431932e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 2.508093643882603e-07\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 3.2255925930257945e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 9.60783737014026e-08\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 3.6821339222115057e-07\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 1.2780435554304859e-07\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 1.7341608327114955e-07\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 8.233393522516508e-09\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 2.467388640070567e-06\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 9.357138992527325e-07\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 8.394648830289952e-06\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 1.8060472939396277e-06\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 8.021361281862482e-05\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 6.773168479412561e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 9.264182153856382e-06\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 1.0040245797426905e-05\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 3.214510434190743e-05\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 1.4508698768622708e-05\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.00039769394788891077\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 1.7818838387029245e-05\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.0005475137731991708\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 2.6734176572063006e-05\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.011171647347509861\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.002816550200805068\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.016697214916348457\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.0024943554308265448\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.0014133155345916748\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.00042827666038647294\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.002528676064684987\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.0006962710758671165\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.00285563082434237\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.0004449266707524657\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.01630757935345173\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.0033252364955842495\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.0013590636663138866\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 4.737514245789498e-05\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.020022714510560036\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.0025655857753008604\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.03477539122104645\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.0036026446614414454\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.15550225973129272\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.00846010074019432\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.012713359668850899\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.01541374996304512\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.0172436460852623\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.018122512847185135\nLayer: final_fc.weight | Grad Norm: 0.9849944114685059\nLayer: final_fc.bias | Grad Norm: 0.04650352895259857\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [16/200] completed | Average Loss: 4.7045\nTraining started for epoch 17/200\nEpoch [17/200] completed | Average Loss: 4.7040\nTraining started for epoch 18/200\nEpoch [18/200] completed | Average Loss: 4.7038\nTraining started for epoch 19/200\nEpoch [19/200] completed | Average Loss: 4.7032\nTraining started for epoch 20/200\nCheckpoint saved at ./checkpoints/model_epoch_20.pth\nEpoch [20/200] completed | Average Loss: 4.7026\nTraining started for epoch 21/200\nLayer: token_embedding.weight | Grad Norm: 2.26359989780045e-12\nLayer: position_embedding.weight | Grad Norm: 1.429285856335838e-11\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 4.820644439185173e-10\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 2.0799693564721444e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 5.948205261674389e-10\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 3.750974877925195e-11\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 3.339343690456076e-09\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 2.3442110475002664e-09\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 4.518812435350128e-09\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 4.1576275755517145e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 4.09979561410978e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 2.372230856195756e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 5.7236779760216905e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 4.140800147212076e-09\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 5.4710631580690006e-09\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 1.6008346870322043e-09\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 4.6932155939316544e-09\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 1.7306992505794483e-09\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 8.283897456884404e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 8.878889984353933e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 1.035053465159308e-08\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 1.5115006135602016e-08\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 2.0818610835249274e-08\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 2.6257568563892164e-08\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 7.813101632336839e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 6.944634378669434e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 2.4037500878648643e-09\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 2.5432849160011983e-09\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 2.4736184212059698e-09\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 2.564984447062102e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 8.537586637658023e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 3.85047016493445e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 1.5468901892745635e-07\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 6.751745829092215e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 5.132237106408866e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 3.5355063232600514e-07\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 9.10682388166606e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 6.072764335840475e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 5.64225899779558e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 2.69550923803763e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 1.2259265531611163e-06\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 5.131191187501827e-07\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 6.03333717208443e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 1.806200913279099e-07\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 6.701833967781567e-07\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 2.475760823017481e-07\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 3.5471529713504424e-07\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 1.690467854587041e-08\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 5.096972017781809e-06\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 1.9065930700890021e-06\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 1.8065253243548796e-05\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 3.4676006634981604e-06\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.00020513535127975047\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 1.3851057701685932e-05\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 3.1841311283642426e-05\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 2.985196260851808e-05\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 9.397438407177106e-05\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 4.060922947246581e-05\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.0012850419152528048\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 5.788951239082962e-05\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.001717016682960093\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 9.688982390798628e-05\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.010116804391145706\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.0023861266672611237\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.016515547409653664\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.0021403732243925333\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.009582220576703548\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.0012181447818875313\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.009445671923458576\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.001159221981652081\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.018278561532497406\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.0021102773025631905\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.013878978788852692\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.002605474554002285\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.004206162411719561\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.00011366667604306713\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.024634074419736862\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.002178044058382511\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.03319227695465088\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.002385002328082919\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.12187635898590088\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.003864900441840291\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.011982988566160202\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.011508115567266941\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.019345683977007866\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.015792541205883026\nLayer: final_fc.weight | Grad Norm: 0.8473592400550842\nLayer: final_fc.bias | Grad Norm: 0.03176671266555786\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [21/200] completed | Average Loss: 4.7022\nTraining started for epoch 22/200\nEpoch [22/200] completed | Average Loss: 4.7016\nTraining started for epoch 23/200\nEpoch [23/200] completed | Average Loss: 4.7011\nTraining started for epoch 24/200\nEpoch [24/200] completed | Average Loss: 4.7004\nTraining started for epoch 25/200\nEpoch [25/200] completed | Average Loss: 4.6996\nTraining started for epoch 26/200\nLayer: token_embedding.weight | Grad Norm: 1.3292551738139369e-12\nLayer: position_embedding.weight | Grad Norm: 8.6912681912521e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 2.769453555373502e-10\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 1.1994120974190281e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 3.22699950094929e-10\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 2.0535758857298525e-11\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 1.9392902839143744e-09\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 1.3885291805237898e-09\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 2.59330534824187e-09\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 2.434296098030586e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 2.3875339483225844e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 1.3908010298990803e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 3.276253268680307e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 2.4003670162642265e-09\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 3.069946963307757e-09\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 9.041863369141367e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 2.5459210295508683e-09\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 9.46537404011849e-10\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 4.833096700629369e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 5.228388208489143e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 5.3854547488185744e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 8.048741229060852e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 1.0615655909873567e-08\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 1.3552708288955273e-08\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 4.0251059374440956e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 3.60716043701359e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 1.3265761822367494e-09\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 1.3144036969947592e-09\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 1.4325903796574835e-09\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 1.3414963584423845e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 4.304580514258305e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 1.9471202428178458e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 7.919479827478426e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 3.475362220228817e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 2.5228612798855465e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 1.6964462190571794e-07\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 4.4280409383645747e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 2.793555893276789e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 2.72883113439093e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 1.2584814612637274e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 5.76413185626734e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 2.2564708501704445e-07\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 2.9418680469461833e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 8.834652476252813e-08\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 3.422454710744205e-07\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 1.2760429513036797e-07\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 1.7405892549504642e-07\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 8.311350718770427e-09\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 2.6954930945066735e-06\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 9.058754244506417e-07\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 1.0032915270130616e-05\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 1.6068743207142688e-06\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.0001225340529344976\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 6.386371296684956e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 2.276394843647722e-05\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 1.9725806851056404e-05\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 2.674917232070584e-05\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 2.1575153368758038e-05\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.000775089138187468\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 3.5118184314342216e-05\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.0010997550562024117\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 7.238371472340077e-05\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.007897076196968555\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.0018075070111081004\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.012908105738461018\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.0013862645719200373\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.008177238516509533\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.0008170320652425289\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.0085185207426548\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.0006831122445873916\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.012022853828966618\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.0013469679979607463\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.010817266069352627\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.0022467407397925854\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.0031641919631510973\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 9.918469004333019e-05\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.020449884235858917\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.0013460458721965551\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.028803111985325813\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.0014783130027353764\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.10772901028394699\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.0034968710970133543\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.010300813242793083\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.01191998552531004\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.01287210825830698\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.013311246410012245\nLayer: final_fc.weight | Grad Norm: 0.6804384589195251\nLayer: final_fc.bias | Grad Norm: 0.03447216376662254\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [26/200] completed | Average Loss: 4.6990\nTraining started for epoch 27/200\nEpoch [27/200] completed | Average Loss: 4.6984\nTraining started for epoch 28/200\nEpoch [28/200] completed | Average Loss: 4.6976\nTraining started for epoch 29/200\nEpoch [29/200] completed | Average Loss: 4.6969\nTraining started for epoch 30/200\nEpoch [30/200] completed | Average Loss: 4.6960\nTraining started for epoch 31/200\nLayer: token_embedding.weight | Grad Norm: 1.1447343723977599e-12\nLayer: position_embedding.weight | Grad Norm: 6.791042121007118e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 2.3397864201690766e-10\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 1.016659586378088e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 2.9273625190562313e-10\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 1.8744283902805137e-11\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 1.549944173007134e-09\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 1.0972851516299897e-09\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 2.1936554794876884e-09\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 2.0406119016769253e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 1.8761354692031773e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 1.08910946927665e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 2.759351414027833e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 2.028736068027115e-09\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 2.6574802358680927e-09\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 7.779571431498766e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 2.040835500594085e-09\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 7.410677627106566e-10\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 3.9936734474999014e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 4.374859502442874e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 4.8200083924143655e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 7.260042345080819e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 9.764843156290226e-09\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 1.255316028903053e-08\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 3.7456956647474726e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 3.3729204318433403e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 1.2585892328331738e-09\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 1.226357904116071e-09\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 1.3613222771269307e-09\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 1.262052573558492e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 3.993735830931655e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 1.8119499234359182e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 7.117640876685982e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 3.137731630076246e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 2.1408621364571445e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 1.4736005482518522e-07\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 3.8781567468504363e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 2.512441596991266e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 2.4717735414014896e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 1.1510039854556453e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 5.37203675321507e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 2.1062092514512187e-07\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 2.5937529812836146e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 7.867976137276855e-08\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 2.8842461574640765e-07\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 1.0620446033726694e-07\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 1.6492182908223185e-07\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 7.904982446405029e-09\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 2.417122914266656e-06\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 7.95478399595595e-07\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 9.947975740942638e-06\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 1.4573569160347688e-06\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.00012871490616817027\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 5.6865169426600914e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 1.9607896319939755e-05\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 2.28371918638004e-05\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 0.00010722228762460873\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 3.455769910942763e-05\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.0012891971273347735\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 5.8746489230543375e-05\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.0014258322771638632\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 0.00010861390182981268\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.008454042486846447\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.0012409405317157507\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.01374888326972723\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.0010779168223962188\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.010707173496484756\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.000805316842161119\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.009666668251156807\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.0006635626195929945\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.014635241590440273\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.0017932918854057789\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.008904993534088135\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.00142819550819695\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.004524554591625929\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.00012442957086022943\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.024431725963950157\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.0012312654871493578\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.044298410415649414\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.0017998209223151207\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.12269455194473267\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.0026937841903418303\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.009851835668087006\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.010596622712910175\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.010979427956044674\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.009929347783327103\nLayer: final_fc.weight | Grad Norm: 0.4572718143463135\nLayer: final_fc.bias | Grad Norm: 0.01597043313086033\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [31/200] completed | Average Loss: 4.6952\nTraining started for epoch 32/200\nEpoch [32/200] completed | Average Loss: 4.6944\nTraining started for epoch 33/200\nEpoch [33/200] completed | Average Loss: 4.6934\nTraining started for epoch 34/200\nEpoch [34/200] completed | Average Loss: 4.6923\nTraining started for epoch 35/200\nEpoch [35/200] completed | Average Loss: 4.6913\nTraining started for epoch 36/200\nLayer: token_embedding.weight | Grad Norm: 9.215058187003744e-13\nLayer: position_embedding.weight | Grad Norm: 5.797817087538348e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 1.9767455727848215e-10\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 8.605418461149483e-12\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 2.2949202260758028e-10\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 1.4798441985708344e-11\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 1.3302701162842823e-09\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 9.175812887285417e-10\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 1.9032913023409037e-09\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 1.7082759651287915e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 1.5871025604852207e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 9.098800601847756e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 2.3332111798168853e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 1.6996352103504364e-09\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 2.0626111929544777e-09\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 5.908067923776628e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 1.828750262333756e-09\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 6.453458323285588e-10\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 3.412368720923098e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 3.772589532435866e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 3.959121919194786e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 5.823069582078233e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 8.247405425265697e-09\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 1.0490345481173335e-08\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 3.1432357161520486e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 2.827799505666917e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 1.127382631693763e-09\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 1.0322304122567516e-09\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 1.144968231336918e-09\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 1.0350177381823755e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 3.297143891245469e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 1.5004667508478065e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 6.122535722852263e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 2.702913892704828e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 1.8637133791798988e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 1.2206770350076113e-07\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 3.5933220488004736e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 2.1837132635482703e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 2.3043335772854334e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 1.0566500918685051e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 4.822854862140957e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 1.8423672543121938e-07\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 2.3123568837490893e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 6.899825422124195e-08\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 2.4399520270890207e-07\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 8.657101346898344e-08\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 1.4437716799875489e-07\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 6.8938645902960616e-09\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 2.2119111235952005e-06\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 6.811910679971334e-07\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 9.663770470069721e-06\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 1.2401827689245692e-06\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.00015409466868732125\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 5.599806627287762e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 3.0528604838764295e-05\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 2.896576734201517e-05\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 0.00021142393234185874\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 5.524489824892953e-05\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.002186099998652935\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 0.00010012093116529286\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.001819971133954823\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 0.00015042226004879922\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.006453223526477814\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.0010474987793713808\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.01125935185700655\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.0009141987538896501\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.013119238428771496\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.0007716093678027391\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.0100695276632905\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.0006408925983123481\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.012553722597658634\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.0012382371351122856\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.007002928294241428\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.001263934187591076\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.004933854565024376\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.00012312945909798145\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.02145395241677761\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.0009785089641809464\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.03629878908395767\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.0012333974009379745\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.1023586168885231\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.004466827027499676\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.008930226787924767\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.01363714225590229\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.022725291550159454\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.020421918481588364\nLayer: final_fc.weight | Grad Norm: 0.9914769530296326\nLayer: final_fc.bias | Grad Norm: 0.05299710854887962\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [36/200] completed | Average Loss: 4.6904\nTraining started for epoch 37/200\nEpoch [37/200] completed | Average Loss: 4.6892\nTraining started for epoch 38/200\nEpoch [38/200] completed | Average Loss: 4.6881\nTraining started for epoch 39/200\nEpoch [39/200] completed | Average Loss: 4.6868\nTraining started for epoch 40/200\nCheckpoint saved at ./checkpoints/model_epoch_40.pth\nEpoch [40/200] completed | Average Loss: 4.6857\nTraining started for epoch 41/200\nLayer: token_embedding.weight | Grad Norm: 1.2722974800441489e-12\nLayer: position_embedding.weight | Grad Norm: 8.058741174377104e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 2.7358998400117684e-10\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 1.1901243011924745e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 3.345631549578343e-10\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 2.146392091839644e-11\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 2.2513155784054106e-09\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 1.5280402498873968e-09\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 2.8210100921910453e-09\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 2.5108961576592037e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 2.390082354253309e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 1.415143446870104e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 3.308211482533352e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 2.499808804401482e-09\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 3.289946093332219e-09\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 9.732261663231156e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 3.0060911537788115e-09\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 1.0433323094360958e-09\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 4.90585072565608e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 5.431161423374853e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 6.033199717592197e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 8.97843754898986e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 1.2686689032648246e-08\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 1.6224865007075095e-08\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 4.7709576733723225e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 4.3291915829968275e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 1.4847273410723005e-09\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 1.5734171743275738e-09\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 1.6493146848262086e-09\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 1.6364422039671922e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 5.119217405535892e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 2.3364203904918668e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 9.136322631775329e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 4.0262051470563165e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 2.810548664911039e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 1.7386835793331556e-07\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 5.183767370908754e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 3.0056571631575935e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 3.4070927767970716e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 1.5803105668510398e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 6.649643182754517e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 2.58061305657975e-07\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 4.424027224558813e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 1.3624780592635943e-07\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 3.8316011341521516e-07\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 1.3014732758165337e-07\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 2.137827266324166e-07\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 1.0139095785177688e-08\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 3.2874447697395226e-06\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 9.857748182184878e-07\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 1.4898932931828313e-05\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 1.7529116576042725e-06\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.00025665294378995895\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 8.090089977486059e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 6.811392813688144e-05\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 5.087516547064297e-05\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 0.0005776439793407917\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 0.00013162930554244667\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.0056253457441926\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 0.0002589596260804683\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.003819984383881092\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 0.000344748143106699\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.007928182370960712\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.0015203829389065504\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.015164526179432869\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.0013234688667580485\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.025527117773890495\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.0012895967811346054\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.015664437785744667\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.0009048033971339464\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.021931715309619904\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.0021689406130462885\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.009110329672694206\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.0016180089442059398\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.008524307981133461\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.00019460504699964076\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.03238239884376526\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.0014303248608484864\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.04899764806032181\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.0016929509583860636\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.13698090612888336\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.005488344002515078\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.011087033897638321\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.01584245078265667\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.02073192037642002\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.02102052979171276\nLayer: final_fc.weight | Grad Norm: 0.9857351779937744\nLayer: final_fc.bias | Grad Norm: 0.054253388196229935\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [41/200] completed | Average Loss: 4.6844\nTraining started for epoch 42/200\nEpoch [42/200] completed | Average Loss: 4.6832\nTraining started for epoch 43/200\nEpoch [43/200] completed | Average Loss: 4.6820\nTraining started for epoch 44/200\nEpoch [44/200] completed | Average Loss: 4.6808\nTraining started for epoch 45/200\nEpoch [45/200] completed | Average Loss: 4.6793\nTraining started for epoch 46/200\nLayer: token_embedding.weight | Grad Norm: 1.80796263377081e-12\nLayer: position_embedding.weight | Grad Norm: 1.3256583331067162e-11\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 4.2704195735154826e-10\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 1.8744185023567006e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 4.972212641618512e-10\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 3.232178066858715e-11\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 3.394003966761261e-09\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 2.399261012087095e-09\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 4.627248806343687e-09\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 4.32264357641543e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 4.1224468283473925e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 2.420289968441125e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 5.7025038024960395e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 4.246627050008556e-09\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 5.846040984636147e-09\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 1.7149591746701276e-09\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 4.491774063808407e-09\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 1.6268816294129351e-09\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 7.462654383338929e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 8.36873764731294e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 1.0050645649073431e-08\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 1.5237372252840942e-08\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 2.0516889520649784e-08\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 2.6623016680105138e-08\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 7.944393587422383e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 7.26155633401504e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 2.7977082872610026e-09\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 2.6280038145642948e-09\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 3.2747300426905213e-09\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 2.8118423145429006e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 8.208890278638137e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 3.7579344080995725e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 1.5326712343721738e-07\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 6.7538206138806345e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 4.060551930251677e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 2.747889027432393e-07\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 7.919695121927361e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 4.971882390236715e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 4.6490538352372823e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 2.1796850546706992e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 1.084471136891807e-06\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 4.1711038534231193e-07\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 4.6445970269815007e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 1.4212596965990087e-07\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 5.488316219270928e-07\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 1.9476080126423767e-07\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 3.4914285151899094e-07\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 1.6466305652329538e-08\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 5.188596787775168e-06\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 1.534252987767104e-06\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 2.7812071493826807e-05\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 3.0858145692036487e-06\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.0005039334064349532\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 1.4356887731992174e-05\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 0.0001450816635042429\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 0.0001045160970534198\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 0.0012607109965756536\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 0.0002792689483612776\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.011892281472682953\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 0.0005499894614331424\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.006524023599922657\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 0.0006221724324859679\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.009859963320195675\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.001264899387024343\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.01987365446984768\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.0013420770410448313\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.03512173518538475\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.0017807797994464636\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.02424008771777153\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.0010700307320803404\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.02829000912606716\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.0024769147858023643\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.010462228208780289\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.0015312617179006338\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.015567103400826454\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.0002864632406271994\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.049161747097969055\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.0016137838829308748\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.08281373232603073\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.0022418948356062174\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.1689574122428894\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.008635465055704117\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.011501986533403397\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.014894049614667892\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.018604297190904617\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.01834173873066902\nLayer: final_fc.weight | Grad Norm: 0.7496261596679688\nLayer: final_fc.bias | Grad Norm: 0.03910009562969208\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [46/200] completed | Average Loss: 4.6781\nTraining started for epoch 47/200\nEpoch [47/200] completed | Average Loss: 4.6768\nTraining started for epoch 48/200\nEpoch [48/200] completed | Average Loss: 4.6754\nTraining started for epoch 49/200\nEpoch [49/200] completed | Average Loss: 4.6740\nTraining started for epoch 50/200\nEpoch [50/200] completed | Average Loss: 4.6726\nTraining started for epoch 51/200\nLayer: token_embedding.weight | Grad Norm: 2.043642598897888e-12\nLayer: position_embedding.weight | Grad Norm: 9.352033904230783e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 3.136388093682996e-10\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 1.3569553466985518e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 3.4034441931396486e-10\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 2.1896323296188847e-11\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 2.0441794923442558e-09\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 1.3272826171473184e-09\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 2.8212281399930816e-09\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 2.4811044330164123e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 2.275725385914029e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 1.3259073838867153e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 3.2855336229431487e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 2.4598760806071596e-09\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 3.1134812505939635e-09\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 9.090962982405415e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 2.893617567778506e-09\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 9.550283897041822e-10\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 5.121855717327151e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 5.732929408974741e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 6.1444862531345734e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 9.244901733040933e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 1.2398587045936438e-08\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 1.610706412691343e-08\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 4.634571837414114e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 4.254164664985183e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 1.5450424273311114e-09\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 1.5369255867980769e-09\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 2.0010375578749517e-09\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 1.7481432967869637e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 5.1647091936501965e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 2.370821761132902e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 9.330777572813531e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 4.095154437777637e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 3.462051836322644e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 1.9779442084200127e-07\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 6.069529376873106e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 3.303598532511387e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 3.3115159681074147e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 1.5972591427271254e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 6.715847575833322e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 2.6713075840234524e-07\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 3.677427287129831e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 1.1159975343844053e-07\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 4.693942514677474e-07\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 1.456689773249309e-07\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 2.2401499677471293e-07\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 1.043553599089364e-08\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 3.6789044770557666e-06\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 1.0619515933285584e-06\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 1.9946455722674727e-05\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 2.057192432403099e-06\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.000374859751900658\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 9.524962479190435e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 0.00011988187907263637\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 8.206161874113604e-05\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 0.00034413623507134616\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 0.00011816731421276927\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.005278395488858223\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 0.00024539107107557356\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.00782681442797184\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 0.0008199899457395077\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.01475314237177372\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.002504295203834772\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.0367165245115757\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.0021201842464506626\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.06644324958324432\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.0043521844781935215\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.04196818917989731\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.0015446854522451758\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.0643644630908966\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.008613012731075287\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.015943843871355057\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.0025512753054499626\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.026027852669358253\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.0004951923037879169\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.0866193100810051\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.0024599016178399324\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.14719747006893158\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.003501719795167446\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.271098256111145\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.008909637108445168\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.014475470408797264\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.016335858032107353\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.0169101282954216\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.012684589251875877\nLayer: final_fc.weight | Grad Norm: 0.9395577311515808\nLayer: final_fc.bias | Grad Norm: 0.02290896698832512\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [51/200] completed | Average Loss: 4.6715\nTraining started for epoch 52/200\nEpoch [52/200] completed | Average Loss: 4.6701\nTraining started for epoch 53/200\nEpoch [53/200] completed | Average Loss: 4.6688\nTraining started for epoch 54/200\nEpoch [54/200] completed | Average Loss: 4.6673\nTraining started for epoch 55/200\nEpoch [55/200] completed | Average Loss: 4.6661\nTraining started for epoch 56/200\nLayer: token_embedding.weight | Grad Norm: 7.263042706324996e-13\nLayer: position_embedding.weight | Grad Norm: 4.432287436378912e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 1.48068432515025e-10\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 6.512154530902148e-12\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 1.8471398022246177e-10\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 1.2120600530185488e-11\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 1.217783651696891e-09\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 8.874429524574623e-10\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 1.5677164011407285e-09\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 1.5264531860736952e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 1.3960388400846568e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 8.4946782941131e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 1.932298987483705e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 1.5072066927857009e-09\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 1.921594661169479e-09\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 5.828643678817969e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 1.6422831983220476e-09\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 6.285096332270257e-10\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 2.7888436004985806e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 3.170361542736977e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 3.544671889343931e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 5.7025957289624785e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 7.449443728546612e-09\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 9.996411698409702e-09\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 2.7478350261844753e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 2.5654973256905578e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 9.75735159336466e-10\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 9.150437074723072e-10\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 9.737664008468983e-10\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 9.20973131091074e-10\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 2.9787260658054038e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 1.3715608648823263e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 5.6354682698156466e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 2.465573523124931e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 2.0386954702189541e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 1.4132164949387516e-07\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 3.744503942471056e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 2.339727558364757e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 1.986732485192988e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 1.0213660317504036e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 4.489699563237082e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 1.9065429057718575e-07\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 2.224831945341066e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 7.048889472116571e-08\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 2.6066035729854775e-07\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 9.943423862068812e-08\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 1.4188684360760817e-07\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 6.549754960616383e-09\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 2.3680534013692522e-06\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 7.247895723594411e-07\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 1.3562492313212715e-05\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 1.4165539141686168e-06\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.0002990642096847296\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 7.394161002594046e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 9.963406046153978e-05\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 6.677825876977295e-05\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 0.000886531372088939\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 0.0001811926340451464\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.008978473953902721\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 0.0004195218498352915\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.004612329415977001\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 0.0005198071012273431\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.01012484822422266\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.001579242991283536\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.022314319387078285\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.0015039710560813546\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.01806112565100193\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.0014084558933973312\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.02386327087879181\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.0009923662291839719\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.014935213141143322\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.0013537347549572587\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.009971125982701778\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.001639411086216569\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.015544500201940536\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.0002739970514085144\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.059086915105581284\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.0017115810187533498\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.09211017191410065\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.002239176305010915\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.18276318907737732\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.005858066026121378\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.010653126053512096\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.012625536881387234\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.013345235958695412\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.012145612388849258\nLayer: final_fc.weight | Grad Norm: 0.6503942012786865\nLayer: final_fc.bias | Grad Norm: 0.023682696744799614\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [56/200] completed | Average Loss: 4.6649\nTraining started for epoch 57/200\nEpoch [57/200] completed | Average Loss: 4.6635\nTraining started for epoch 58/200\nEpoch [58/200] completed | Average Loss: 4.6623\nTraining started for epoch 59/200\nEpoch [59/200] completed | Average Loss: 4.6610\nTraining started for epoch 60/200\nCheckpoint saved at ./checkpoints/model_epoch_60.pth\nEpoch [60/200] completed | Average Loss: 4.6598\nTraining started for epoch 61/200\nLayer: token_embedding.weight | Grad Norm: 3.7640222427630377e-13\nLayer: position_embedding.weight | Grad Norm: 2.52645213777436e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 8.637328352545381e-11\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 3.822476189740964e-12\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 1.0343734896389734e-10\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 6.869035722167904e-12\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 6.591970858060847e-10\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 4.889886828784995e-10\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 8.579348897974626e-10\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 8.473309276446628e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 7.610863606011264e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 4.563723010608811e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 1.0803344885346178e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 8.314225419248089e-10\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 1.008735872609634e-09\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 3.010116045309985e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 9.171974846289288e-10\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 3.499794898331743e-10\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 1.4845941143093455e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 1.722025647077352e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 1.8781820543267713e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 2.9828532976949873e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 3.785853852633636e-09\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 5.057167573596644e-09\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 1.3789251518403489e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 1.289181561503483e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 4.4872863758094184e-10\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 4.6062531566803955e-10\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 7.665624246477876e-10\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 5.782859191505452e-10\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 1.696326989986119e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 7.835216364604491e-10\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 3.171653517597406e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 1.3789254182938748e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 8.990870981051557e-08\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 6.241053540634312e-08\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 1.8029527382168453e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 1.1405651179074994e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 9.723999738753264e-08\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 4.983041890227469e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 2.1255081605886517e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 8.988042310420497e-08\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 1.1573760616556683e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 3.616011312601586e-08\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 1.1946252698180615e-07\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 4.295819167055015e-08\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 7.619995301411109e-08\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 3.459439845698853e-09\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 1.2212100273245596e-06\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 3.7059112401038874e-07\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 7.211587671918096e-06\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 7.267769319696527e-07\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.0001468990376451984\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 3.401190269869403e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 4.9740516260499135e-05\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 3.4642205719137564e-05\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 0.00022980925859883428\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 5.847660213476047e-05\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.002928375266492367\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 0.0001373994891764596\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.0031860736198723316\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 0.0003682493115775287\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.007038574665784836\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.0011186526389792562\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.01639704965054989\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.001082730246707797\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.023361848667263985\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.0017127225873991847\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.02086297608911991\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.0009624536032788455\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.015843162313103676\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.0018560067983344197\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.007203382905572653\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.0011753370054066181\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.01303053554147482\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.00021389551693573594\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.0486585758626461\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.0013185238931328058\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.08863672614097595\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.0018010808853432536\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.17911934852600098\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.008265918120741844\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.01241539977490902\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.015513334423303604\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.022805145010352135\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.020555824041366577\nLayer: final_fc.weight | Grad Norm: 0.9752929210662842\nLayer: final_fc.bias | Grad Norm: 0.05728742852807045\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [61/200] completed | Average Loss: 4.6586\nTraining started for epoch 62/200\nEpoch [62/200] completed | Average Loss: 4.6573\nTraining started for epoch 63/200\nEpoch [63/200] completed | Average Loss: 4.6560\nTraining started for epoch 64/200\nEpoch [64/200] completed | Average Loss: 4.6551\nTraining started for epoch 65/200\nEpoch [65/200] completed | Average Loss: 4.6542\nTraining started for epoch 66/200\nLayer: token_embedding.weight | Grad Norm: 7.310298742214949e-13\nLayer: position_embedding.weight | Grad Norm: 4.946295976970827e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 1.5364599870171247e-10\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 6.8612472474416375e-12\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 1.9638363157881145e-10\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 1.3175320209835029e-11\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 1.2185615849702458e-09\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 8.594970291042614e-10\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 1.6632127897153737e-09\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 1.5747239068275576e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 1.5400547503929829e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 8.944552321032972e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 2.0838597514227786e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 1.5617462878481092e-09\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 2.0974015857433415e-09\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 5.975794858947836e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 1.5584988855010806e-09\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 5.605678143894011e-10\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 2.9173269355808884e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 3.416268309908155e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 3.6470491071582956e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 5.670008906832891e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 7.670448276542174e-09\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 1.0141198103497118e-08\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 2.9648958843608852e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 2.759357720094613e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 9.99284766045605e-10\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 9.90383108856463e-10\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 1.6759068577343328e-09\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 1.2702349172499794e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 3.5838230161289175e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 1.6602629271389446e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 6.461095125587235e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 2.833290935200239e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 1.973840682012451e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 1.274664356287758e-07\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 3.3578351121832384e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 2.1372741798586503e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 1.782932770311163e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 8.703664633458175e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 4.243890714406007e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 1.6973449135093688e-07\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 1.986888520377761e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 5.9367877724980644e-08\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 3.165242219438369e-07\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 1.0245005910292093e-07\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 1.5432426891948126e-07\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 7.085334541301336e-09\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 2.5203735276591033e-06\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 7.349720476668153e-07\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 1.540580706205219e-05\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 1.4573083717550617e-06\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.0003621425712481141\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 7.595918759761844e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 0.00019456692098174244\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 8.14100494608283e-05\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 0.0019738459959626198\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 0.0003461664018686861\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.018885180354118347\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 0.0008901668479666114\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.009686747565865517\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 0.0011671266984194517\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.006711232475936413\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.0011770324781537056\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.01934787631034851\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.001376561471261084\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.048140812665224075\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.0041804020293056965\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.032036345452070236\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.0012691020965576172\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.022320419549942017\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.0018425255548208952\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.007153050508350134\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.0010817028814926744\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.0254046693444252\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.0003593550936784595\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.05761222541332245\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.0016247878083959222\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.09401800483465195\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.002245094859972596\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.17512643337249756\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.008435659110546112\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.011203376576304436\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.016055773943662643\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.023834414780139923\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.021090013906359673\nLayer: final_fc.weight | Grad Norm: 0.9732391238212585\nLayer: final_fc.bias | Grad Norm: 0.055315982550382614\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [66/200] completed | Average Loss: 4.6528\nTraining started for epoch 67/200\nEpoch [67/200] completed | Average Loss: 4.6518\nTraining started for epoch 68/200\nEpoch [68/200] completed | Average Loss: 4.6507\nTraining started for epoch 69/200\nEpoch [69/200] completed | Average Loss: 4.6493\nTraining started for epoch 70/200\nEpoch [70/200] completed | Average Loss: 4.6484\nTraining started for epoch 71/200\nLayer: token_embedding.weight | Grad Norm: 4.861280648860156e-13\nLayer: position_embedding.weight | Grad Norm: 3.5576953709426373e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 1.1707693148288456e-10\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 5.216826970411148e-12\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 1.3971611367846748e-10\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 9.394733255230214e-12\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 8.41388791972264e-10\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 6.261957063991019e-10\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 1.1548115796955472e-09\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 1.1584562198407866e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 1.074570210590764e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 6.475470160083319e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 1.4774744760970293e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 1.148574568787808e-09\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 1.489017908973267e-09\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 4.408971798763872e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 1.1235090635608458e-09\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 4.326637104146158e-10\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 2.032831541987079e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 2.4048544405852468e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 2.619840566708831e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 4.2486121287765855e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 5.655767854051419e-09\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 7.67626762154805e-09\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 2.1319005227837806e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 2.015232958285651e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 7.279675751092896e-10\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 7.150096625885283e-10\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 7.293132209262865e-10\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 7.147716307720486e-10\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 2.320073733130812e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 1.0780597525794633e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 4.5905284906666566e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 2.011137256729967e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 1.4928673408576287e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 1.1117273857053078e-07\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 2.761475172974315e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 1.8274850788202457e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 1.4895233846345945e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 7.645924426924466e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 3.3194382353940455e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 1.4018364424828178e-07\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 1.6801378421860136e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 5.2341054157523104e-08\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 2.0626238494969584e-07\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 7.823105363513605e-08\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 1.1114596532024734e-07\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 5.0803503626184465e-09\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 1.976683279281133e-06\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 5.875463102711365e-07\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 1.3889304682379588e-05\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 1.2914995295432163e-06\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.0003465029294602573\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 7.002650363574503e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 0.00017277423467021435\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 8.159418939612806e-05\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 0.0015959833981469274\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 0.00028414381085895\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.015907829627394676\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 0.000753035768866539\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.007939248345792294\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 0.001007886603474617\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.007567616645246744\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.0016617815708741546\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.023579241707921028\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.00165267800912261\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.04544539749622345\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.00332618341781199\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.03247589245438576\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.0012921254383400083\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.0440736822783947\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.006656281184405088\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.008174819871783257\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.0020479040686041117\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.02031116560101509\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.00031778818811289966\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.06581626087427139\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.0019203111296519637\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.11709010601043701\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.002626202069222927\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.2414875328540802\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.012166890315711498\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.01474557165056467\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.016409214586019516\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.02207009494304657\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.018124688416719437\nLayer: final_fc.weight | Grad Norm: 0.819597065448761\nLayer: final_fc.bias | Grad Norm: 0.03775937110185623\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [71/200] completed | Average Loss: 4.6474\nTraining started for epoch 72/200\nEpoch [72/200] completed | Average Loss: 4.6465\nTraining started for epoch 73/200\nEpoch [73/200] completed | Average Loss: 4.6450\nTraining started for epoch 74/200\nEpoch [74/200] completed | Average Loss: 4.6444\nTraining started for epoch 75/200\nEpoch [75/200] completed | Average Loss: 4.6436\nTraining started for epoch 76/200\nLayer: token_embedding.weight | Grad Norm: 2.5407692443024654e-13\nLayer: position_embedding.weight | Grad Norm: 1.617482840374218e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 5.638429653731514e-11\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 2.52868125744099e-12\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 6.993763207452375e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 4.735828482843596e-12\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 4.6476178461318796e-10\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 3.196067854815965e-10\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 6.211826608648607e-10\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 5.742348818671417e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 5.335467623268642e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 3.117631708349222e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 7.585191919012857e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 5.77457970329931e-10\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 7.401464996448226e-10\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 2.1102518343862897e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 6.220044479476883e-10\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 2.1110385661771147e-10\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 1.0132458067024785e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 1.2045849560882171e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 1.3557950317988343e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 2.1350101686579137e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 3.015032001840723e-09\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 4.0444114723925395e-09\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 1.1170601332821661e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 1.0512410497653946e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 3.7668773655852306e-10\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 3.746487287070721e-10\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 5.675446779207505e-10\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 4.5490172739803825e-10\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 1.325842990951287e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 6.180034262115441e-10\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 2.4845393298278395e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 1.094671908674627e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 7.527339818125256e-08\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 4.392587271695447e-08\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 1.3418403455034422e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 7.665290269187608e-08\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 7.408227986616112e-08\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 3.68117767379772e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 1.49098312363094e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 6.178385802968478e-08\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 1.017932333979843e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 3.0921260218974567e-08\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 1.1496510410324845e-07\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 3.43059980423277e-08\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 5.7627858041087165e-08\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 2.6538549136034817e-09\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 8.557805131204077e-07\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 2.486211769792135e-07\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 5.323746336216573e-06\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 4.836678044739529e-07\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.0001353104889858514\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 2.6264071948389756e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 7.013627327978611e-05\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 3.258154902141541e-05\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 0.0006731236935593188\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 0.00011447394354036078\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.006576313637197018\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 0.0003124353534076363\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.003715828526765108\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 0.0004801393370144069\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.007822472602128983\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.0013760799774900079\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.02227599546313286\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.0013739506248384714\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.015082994475960732\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.0013460424961522222\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.029212605208158493\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.0010064911330118775\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.010680991224944592\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.0010535543551668525\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.007890393026173115\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.0014252812834456563\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.02402239851653576\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.000255438411841169\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.06749999523162842\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.0015730090672150254\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.11452790349721909\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.002293984405696392\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.21092726290225983\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.0078553082421422\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.015245146118104458\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.01981543004512787\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.024365132674574852\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.025101160630583763\nLayer: final_fc.weight | Grad Norm: 0.9645283818244934\nLayer: final_fc.bias | Grad Norm: 0.05606229603290558\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [76/200] completed | Average Loss: 4.6421\nTraining started for epoch 77/200\nEpoch [77/200] completed | Average Loss: 4.6415\nTraining started for epoch 78/200\nEpoch [78/200] completed | Average Loss: 4.6411\nTraining started for epoch 79/200\nEpoch [79/200] completed | Average Loss: 4.6394\nTraining started for epoch 80/200\nCheckpoint saved at ./checkpoints/model_epoch_80.pth\nEpoch [80/200] completed | Average Loss: 4.6387\nTraining started for epoch 81/200\nLayer: token_embedding.weight | Grad Norm: 1.1494651801571831e-12\nLayer: position_embedding.weight | Grad Norm: 7.8033803377342e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 2.734268644832838e-10\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 1.2227711898571414e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 3.427551298340603e-10\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 2.327797156420619e-11\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 2.3203947652206125e-09\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 1.6586055862077842e-09\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 3.277719651251232e-09\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 3.1021694102406627e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 2.6737554392752827e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 1.600707122406675e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 3.940534565316511e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 3.0321947175337982e-09\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 3.726949859839124e-09\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 1.100536550779907e-09\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 3.115606661552306e-09\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 1.1235750108085085e-09\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 4.974213263508886e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 5.92673271571087e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 7.258277978650085e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 1.1483378692389579e-08\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 1.6129375168816296e-08\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 2.1709146480475283e-08\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 6.045927136710816e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 5.730515439950068e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 1.9325927524960207e-09\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 2.034108881332486e-09\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 2.4564204004207113e-09\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 2.1949904116524976e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 6.55728626952623e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 3.066202625134906e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 1.2449400799141586e-07\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 5.4878541710934314e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 6.868185096209345e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 5.153525535206427e-07\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 1.0119732678504079e-06\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 6.700979042761901e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 4.313261570132454e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 2.302138284449029e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 9.82371716418129e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 4.386095042718807e-07\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 4.924097538605565e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 1.5364800276529422e-07\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 1.4623914239564328e-06\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 5.302510999172227e-07\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 3.09590689084871e-07\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 1.4235248535499068e-08\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 5.555953975999728e-06\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 1.6973386891550035e-06\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 3.8450383726740256e-05\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 3.5116445360472426e-06\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.0011358988704159856\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 2.1482061129063368e-05\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 0.0005310745909810066\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 0.0002700879704207182\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 0.0062746466137468815\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 0.0010207185987383127\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.05934562161564827\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 0.0028309838380664587\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.024946197867393494\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 0.0033811440225690603\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.014859137125313282\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.0030420057009905577\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.03656315058469772\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.0029249670915305614\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.14704369008541107\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.005898250266909599\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.07506118714809418\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.0028740675188601017\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.18655617535114288\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.005522281397134066\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.015948724001646042\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.003677709260955453\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.0898551270365715\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.0009853719966486096\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.12562844157218933\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.003260531695559621\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.15878690779209137\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.003266600426286459\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.30312255024909973\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.014291387051343918\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.01971597969532013\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.03021291084587574\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.029661541804671288\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.03490350767970085\nLayer: final_fc.weight | Grad Norm: 0.8859823346138\nLayer: final_fc.bias | Grad Norm: 0.047671254724264145\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [81/200] completed | Average Loss: 4.6383\nTraining started for epoch 82/200\nEpoch [82/200] completed | Average Loss: 4.6371\nTraining started for epoch 83/200\nEpoch [83/200] completed | Average Loss: 4.6361\nTraining started for epoch 84/200\nEpoch [84/200] completed | Average Loss: 4.6354\nTraining started for epoch 85/200\nEpoch [85/200] completed | Average Loss: 4.6347\nTraining started for epoch 86/200\nLayer: token_embedding.weight | Grad Norm: 2.672289744088535e-13\nLayer: position_embedding.weight | Grad Norm: 1.7546096953841017e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 5.850751133573695e-11\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 2.6365025614094595e-12\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 7.031836224413723e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 4.836737781122036e-12\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 4.673343378946981e-10\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 3.20668241959865e-10\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 6.572136723725919e-10\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 6.091624427106979e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 5.261553970292709e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 3.1240685038902427e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 7.809807245351408e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 6.071237401705787e-10\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 7.097534227007429e-10\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 2.069493465484129e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 6.091494531013097e-10\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 2.1404879257946874e-10\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 1.0259931793044075e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 1.2480495803490754e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 1.4268954906526687e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 2.2663915189014006e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 3.027867734317624e-09\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 4.079838689108328e-09\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 1.1359448492953561e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 1.0806266992346991e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 3.426107175741322e-10\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 3.827197170291896e-10\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 4.158340338733524e-10\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 4.119646845879288e-10\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 1.2740382970832798e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 5.974779004880304e-10\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 2.4215506044811264e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 1.069096589034757e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 8.939627349491275e-08\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 6.149495135332472e-08\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 1.5018279952982994e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 9.718752380649676e-08\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 8.605918822013336e-08\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 4.712984491561656e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 1.5414346421493974e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 7.283182412720635e-08\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 1.331277701410727e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 4.135424802598209e-08\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 1.6089910559458076e-07\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 5.291550664310307e-08\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 5.323947149804553e-08\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 2.452245073669701e-09\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 9.175191735266708e-07\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 2.9439902959893516e-07\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 6.4206237766484264e-06\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 6.077456760067435e-07\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.00016699251136742532\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 3.204540689694113e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 9.046978084370494e-05\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 4.546858326648362e-05\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 0.00010863044735742733\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 6.71594389132224e-05\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.0034349204506725073\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 0.00016447764937765896\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.0035773790441453457\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 0.0005001793033443391\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.011137486435472965\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.0016590558225288987\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.027683567255735397\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.0017479887465015054\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.027949705719947815\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.001975215505808592\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.03773554787039757\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.0011347067775204778\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.03689780458807945\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.0017163503216579556\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.011167719028890133\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.0017331175040453672\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.028945419937372208\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.00028594271861948073\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.08408113569021225\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.0017967785242944956\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.11971063166856766\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.002245640382170677\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.22289389371871948\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.005659597460180521\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.012192352674901485\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.011849119327962399\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.012317834421992302\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.010419441387057304\nLayer: final_fc.weight | Grad Norm: 0.5685148239135742\nLayer: final_fc.bias | Grad Norm: 0.018293168395757675\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [86/200] completed | Average Loss: 4.6338\nTraining started for epoch 87/200\nEpoch [87/200] completed | Average Loss: 4.6331\nTraining started for epoch 88/200\nEpoch [88/200] completed | Average Loss: 4.6323\nTraining started for epoch 89/200\nEpoch [89/200] completed | Average Loss: 4.6318\nTraining started for epoch 90/200\nEpoch [90/200] completed | Average Loss: 4.6307\nTraining started for epoch 91/200\nLayer: token_embedding.weight | Grad Norm: 4.189990910653807e-13\nLayer: position_embedding.weight | Grad Norm: 2.0814081794912065e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 6.597600243907209e-11\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 2.9658066367371783e-12\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 8.079371199842811e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 5.5601864258625344e-12\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 5.941657166275149e-10\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 3.99630994962763e-10\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 7.647136812671818e-10\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 7.077015640177819e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 6.554342069087227e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 3.928642133832483e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 8.99945118426615e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 7.0632638626833e-10\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 9.523893895746482e-10\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 2.723475056587432e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 7.89627752073585e-10\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 2.7083379983139366e-10\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 1.1613919548292273e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 1.4200577345968579e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 1.6682396575262715e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 2.697540635310247e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 3.616340116252559e-09\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 4.95769070241181e-09\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 1.3532259757198517e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 1.3015516664438564e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 4.306243972518331e-10\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 4.5603690268514185e-10\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 1.2408639671335209e-09\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 8.364632875235145e-10\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 1.9780516780087964e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 9.303117720627085e-10\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 3.0917767901428306e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 1.3553201894112021e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 1.385430010714117e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 7.718573868942258e-08\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 1.777230522748141e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 1.0720731324909138e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 6.741406366472802e-08\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 3.7673469677201865e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 1.409864580637077e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 7.010189051470661e-08\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 9.507405707154248e-08\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 2.815069777284407e-08\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 2.7721580408979207e-07\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 7.646042377018603e-08\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 6.07941501584719e-08\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 2.755633943252178e-09\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 9.157759564004664e-07\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 3.210451211543841e-07\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 6.6420216171536595e-06\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 6.944186452528811e-07\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.0002116271061822772\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 4.405915660754545e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 0.00011239117884542793\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 5.361721196095459e-05\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 0.001191907562315464\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 0.00018731396994553506\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.011467852629721165\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 0.0005513539654202759\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.004720455966889858\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 0.0006887384806759655\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.011030193418264389\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.0014071041950955987\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.02726990170776844\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.0015925692860037088\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.01577315479516983\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.0014623776078224182\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.02691785804927349\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.001103343558497727\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.015155022032558918\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.0025482429191470146\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.00993382278829813\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.001510773436166346\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.020848024636507034\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.0002260806068079546\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.05990228056907654\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.0015287146670743823\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.07672368735074997\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.0016616488574072719\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.1790146678686142\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.005123662296682596\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.010874184779822826\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.012409714050590992\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.017229238525032997\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.014288131147623062\nLayer: final_fc.weight | Grad Norm: 0.9539591073989868\nLayer: final_fc.bias | Grad Norm: 0.049230508506298065\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [91/200] completed | Average Loss: 4.6301\nTraining started for epoch 92/200\nEpoch [92/200] completed | Average Loss: 4.6293\nTraining started for epoch 93/200\nEpoch [93/200] completed | Average Loss: 4.6288\nTraining started for epoch 94/200\nEpoch [94/200] completed | Average Loss: 4.6285\nTraining started for epoch 95/200\nEpoch [95/200] completed | Average Loss: 4.6275\nTraining started for epoch 96/200\nLayer: token_embedding.weight | Grad Norm: 5.142504803067049e-13\nLayer: position_embedding.weight | Grad Norm: 2.963324030602621e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 9.562188540979122e-11\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 4.284749638427954e-12\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 1.1387300130616396e-10\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 7.82348231337382e-12\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 8.143164476059894e-10\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 5.783757917043886e-10\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 1.0444810571996754e-09\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 9.99115790101257e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 8.677239482501875e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 5.292186133765142e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 1.2419441031141787e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 9.983863735740783e-10\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 1.2128117399257121e-09\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 3.5984867863270154e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 1.1000308441921902e-09\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 3.851403085342042e-10\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 1.6391692170270034e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 1.9942341181189427e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 2.2432120605486716e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 3.7555731857708e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 5.07862329968134e-09\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 7.065018348129115e-09\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 1.942837002388842e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 1.8853013372677196e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 6.198933588663635e-10\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 6.56849796776271e-10\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 1.241765579251819e-09\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 9.226638342241245e-10\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 2.4712583979180636e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 1.166145291442433e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 4.311229062636812e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 1.8932271306226767e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 2.2329821547373285e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 1.5905887096323568e-07\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 3.084834361288813e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 2.058974075680453e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 1.1937179067444958e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 7.390601552970111e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 2.540291177410836e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 1.3945007992788305e-07\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 1.548785633076477e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 4.8472070801608425e-08\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 5.119626962368784e-07\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 1.673668350576918e-07\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 8.829796627196629e-08\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 4.007294496233271e-09\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 1.587392034707591e-06\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 5.698791483155219e-07\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 1.2376380254863761e-05\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 1.265887249246589e-06\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.00042989931534975767\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 8.511730811733287e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 0.0003223842941224575\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 0.00010663903231034055\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 0.0024054923560470343\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 0.00037853908725082874\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.023573320358991623\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 0.001137757790274918\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.009816776029765606\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 0.0014741779305040836\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.013927088119089603\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.0017141897697001696\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.031747784465551376\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.001850782660767436\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.03725455328822136\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.0038248877972364426\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.03692765533924103\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.0012927551288157701\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.029167206957936287\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.00507742166519165\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.013006463646888733\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.0019307860638946295\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.025926124304533005\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.00027557098655961454\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.09290385246276855\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.0021429972257465124\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.13362769782543182\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.002705284859985113\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.2822049558162689\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.00660216948017478\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.01624816097319126\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.015467263758182526\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.017916908487677574\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.013509253039956093\nLayer: final_fc.weight | Grad Norm: 0.9096448421478271\nLayer: final_fc.bias | Grad Norm: 0.021410830318927765\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [96/200] completed | Average Loss: 4.6271\nTraining started for epoch 97/200\nEpoch [97/200] completed | Average Loss: 4.6263\nTraining started for epoch 98/200\nEpoch [98/200] completed | Average Loss: 4.6259\nTraining started for epoch 99/200\nEpoch [99/200] completed | Average Loss: 4.6253\nTraining started for epoch 100/200\nCheckpoint saved at ./checkpoints/model_epoch_100.pth\nEpoch [100/200] completed | Average Loss: 4.6247\nTraining started for epoch 101/200\nLayer: token_embedding.weight | Grad Norm: 1.4443304950477465e-13\nLayer: position_embedding.weight | Grad Norm: 8.176545378268951e-13\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 2.7421259707338663e-11\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 1.2398416711703608e-12\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 3.238226006785361e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 2.2355446487043462e-12\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 2.448636016172401e-10\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 1.6377343925455534e-10\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 3.2065430866090594e-10\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 3.028880202204931e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 2.818377475843903e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 1.672841587474494e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 3.8398853541288247e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 3.0041802379088267e-10\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 3.897327460755662e-10\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 1.1214069256526571e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 3.4928057668359713e-10\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 1.1340059447029205e-10\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 5.054762511336186e-11\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 6.231337060930153e-12\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 6.771397886851105e-10\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 1.1125459442595798e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 1.4585902485819702e-09\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 2.015741795702297e-09\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 5.578765893687887e-09\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 5.414448445151265e-09\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 1.7341908464807432e-10\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 1.8873320961620976e-10\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 2.871193005571371e-10\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 2.3453783359883573e-10\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 6.6632597217619605e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 3.153776129138919e-10\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 1.2864425968928117e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 5.668342462072928e-10\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 4.886449644914137e-08\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 3.1624875163061006e-08\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 8.001369167232042e-08\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 5.198993946464725e-08\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 4.345492499169268e-08\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 2.6076866888047334e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 7.301883186983105e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 4.038608025780377e-08\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 7.698874782136045e-08\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 2.3315044117566686e-08\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 9.276973855776305e-08\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 2.7680309599986685e-08\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 2.8796865336744304e-08\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 1.3128058640177187e-09\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 4.426268844781589e-07\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 1.6459578944250097e-07\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 3.1949373351380927e-06\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 3.427798844768404e-07\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.00010741021105786785\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 2.2159376840136247e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 6.338729144772515e-05\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 2.8688171369140036e-05\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 0.0005045773577876389\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 8.516093657817692e-05\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.0052259331569075584\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 0.00025317430845461786\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.0026584756560623646\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 0.0004085509281139821\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.00988788716495037\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.0011334378505125642\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.02818245440721512\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.0012627984397113323\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.015390703454613686\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.0018782188417389989\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.031347401440143585\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.000898487342055887\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.008940568193793297\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.0009477686835452914\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.00809429120272398\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.001252531073987484\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.025085672736167908\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.00022187169815879315\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.07562783360481262\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.0015062387101352215\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.10311480611562729\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.001851108856499195\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.22765973210334778\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.005989531986415386\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.01505110040307045\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.01753873936831951\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.018993977457284927\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.0172591470181942\nLayer: final_fc.weight | Grad Norm: 0.787147581577301\nLayer: final_fc.bias | Grad Norm: 0.02354268543422222\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [101/200] completed | Average Loss: 4.6238\nTraining started for epoch 102/200\nEpoch [102/200] completed | Average Loss: 4.6231\nTraining started for epoch 103/200\nEpoch [103/200] completed | Average Loss: 4.6228\nTraining started for epoch 104/200\nEpoch [104/200] completed | Average Loss: 4.6223\nTraining started for epoch 105/200\nEpoch [105/200] completed | Average Loss: 4.6222\nTraining started for epoch 106/200\nLayer: token_embedding.weight | Grad Norm: 6.994484743998164e-13\nLayer: position_embedding.weight | Grad Norm: 4.851524997712131e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 1.645460712129676e-10\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 7.461749967907494e-12\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 1.909389729659594e-10\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 1.335768735205578e-11\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 1.4923339231032173e-09\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 1.1088215901011722e-09\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 2.02503303015078e-09\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 2.0429418157164037e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 1.8088419651007825e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 1.087720358228239e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 2.4996715808356385e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 1.9895840530637088e-09\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 2.421912448369312e-09\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 7.052109451954891e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 1.9114838600842177e-09\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 7.119929090748656e-10\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 2.871862747610976e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 3.604660667178017e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 4.608288861618348e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 7.728766071579685e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 1.0069244105181951e-08\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 1.4091006583782928e-08\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 3.999051756409244e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 3.915122448461261e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 1.3982072166740522e-09\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 1.3582475144602313e-09\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 3.6298077876750767e-09\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 2.4355266692310806e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 5.6652204705187614e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 2.690169198515946e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 8.790716776729823e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 3.8494962772972485e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 4.1924761262635e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 3.9486334912908205e-07\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 5.082752068119589e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 4.44084150785784e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 2.731933648192353e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 1.8873147666909063e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 4.4281350142227893e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 2.8908860372212075e-07\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 5.044906288276252e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 1.5817950327345898e-07\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 1.2524525345725124e-06\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 4.645578997042321e-07\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 1.4272805515247455e-07\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 6.4234431107479395e-09\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 2.6387701836938504e-06\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 1.1459665074653458e-06\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 2.2416414140025154e-05\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 2.6982579584000632e-06\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.0008964395965449512\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 2.017716360569466e-05\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 0.000480061920825392\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 0.00022127135889604688\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 0.007025130558758974\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 0.0010221900884062052\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.06798546016216278\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 0.003306793048977852\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.02618434838950634\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 0.004141674842685461\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.011305376887321472\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.0016946776304394007\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.03400257229804993\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.0016292977379634976\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.04413696750998497\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.005419796798378229\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.04125141724944115\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.0012732325121760368\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.026761485263705254\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.0023485152050852776\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.011015980504453182\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.0019112558802589774\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.043932024389505386\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.0005232220282778144\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.09215661138296127\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.0018135694554075599\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.13999944925308228\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.00259613711386919\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.3184471130371094\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.010123361833393574\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.01701107993721962\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.017262380570173264\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.022425208240747452\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.01563500612974167\nLayer: final_fc.weight | Grad Norm: 0.9248234033584595\nLayer: final_fc.bias | Grad Norm: 0.024838393554091454\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [106/200] completed | Average Loss: 4.6214\nTraining started for epoch 107/200\nEpoch [107/200] completed | Average Loss: 4.6210\nTraining started for epoch 108/200\nEpoch [108/200] completed | Average Loss: 4.6209\nTraining started for epoch 109/200\nEpoch [109/200] completed | Average Loss: 4.6201\nTraining started for epoch 110/200\nEpoch [110/200] completed | Average Loss: 4.6197\nTraining started for epoch 111/200\nLayer: token_embedding.weight | Grad Norm: 7.965385621054588e-13\nLayer: position_embedding.weight | Grad Norm: 4.96782779843552e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 1.6452643414321955e-10\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 7.47321215327501e-12\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 1.9509728554911732e-10\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 1.3667049263144104e-11\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 1.3696740408519759e-09\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 1.0089796775858417e-09\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 1.8085051234351113e-09\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 1.8234385112947393e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 1.5599316283143594e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 9.458259730976692e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 2.261929976654642e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 1.8058272654997154e-09\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 2.19410911661555e-09\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 6.424474841004724e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 1.875099631121202e-09\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 6.880581659984841e-10\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 2.829486367428302e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 3.539991563772382e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 4.005829445929976e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 6.866259116833362e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 8.819214691868638e-09\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 1.2491900847066972e-08\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 3.527702929773113e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 3.4788083524972535e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 1.1435654645453042e-09\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 1.1998854132500014e-09\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 4.646141249509128e-09\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 2.978589597191217e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 6.382243356028994e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 3.0384286198170685e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 8.443002741387318e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 3.641877022175777e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 4.3633320956359967e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 4.0053217276181385e-07\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 4.922013658870128e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 4.198060423732386e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 1.5819099985492358e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 1.2438087537702813e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 3.065369753585401e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 2.3950113359205716e-07\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 2.764352871054143e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 8.626138736644862e-08\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 1.3448867548504495e-06\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 4.880461688117066e-07\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 1.312948256781965e-07\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 5.7305702405585635e-09\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 1.911031858981005e-06\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 9.40374945912481e-07\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 1.4419932995224372e-05\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 1.95430516214401e-06\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.00040415095281787217\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 9.988783858716488e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 0.00031026298529468477\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 0.00011358819756424055\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 0.0035750793758779764\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 0.0005185367772355676\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.03570905700325966\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 0.0017439894145354629\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.015108240768313408\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 0.0024843851570039988\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.011066529899835587\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.0014813259476795793\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.02957785502076149\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.0014458063524216413\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.062308866530656815\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.008817634545266628\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.038625605404376984\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.001189716043882072\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.033457498997449875\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.006475488655269146\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.010395868681371212\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.0020408122800290585\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.05324775353074074\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.0004254658706486225\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.08009252697229385\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.0015888874186202884\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.10391110926866531\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.0019491390557959676\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.28287988901138306\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.008726322092115879\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.017711007967591286\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.014041251502931118\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.01889144629240036\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.008995577692985535\nLayer: final_fc.weight | Grad Norm: 0.9430287480354309\nLayer: final_fc.bias | Grad Norm: 0.01846494898200035\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [111/200] completed | Average Loss: 4.6192\nTraining started for epoch 112/200\nEpoch [112/200] completed | Average Loss: 4.6187\nTraining started for epoch 113/200\nEpoch [113/200] completed | Average Loss: 4.6182\nTraining started for epoch 114/200\nEpoch [114/200] completed | Average Loss: 4.6176\nTraining started for epoch 115/200\nEpoch [115/200] completed | Average Loss: 4.6177\nTraining started for epoch 116/200\nLayer: token_embedding.weight | Grad Norm: 3.580141283258953e-13\nLayer: position_embedding.weight | Grad Norm: 2.410874451463929e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 7.902806881121549e-11\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 3.644951094422533e-12\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 9.974544801227836e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 7.116718239025266e-12\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 8.011113439287953e-10\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 5.991785401171512e-10\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 1.0443959030936867e-09\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 1.079749067933733e-09\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 9.295379466145448e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 5.835693595024338e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 1.2975326368902529e-09\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 1.079097367018278e-09\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 1.3446482816092953e-09\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 4.03845623608845e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 1.1441022573777104e-09\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 4.16706363859376e-10\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 1.4382035840032614e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 1.846168704022766e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 2.25275376131151e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 3.991572850026159e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 5.158061977539319e-09\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 7.444778571397137e-09\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 2.012158617503701e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 2.0044584658762687e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 6.733524848812067e-10\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 6.860761958549233e-10\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 2.7976942984508923e-09\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 1.820270156827064e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 3.837701356701473e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 1.8321959505129826e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 5.327575536284712e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 2.305633906019011e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 1.966262317409928e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 1.5722916657523456e-07\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 2.216076779859577e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 1.6310866612911923e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 1.141646279734232e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 8.27674142556134e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 1.4214545274171542e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 1.006603085329516e-07\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 2.5291609517807956e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 8.191249634137421e-08\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 5.318682383403939e-07\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 1.914223730636877e-07\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 1.0712316367289532e-07\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 4.685949406280088e-09\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 7.665269095014082e-07\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 3.859772732539568e-07\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 5.195091489440529e-06\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 8.43194470689923e-07\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.0002146234328392893\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 6.549984391313046e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 0.00015379246906377375\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 5.751606295234524e-05\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 0.0016095435712486506\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 0.00023683751351200044\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.015730462968349457\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 0.0007708346238359809\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.006618802435696125\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 0.0010865511139854789\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.007316852454096079\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.0011581922881305218\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.02360028773546219\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.0011855907505378127\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.030512647703289986\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.0029037082567811012\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.02995850145816803\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.0010197584051638842\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.02332252822816372\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.003989174962043762\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.006530295591801405\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.0012136262375861406\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.026953348889946938\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.00029020479996688664\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.07189404219388962\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.0014312916900962591\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.10861001908779144\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.0018330629682168365\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.2551521956920624\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.011789006181061268\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.019736433401703835\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.02640809863805771\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.02801910787820816\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.030437201261520386\nLayer: final_fc.weight | Grad Norm: 0.9527259469032288\nLayer: final_fc.bias | Grad Norm: 0.05678027868270874\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [116/200] completed | Average Loss: 4.6169\nTraining started for epoch 117/200\nEpoch [117/200] completed | Average Loss: 4.6169\nTraining started for epoch 118/200\nEpoch [118/200] completed | Average Loss: 4.6163\nTraining started for epoch 119/200\nEpoch [119/200] completed | Average Loss: 4.6161\nTraining started for epoch 120/200\nCheckpoint saved at ./checkpoints/model_epoch_120.pth\nEpoch [120/200] completed | Average Loss: 4.6158\nTraining started for epoch 121/200\nLayer: token_embedding.weight | Grad Norm: 3.7184412992295746e-13\nLayer: position_embedding.weight | Grad Norm: 2.0218193438892307e-12\nLayer: transformer_blocks.0.input_self_attention.in_proj_weight | Grad Norm: 6.344986341888514e-11\nLayer: transformer_blocks.0.input_self_attention.in_proj_bias | Grad Norm: 2.9148038149795497e-12\nLayer: transformer_blocks.0.input_self_attention.out_proj.weight | Grad Norm: 7.652958128323561e-11\nLayer: transformer_blocks.0.input_self_attention.out_proj.bias | Grad Norm: 5.4942847146910445e-12\nLayer: transformer_blocks.0.context_attn.in_proj_weight | Grad Norm: 6.112346739861607e-10\nLayer: transformer_blocks.0.context_attn.in_proj_bias | Grad Norm: 4.112440665782202e-10\nLayer: transformer_blocks.0.context_attn.out_proj.weight | Grad Norm: 7.974464977245077e-10\nLayer: transformer_blocks.0.context_attn.out_proj.bias | Grad Norm: 7.447991112741192e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_weight | Grad Norm: 6.481022385429469e-10\nLayer: transformer_blocks.0.persona_attn.in_proj_bias | Grad Norm: 3.8237493726889227e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.weight | Grad Norm: 9.327765226885276e-10\nLayer: transformer_blocks.0.persona_attn.out_proj.bias | Grad Norm: 7.399422186082916e-10\nLayer: transformer_blocks.0.persona_proj.weight | Grad Norm: 9.363048114607864e-10\nLayer: transformer_blocks.0.persona_proj.bias | Grad Norm: 2.603762760955419e-10\nLayer: transformer_blocks.0.context_proj.weight | Grad Norm: 8.231357262467043e-10\nLayer: transformer_blocks.0.context_proj.bias | Grad Norm: 2.732171711095077e-10\nLayer: transformer_blocks.0.paa_layer.fc.weight | Grad Norm: 1.0940123806868485e-10\nLayer: transformer_blocks.0.paa_layer.fc.bias | Grad Norm: 1.4179706020467364e-11\nLayer: transformer_blocks.0.paa_layer.fc_out.weight | Grad Norm: 1.6985811646108573e-09\nLayer: transformer_blocks.0.paa_layer.fc_out.bias | Grad Norm: 2.821668676489253e-09\nLayer: transformer_blocks.0.mlp.0.weight | Grad Norm: 3.848250607063619e-09\nLayer: transformer_blocks.0.mlp.0.bias | Grad Norm: 5.419858339905659e-09\nLayer: transformer_blocks.0.mlp.3.weight | Grad Norm: 1.537993377098701e-08\nLayer: transformer_blocks.0.mlp.3.bias | Grad Norm: 1.5190916968776946e-08\nLayer: transformer_blocks.0.mlp.4.weight | Grad Norm: 4.864070812793386e-10\nLayer: transformer_blocks.0.mlp.4.bias | Grad Norm: 5.237783540223973e-10\nLayer: transformer_blocks.0.layer_norm2.weight | Grad Norm: 2.0875290385191647e-09\nLayer: transformer_blocks.0.layer_norm2.bias | Grad Norm: 1.3372949414502955e-09\nLayer: transformer_blocks.1.input_self_attention.in_proj_weight | Grad Norm: 2.8635120941089554e-08\nLayer: transformer_blocks.1.input_self_attention.in_proj_bias | Grad Norm: 1.370390578792069e-09\nLayer: transformer_blocks.1.input_self_attention.out_proj.weight | Grad Norm: 3.964666461797606e-08\nLayer: transformer_blocks.1.input_self_attention.out_proj.bias | Grad Norm: 1.7308544597582909e-09\nLayer: transformer_blocks.1.context_attn.in_proj_weight | Grad Norm: 1.3695921552425716e-07\nLayer: transformer_blocks.1.context_attn.in_proj_bias | Grad Norm: 9.499581921090794e-08\nLayer: transformer_blocks.1.context_attn.out_proj.weight | Grad Norm: 1.7423528220206208e-07\nLayer: transformer_blocks.1.context_attn.out_proj.bias | Grad Norm: 1.1407154687503862e-07\nLayer: transformer_blocks.1.persona_attn.in_proj_weight | Grad Norm: 6.815145070504514e-08\nLayer: transformer_blocks.1.persona_attn.in_proj_bias | Grad Norm: 4.583578316896819e-08\nLayer: transformer_blocks.1.persona_attn.out_proj.weight | Grad Norm: 1.1588613801905012e-07\nLayer: transformer_blocks.1.persona_attn.out_proj.bias | Grad Norm: 7.34143554836919e-08\nLayer: transformer_blocks.1.persona_proj.weight | Grad Norm: 1.250070624791988e-07\nLayer: transformer_blocks.1.persona_proj.bias | Grad Norm: 3.8993530182551694e-08\nLayer: transformer_blocks.1.context_proj.weight | Grad Norm: 3.5246111451670004e-07\nLayer: transformer_blocks.1.context_proj.bias | Grad Norm: 1.093225492354577e-07\nLayer: transformer_blocks.1.paa_layer.fc.weight | Grad Norm: 6.09340986557072e-08\nLayer: transformer_blocks.1.paa_layer.fc.bias | Grad Norm: 2.7044131378772818e-09\nLayer: transformer_blocks.1.paa_layer.fc_out.weight | Grad Norm: 7.467958766937954e-07\nLayer: transformer_blocks.1.paa_layer.fc_out.bias | Grad Norm: 3.050533052828541e-07\nLayer: transformer_blocks.1.mlp.0.weight | Grad Norm: 5.7584284149925224e-06\nLayer: transformer_blocks.1.mlp.0.bias | Grad Norm: 6.628720825574419e-07\nLayer: transformer_blocks.1.mlp.3.weight | Grad Norm: 0.00020511604088824242\nLayer: transformer_blocks.1.mlp.3.bias | Grad Norm: 4.149397227592999e-06\nLayer: transformer_blocks.1.mlp.4.weight | Grad Norm: 0.00011889161396538839\nLayer: transformer_blocks.1.mlp.4.bias | Grad Norm: 5.948083708062768e-05\nLayer: transformer_blocks.1.layer_norm2.weight | Grad Norm: 0.001537929754704237\nLayer: transformer_blocks.1.layer_norm2.bias | Grad Norm: 0.0002238348824903369\nLayer: transformer_blocks.2.input_self_attention.in_proj_weight | Grad Norm: 0.015562486834824085\nLayer: transformer_blocks.2.input_self_attention.in_proj_bias | Grad Norm: 0.0007660620613023639\nLayer: transformer_blocks.2.input_self_attention.out_proj.weight | Grad Norm: 0.006485005374997854\nLayer: transformer_blocks.2.input_self_attention.out_proj.bias | Grad Norm: 0.0011177482083439827\nLayer: transformer_blocks.2.context_attn.in_proj_weight | Grad Norm: 0.007344136014580727\nLayer: transformer_blocks.2.context_attn.in_proj_bias | Grad Norm: 0.0010423986241221428\nLayer: transformer_blocks.2.context_attn.out_proj.weight | Grad Norm: 0.024832231923937798\nLayer: transformer_blocks.2.context_attn.out_proj.bias | Grad Norm: 0.0011854000622406602\nLayer: transformer_blocks.2.persona_attn.in_proj_weight | Grad Norm: 0.04985083267092705\nLayer: transformer_blocks.2.persona_attn.in_proj_bias | Grad Norm: 0.004620586987584829\nLayer: transformer_blocks.2.persona_attn.out_proj.weight | Grad Norm: 0.03791268169879913\nLayer: transformer_blocks.2.persona_attn.out_proj.bias | Grad Norm: 0.0010306277545168996\nLayer: transformer_blocks.2.persona_proj.weight | Grad Norm: 0.03917168453335762\nLayer: transformer_blocks.2.persona_proj.bias | Grad Norm: 0.007796297315508127\nLayer: transformer_blocks.2.context_proj.weight | Grad Norm: 0.007482935208827257\nLayer: transformer_blocks.2.context_proj.bias | Grad Norm: 0.0011733685387298465\nLayer: transformer_blocks.2.paa_layer.fc.weight | Grad Norm: 0.04852460324764252\nLayer: transformer_blocks.2.paa_layer.fc.bias | Grad Norm: 0.00042526909965090454\nLayer: transformer_blocks.2.paa_layer.fc_out.weight | Grad Norm: 0.07894689589738846\nLayer: transformer_blocks.2.paa_layer.fc_out.bias | Grad Norm: 0.001289190026000142\nLayer: transformer_blocks.2.mlp.0.weight | Grad Norm: 0.10092905163764954\nLayer: transformer_blocks.2.mlp.0.bias | Grad Norm: 0.0015699524665251374\nLayer: transformer_blocks.2.mlp.3.weight | Grad Norm: 0.23825304210186005\nLayer: transformer_blocks.2.mlp.3.bias | Grad Norm: 0.0037376233376562595\nLayer: transformer_blocks.2.mlp.4.weight | Grad Norm: 0.015312734059989452\nLayer: transformer_blocks.2.mlp.4.bias | Grad Norm: 0.015236622653901577\nLayer: transformer_blocks.2.layer_norm2.weight | Grad Norm: 0.018329381942749023\nLayer: transformer_blocks.2.layer_norm2.bias | Grad Norm: 0.015117606148123741\nLayer: final_fc.weight | Grad Norm: 0.9570327401161194\nLayer: final_fc.bias | Grad Norm: 0.030659722164273262\nLayer: layer_norm.weight | Grad: None\nLayer: layer_norm.bias | Grad: None\nEpoch [121/200] completed | Average Loss: 4.6151\nTraining started for epoch 122/200\nEpoch [122/200] completed | Average Loss: 4.6146\nTraining started for epoch 123/200\nBatch 1031/1250 | Loss: 4.7222\r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-66-6430a21f3f39>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLOSS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":66},{"cell_type":"code","source":"torch.cuda.memory_summary(device='cuda', abbreviated=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T07:49:46.167024Z","iopub.execute_input":"2025-01-10T07:49:46.167397Z","iopub.status.idle":"2025-01-10T07:49:46.173875Z","shell.execute_reply.started":"2025-01-10T07:49:46.167365Z","shell.execute_reply":"2025-01-10T07:49:46.172871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"export CUDA_LAUNCH_BLOCKING=1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T14:16:16.118335Z","iopub.execute_input":"2025-01-08T14:16:16.118626Z","iopub.status.idle":"2025-01-08T14:16:16.123946Z","shell.execute_reply.started":"2025-01-08T14:16:16.118603Z","shell.execute_reply":"2025-01-08T14:16:16.122821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), 'paamodel.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T02:20:51.210981Z","iopub.execute_input":"2025-01-11T02:20:51.211274Z","iopub.status.idle":"2025-01-11T02:20:52.217231Z","shell.execute_reply.started":"2025-01-11T02:20:51.211251Z","shell.execute_reply":"2025-01-11T02:20:52.216526Z"},"id":"wkDGQ4eRa7JH"},"outputs":[],"execution_count":67},{"cell_type":"code","source":"model = PAAModel()\n\n# Load the saved state dict\nmodel.load_state_dict(torch.load('paamodel.pth'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T02:21:12.176284Z","iopub.execute_input":"2025-01-11T02:21:12.176590Z","iopub.status.idle":"2025-01-11T02:21:14.253368Z","shell.execute_reply.started":"2025-01-11T02:21:12.176566Z","shell.execute_reply":"2025-01-11T02:21:14.252447Z"},"id":"7TcPgRsja7JH","outputId":"9367d669-eabf-48fb-e834-0d05129193bf"},"outputs":[{"name":"stderr","text":"<ipython-input-68-576f8403c810>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('paamodel.pth'))\n","output_type":"stream"},{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":68},{"cell_type":"code","source":"df_eval = df[50:51].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T02:22:44.350902Z","iopub.execute_input":"2025-01-11T02:22:44.351331Z","iopub.status.idle":"2025-01-11T02:22:44.356513Z","shell.execute_reply.started":"2025-01-11T02:22:44.351297Z","shell.execute_reply":"2025-01-11T02:22:44.355477Z"},"id":"K1MXJesua7JI"},"outputs":[],"execution_count":78},{"cell_type":"code","source":"df_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T02:22:47.551218Z","iopub.execute_input":"2025-01-11T02:22:47.551522Z","iopub.status.idle":"2025-01-11T02:22:47.567780Z","shell.execute_reply.started":"2025-01-11T02:22:47.551498Z","shell.execute_reply":"2025-01-11T02:22:47.566878Z"},"id":"9D4sb3pxa7JI","outputId":"114e6bf8-46fd-487a-d0d7-713c7e4ec370"},"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"                                             Problem  \\\n0  Develop a Python program that takes the name o...   \n\n                                        Student_code  \\\n0  file_name=input()            def read_file(fil...   \n\n                                       Expected_code        Q01        Q02  \\\n0  def substitute_vowels(chunk, vowel_substitutes...  3 : Often  3 : Often   \n\n             Q03            Q04        Q05        Q06               Q07  ...  \\\n0  2 : Sometimes  2 : Sometimes  3 : Often  3 : Often  1 : Almost Never  ...   \n\n             Q12            Q13        Q14        Q15            Q16  \\\n0  2 : Sometimes  2 : Sometimes  3 : Often  3 : Often  2 : Sometimes   \n\n                                metacognitive_vector  \\\n0  ['3 ', '3 ', '2 ', '2 ', '3 ', '3 ', '1 ', '2 ...   \n\n                              metacognitive_feedback  \\\n0  Your approach to reading the file and extracti...   \n\n                            combined_problem_student  \\\n0  This is the combination of the problem desccri...   \n\n                           combined_problem_expected  \\\n0  This is the combination of the problem descrip...   \n\n                        combined_metacogntion_prompt  \n0  Metacognitive feedback helps students reflect ...  \n\n[1 rows x 24 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Problem</th>\n      <th>Student_code</th>\n      <th>Expected_code</th>\n      <th>Q01</th>\n      <th>Q02</th>\n      <th>Q03</th>\n      <th>Q04</th>\n      <th>Q05</th>\n      <th>Q06</th>\n      <th>Q07</th>\n      <th>...</th>\n      <th>Q12</th>\n      <th>Q13</th>\n      <th>Q14</th>\n      <th>Q15</th>\n      <th>Q16</th>\n      <th>metacognitive_vector</th>\n      <th>metacognitive_feedback</th>\n      <th>combined_problem_student</th>\n      <th>combined_problem_expected</th>\n      <th>combined_metacogntion_prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Develop a Python program that takes the name o...</td>\n      <td>file_name=input()            def read_file(fil...</td>\n      <td>def substitute_vowels(chunk, vowel_substitutes...</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>1 : Almost Never</td>\n      <td>...</td>\n      <td>2 : Sometimes</td>\n      <td>2 : Sometimes</td>\n      <td>3 : Often</td>\n      <td>3 : Often</td>\n      <td>2 : Sometimes</td>\n      <td>['3 ', '3 ', '2 ', '2 ', '3 ', '3 ', '1 ', '2 ...</td>\n      <td>Your approach to reading the file and extracti...</td>\n      <td>This is the combination of the problem desccri...</td>\n      <td>This is the combination of the problem descrip...</td>\n      <td>Metacognitive feedback helps students reflect ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 24 columns</p>\n</div>"},"metadata":{}}],"execution_count":79},{"cell_type":"code","source":"eval_dataset = CustomDataset(df_eval, t5_tokenizer,gpt2_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T02:22:49.221100Z","iopub.execute_input":"2025-01-11T02:22:49.221443Z","iopub.status.idle":"2025-01-11T02:22:49.225274Z","shell.execute_reply.started":"2025-01-11T02:22:49.221412Z","shell.execute_reply":"2025-01-11T02:22:49.224419Z"},"id":"p0goTw6Da7JI"},"outputs":[],"execution_count":80},{"cell_type":"code","source":"\n\ndef inference(model,gpt2_tokenizer, t5_tokenizer, eval_dataset, device):\n    model.eval()\n    model.to(device)\n\n    metacognition_prompt_ids, problem_student_code_ids,problem_expected_code_ids,student_code_ids,target_ids = eval_dataset[0]\n\n    metacognition_prompt_ids = metacognition_prompt_ids.unsqueeze(0).to(device)\n    problem_student_code_ids = problem_student_code_ids.unsqueeze(0).to(device)\n    problem_expected_code_ids = problem_expected_code_ids.unsqueeze(0).to(device)\n    student_code_ids = student_code_ids.unsqueeze(0).to(device)\n    target_ids = target_ids.unsqueeze(0).to(device)\n\n    student_attention_mask = (problem_student_code_ids != t5_pad_token_id).long().to(device)\n    expected_attention_mask = (problem_expected_code_ids != t5_pad_token_id).long().to(device)\n    metacognition_attention_mask = (metacognition_prompt_ids != t5_pad_token_id).long().to(device)\n\n\n    with torch.no_grad():\n\n        logits = model(\n                      metacognition_prompt_ids,\n                       problem_student_code_ids ,\n                       problem_expected_code_ids,\n                      metacognition_attention_mask,\n                      expected_attention_mask\n        )\n        \n\n        predictions = logits.argmax(dim=-1).squeeze().tolist()\n        filtered_tokens = [token for token in predictions if token != 0]\n        #decoded_text = t5_tokenizer.decode(filtered_tokens, skip_special_tokens=False)\n        decoded_text = t5_tokenizer.decode(predictions, skip_special_tokens=True)\n\n\n        return filtered_tokens, decoded_text,logits\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T02:36:56.975799Z","iopub.execute_input":"2025-01-11T02:36:56.976155Z","iopub.status.idle":"2025-01-11T02:36:56.982767Z","shell.execute_reply.started":"2025-01-11T02:36:56.976127Z","shell.execute_reply":"2025-01-11T02:36:56.981812Z"},"id":"GGKQrOYQa7JJ"},"outputs":[],"execution_count":91},{"cell_type":"code","source":"predictions, decoded_text,logits = inference(model, gpt2_tokenizer, t5_tokenizer, eval_dataset, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T02:37:01.410089Z","iopub.execute_input":"2025-01-11T02:37:01.410384Z","iopub.status.idle":"2025-01-11T02:37:01.490378Z","shell.execute_reply.started":"2025-01-11T02:37:01.410361Z","shell.execute_reply":"2025-01-11T02:37:01.489692Z"},"id":"Nk1Ua-_va7JJ"},"outputs":[],"execution_count":92},{"cell_type":"code","source":"print(logits)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T02:37:39.955212Z","iopub.execute_input":"2025-01-11T02:37:39.955554Z","iopub.status.idle":"2025-01-11T02:37:39.962993Z","shell.execute_reply.started":"2025-01-11T02:37:39.955525Z","shell.execute_reply":"2025-01-11T02:37:39.962173Z"}},"outputs":[{"name":"stdout","text":"tensor([[[  6.8125,   1.9549,   4.0246,  ..., -15.1763, -14.7708, -13.9994],\n         [  6.8125,   1.9549,   4.0246,  ..., -15.1763, -14.7708, -13.9994],\n         [  6.8125,   1.9549,   4.0246,  ..., -15.1763, -14.7708, -13.9994],\n         ...,\n         [  6.8125,   1.9549,   4.0246,  ..., -15.1763, -14.7708, -13.9994],\n         [  6.8125,   1.9549,   4.0246,  ..., -15.1763, -14.7708, -13.9994],\n         [  6.8125,   1.9549,   4.0246,  ..., -15.1763, -14.7708, -13.9994]]],\n       device='cuda:0')\n","output_type":"stream"}],"execution_count":95},{"cell_type":"code","source":"decoded_text = t5_tokenizer.decode(logits.tolist(), skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T02:40:42.762041Z","iopub.execute_input":"2025-01-11T02:40:42.762303Z","iopub.status.idle":"2025-01-11T02:40:42.834141Z","shell.execute_reply.started":"2025-01-11T02:40:42.762274Z","shell.execute_reply":"2025-01-11T02:40:42.833006Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-8a602caeb5df>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecoded_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt5_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 't5_tokenizer' is not defined"],"ename":"NameError","evalue":"name 't5_tokenizer' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"print(\"Predicted Tokens:\", predictions.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T02:39:48.025417Z","iopub.execute_input":"2025-01-11T02:39:48.025718Z","iopub.status.idle":"2025-01-11T02:39:48.046852Z","shell.execute_reply.started":"2025-01-11T02:39:48.025694Z","shell.execute_reply":"2025-01-11T02:39:48.045640Z"},"id":"0i90o8Nfa7JK","outputId":"f02ea63b-45b7-4fbf-84b4-46ed4860ea3d"},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-100-90eb31917a1c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted Tokens:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"],"ename":"AttributeError","evalue":"'list' object has no attribute 'shape'","output_type":"error"}],"execution_count":100},{"cell_type":"code","source":"print(\"Decoded Text:\", decoded_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T02:22:55.756031Z","iopub.execute_input":"2025-01-11T02:22:55.756322Z","iopub.status.idle":"2025-01-11T02:22:55.760892Z","shell.execute_reply.started":"2025-01-11T02:22:55.756300Z","shell.execute_reply":"2025-01-11T02:22:55.760060Z"},"id":"Mv7lQhaFa7JK","outputId":"982d3c1f-680d-4721-f878-71bc628e6a92"},"outputs":[{"name":"stdout","text":"Decoded Text: \n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"def load_checkpoint(checkpoint_path, model, optimizer=None):\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    loss = checkpoint['loss']\n    print(f\"Checkpoint loaded: Epoch {epoch}, Loss: {loss:.4f}\")\n    return model, optimizer, epoch, loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:14:56.480065Z","iopub.execute_input":"2025-01-02T09:14:56.480394Z","iopub.status.idle":"2025-01-02T09:14:56.484822Z","shell.execute_reply.started":"2025-01-02T09:14:56.480367Z","shell.execute_reply":"2025-01-02T09:14:56.483857Z"},"id":"XRItYXvQa7JN"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint_path = \"./checkpoints/model_epoch_20.pth\"\nmodel, optimizer, start_epoch, _ = load_checkpoint(checkpoint_path, model, optimizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:14:20.031344Z","iopub.execute_input":"2025-01-02T09:14:20.031652Z","iopub.status.idle":"2025-01-02T09:14:21.532472Z","shell.execute_reply.started":"2025-01-02T09:14:20.031628Z","shell.execute_reply":"2025-01-02T09:14:21.531698Z"},"id":"iJ7FRM_wa7JO","outputId":"9cca23dd-209b-4ba0-b713-602052e8deb7"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint_path = \"./checkpoints/model_epoch_120.pth\"\ncheckpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))  # Use GPU if available: 'cuda'\nmodel.load_state_dict(checkpoint['model_state_dict'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T02:23:02.531189Z","iopub.execute_input":"2025-01-11T02:23:02.531497Z","iopub.status.idle":"2025-01-11T02:23:03.368462Z","shell.execute_reply.started":"2025-01-11T02:23:02.531472Z","shell.execute_reply":"2025-01-11T02:23:03.367561Z"},"id":"tBk9w-l_a7JO","outputId":"046613ba-e750-46b5-8175-67d5f9e61095"},"outputs":[{"name":"stderr","text":"<ipython-input-85-3dc8ea0944d1>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))  # Use GPU if available: 'cuda'\n","output_type":"stream"},{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":85},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:14:26.196139Z","iopub.execute_input":"2025-01-02T09:14:26.19651Z","iopub.status.idle":"2025-01-02T09:14:26.201901Z","shell.execute_reply.started":"2025-01-02T09:14:26.196481Z","shell.execute_reply":"2025-01-02T09:14:26.200956Z"},"id":"b9Shapm3a7JP","outputId":"84970b93-34af-46a0-e4ab-d333fbfb7627"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_eval1 = df[449:450].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T02:23:11.740840Z","iopub.execute_input":"2025-01-11T02:23:11.741128Z","iopub.status.idle":"2025-01-11T02:23:11.745711Z","shell.execute_reply.started":"2025-01-11T02:23:11.741105Z","shell.execute_reply":"2025-01-11T02:23:11.744626Z"},"id":"QznZNXd2a7JP"},"outputs":[],"execution_count":86},{"cell_type":"code","source":"eval_dataset1 = CustomDataset(df_eval1, t5_tokenizer,gpt2_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T02:23:13.710859Z","iopub.execute_input":"2025-01-11T02:23:13.711155Z","iopub.status.idle":"2025-01-11T02:23:13.714928Z","shell.execute_reply.started":"2025-01-11T02:23:13.711134Z","shell.execute_reply":"2025-01-11T02:23:13.713998Z"},"id":"JuJQTbuma7JQ"},"outputs":[],"execution_count":87},{"cell_type":"code","source":"predictions, decoded_text = inference(model, gpt2_tokenizer, t5_tokenizer, eval_dataset1, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T02:23:16.001214Z","iopub.execute_input":"2025-01-11T02:23:16.001673Z","iopub.status.idle":"2025-01-11T02:23:16.078442Z","shell.execute_reply.started":"2025-01-11T02:23:16.001625Z","shell.execute_reply":"2025-01-11T02:23:16.077768Z"},"id":"450TkAAMa7JQ"},"outputs":[],"execution_count":88},{"cell_type":"code","source":"print(\"Decoded Text:\", decoded_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T02:23:23.025821Z","iopub.execute_input":"2025-01-11T02:23:23.026129Z","iopub.status.idle":"2025-01-11T02:23:23.030591Z","shell.execute_reply.started":"2025-01-11T02:23:23.026105Z","shell.execute_reply":"2025-01-11T02:23:23.029682Z"},"id":"wOV8HjGSa7JQ","outputId":"18b10659-bb6a-4ab0-e8b7-460d63296e53"},"outputs":[{"name":"stdout","text":"Decoded Text: \n","output_type":"stream"}],"execution_count":90},{"cell_type":"code","source":"","metadata":{"trusted":true,"id":"YVP6ZYJXa7JQ"},"outputs":[],"execution_count":null}]}